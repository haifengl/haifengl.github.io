<!DOCTYPE HTML>
<html lang="en">
<head>
<!-- Generated by javadoc (21) on Fri Jan 31 17:38:53 EST 2025 -->
<title>smile.feature.extraction</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="dc.created" content="2025-01-31">
<meta name="description" content="declaration: package: smile.feature.extraction">
<meta name="generator" content="javadoc/PackageWriterImpl">
<link rel="stylesheet" type="text/css" href="../../../stylesheet.css" title="Style">
<link rel="stylesheet" type="text/css" href="../../../script-dir/jquery-ui.min.css" title="Style">
<script type="text/javascript" src="../../../script.js"></script>
<script type="text/javascript" src="../../../script-dir/jquery-3.7.1.min.js"></script>
<script type="text/javascript" src="../../../script-dir/jquery-ui.min.js"></script>
<script type="text/javascript" src="../../../script-dir/gtag.js"></script>
</head>
<body class="package-declaration-page">
<script type="text/javascript">var pathtoroot = "../../../";
loadScripts(document, 'script');</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<div class="flex-box">
<header role="banner" class="flex-header">
<nav role="navigation">
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="top-nav" id="navbar-top"><button id="navbar-toggle-button" aria-controls="navbar-top" aria-expanded="false" aria-label="Toggle navigation links"><span class="nav-bar-toggle-icon">&nbsp;</span><span class="nav-bar-toggle-icon">&nbsp;</span><span class="nav-bar-toggle-icon">&nbsp;</span></button>
<div class="skip-nav"><a href="#skip-navbar-top" title="Skip navigation links">Skip navigation links</a></div>
<ul id="navbar-top-firstrow" class="nav-list" title="Navigation">
<li><a href="../../../index.html">Overview</a></li>
<li class="nav-bar-cell1-rev">Package</li>
<li>Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../index-all.html">Index</a></li>
<li><a href="../../../help-doc.html#package">Help</a></li>
</ul>
<ul class="sub-nav-list-small">
<li>
<p>Package:</p>
<ul>
<li><a href="#package-description">Description</a></li>
<li>Related Packages</li>
<li><a href="#class-summary">Classes and Interfaces</a></li>
</ul>
</li>
</ul>
</div>
<div class="sub-nav">
<div id="navbar-sub-list">
<ul class="sub-nav-list">
<li>Package:&nbsp;</li>
<li><a href="#package-description">Description</a>&nbsp;|&nbsp;</li>
<li>Related Packages&nbsp;|&nbsp;</li>
<li><a href="#class-summary">Classes and Interfaces</a></li>
</ul>
</div>
<div class="nav-list-search"><a href="../../../search.html">SEARCH</a>
<input type="text" id="search-input" disabled placeholder="Search">
<input type="reset" id="reset-button" disabled value="reset">
</div>
</div>
<!-- ========= END OF TOP NAVBAR ========= -->
<span class="skip-nav" id="skip-navbar-top"></span></nav>
</header>
<div class="flex-content">
<main role="main">
<div class="header">
<h1 title="Package smile.feature.extraction" class="title">Package smile.feature.extraction</h1>
</div>
<hr>
<div class="package-signature">package <span class="element-name">smile.feature.extraction</span></div>
<section class="package-description" id="package-description">
<div class="block">Feature extraction. Feature extraction transforms the data in the
 high-dimensional space to a space of fewer dimensions. The data
 transformation may be linear, as in principal component analysis (PCA),
 but many nonlinear dimensionality reduction techniques also exist.
 <p>
 The main linear technique for dimensionality reduction, principal component
 analysis, performs a linear mapping of the data to a lower dimensional
 space in such a way that the variance of the data in the low-dimensional
 representation is maximized. In practice, the correlation matrix of the
 data is constructed and the eigenvectors on this matrix are computed.
 The eigenvectors that correspond to the largest eigenvalues (the principal
 components) can now be used to reconstruct a large fraction of the variance
 of the original data. Moreover, the first few eigenvectors can often be
 interpreted in terms of the large-scale physical behavior of the system.
 The original space has been reduced (with data loss, but hopefully
 retaining the most important variance) to the space spanned by a few
 eigenvectors.
 <p>
 Compared to regular batch PCA algorithm, the generalized Hebbian algorithm
 is an adaptive method to find the largest k eigenvectors of the covariance
 matrix, assuming that the associated eigenvalues are distinct. GHA works
 with an arbitrarily large sample size and the storage requirement is modest.
 Another attractive feature is that, in a non-stationary environment, it
 has an inherent ability to track gradual changes in the optimal solution
 in an inexpensive way.
 <p>
 Random projection is a promising linear dimensionality reduction technique
 for learning mixtures of Gaussians. The key idea of random projection arises
 from the Johnson-Lindenstrauss lemma: if points in a vector space are
 projected onto a randomly selected subspace of suitably high dimension,
 then the distances between the points are approximately preserved.
 <p>
 Principal component analysis can be employed in a nonlinear way by means
 of the kernel trick. The resulting technique is capable of constructing
 nonlinear mappings that maximize the variance in the data. The resulting
 technique is entitled Kernel PCA. Other prominent nonlinear techniques
 include manifold learning techniques such as locally linear embedding
 (LLE), Hessian LLE, Laplacian eigenmaps, and LTSA. These techniques
 construct a low-dimensional data representation using a cost function
 that retains local properties of the data, and can be viewed as defining
 a graph-based kernel for Kernel PCA. More recently, techniques have been
 proposed that, instead of defining a fixed kernel, try to learn the kernel
 using semidefinite programming. The most prominent example of such a
 technique is maximum variance unfolding (MVU). The central idea of MVU
 is to exactly preserve all pairwise distances between nearest neighbors
 (in the inner product space), while maximizing the distances between points
 that are not nearest neighbors.
 <p>
 An alternative approach to neighborhood preservation is through the
 minimization of a cost function that measures differences between
 distances in the input and output spaces. Important examples of such
 techniques include classical multidimensional scaling (which is identical
 to PCA), Isomap (which uses geodesic distances in the data space), diffusion
 maps (which uses diffusion distances in the data space), t-SNE (which
 minimizes the divergence between distributions over pairs of points),
 and curvilinear component analysis.
 <p>
 A different approach to nonlinear dimensionality reduction is through the
 use of autoencoders, a special kind of feed-forward neural networks with
 a bottleneck hidden layer. The training of deep encoders is typically
 performed using a greedy layer-wise pre-training (e.g., using a stack of
 Restricted Boltzmann machines) that is followed by a fine-tuning stage based
 on backpropagation.</div>
</section>
<section class="summary">
<ul class="summary-list">
<li>
<div id="class-summary">
<div class="caption"><span>Classes</span></div>
<div class="summary-table two-column-summary">
<div class="table-header col-first">Class</div>
<div class="table-header col-last">Description</div>
<div class="col-first even-row-color class-summary class-summary-tab2"><a href="BagOfWords.html" title="class in smile.feature.extraction">BagOfWords</a></div>
<div class="col-last even-row-color class-summary class-summary-tab2">
<div class="block">The bag-of-words feature of text used in natural language
 processing and information retrieval.</div>
</div>
<div class="col-first odd-row-color class-summary class-summary-tab2"><a href="BinaryEncoder.html" title="class in smile.feature.extraction">BinaryEncoder</a></div>
<div class="col-last odd-row-color class-summary class-summary-tab2">
<div class="block">Encodes categorical features using sparse one-hot scheme.</div>
</div>
<div class="col-first even-row-color class-summary class-summary-tab2"><a href="GHA.html" title="class in smile.feature.extraction">GHA</a></div>
<div class="col-last even-row-color class-summary class-summary-tab2">
<div class="block">Generalized Hebbian Algorithm.</div>
</div>
<div class="col-first odd-row-color class-summary class-summary-tab2"><a href="HashEncoder.html" title="class in smile.feature.extraction">HashEncoder</a></div>
<div class="col-last odd-row-color class-summary class-summary-tab2">
<div class="block">Feature hashing, also known as the hashing trick, is a fast and
 space-efficient way of vectorizing features, i.e.</div>
</div>
<div class="col-first even-row-color class-summary class-summary-tab2"><a href="KernelPCA.html" title="class in smile.feature.extraction">KernelPCA</a></div>
<div class="col-last even-row-color class-summary class-summary-tab2">
<div class="block">Kernel PCA transform.</div>
</div>
<div class="col-first odd-row-color class-summary class-summary-tab2"><a href="PCA.html" title="class in smile.feature.extraction">PCA</a></div>
<div class="col-last odd-row-color class-summary class-summary-tab2">
<div class="block">Principal component analysis.</div>
</div>
<div class="col-first even-row-color class-summary class-summary-tab2"><a href="ProbabilisticPCA.html" title="class in smile.feature.extraction">ProbabilisticPCA</a></div>
<div class="col-last even-row-color class-summary class-summary-tab2">
<div class="block">Probabilistic principal component analysis.</div>
</div>
<div class="col-first odd-row-color class-summary class-summary-tab2"><a href="Projection.html" title="class in smile.feature.extraction">Projection</a></div>
<div class="col-last odd-row-color class-summary class-summary-tab2">
<div class="block">A projection is a kind of feature extraction technique that transforms data
 from the input space to a feature space, linearly or non-linearly.</div>
</div>
<div class="col-first even-row-color class-summary class-summary-tab2"><a href="RandomProjection.html" title="class in smile.feature.extraction">RandomProjection</a></div>
<div class="col-last even-row-color class-summary class-summary-tab2">
<div class="block">Random projection is a promising dimensionality reduction technique for
 learning mixtures of Gaussians.</div>
</div>
<div class="col-first odd-row-color class-summary class-summary-tab2"><a href="SparseEncoder.html" title="class in smile.feature.extraction">SparseEncoder</a></div>
<div class="col-last odd-row-color class-summary class-summary-tab2">
<div class="block">Encodes numeric and categorical features into sparse array
 with on-hot encoding of categorical variables.</div>
</div>
</div>
</div>
</li>
</ul>
</section>
</main>
<footer role="contentinfo">
<hr>
<p class="legal-copy"><small>Copyright &copy; 2010-2024 Haifeng Li. All rights reserved.
Use is subject to <a href="https://raw.githubusercontent.com/haifengl/smile/master/LICENSE">license terms.</a>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-57GD08QCML"></script></small></p>
</footer>
</div>
</div>
</body>
</html>
