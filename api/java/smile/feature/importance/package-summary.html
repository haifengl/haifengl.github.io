<!DOCTYPE HTML>
<html lang="en">
<head>
<!-- Generated by javadoc (17) on Sat Mar 30 12:01:19 EDT 2024 -->
<title>smile.feature.importance</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="dc.created" content="2024-03-30">
<meta name="description" content="declaration: package: smile.feature.importance">
<meta name="generator" content="javadoc/PackageWriterImpl">
<link rel="stylesheet" type="text/css" href="../../../stylesheet.css" title="Style">
<link rel="stylesheet" type="text/css" href="../../../script-dir/jquery-ui.min.css" title="Style">
<link rel="stylesheet" type="text/css" href="../../../jquery-ui.overrides.css" title="Style">
<script type="text/javascript" src="../../../script.js"></script>
<script type="text/javascript" src="../../../script-dir/jquery-3.5.1.min.js"></script>
<script type="text/javascript" src="../../../script-dir/jquery-ui.min.js"></script>
</head>
<body class="package-declaration-page">
<script type="text/javascript">var pathtoroot = "../../../";
loadScripts(document, 'script');</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<div class="flex-box">
<header role="banner" class="flex-header">
<nav role="navigation">
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="top-nav" id="navbar-top">
<div class="skip-nav"><a href="#skip-navbar-top" title="Skip navigation links">Skip navigation links</a></div>
<ul id="navbar-top-firstrow" class="nav-list" title="Navigation">
<li><a href="../../../index.html">Overview</a></li>
<li class="nav-bar-cell1-rev">Package</li>
<li>Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../index-all.html">Index</a></li>
<li><a href="../../../help-doc.html#package">Help</a></li>
</ul>
</div>
<div class="sub-nav">
<div>
<ul class="sub-nav-list">
<li>Package:&nbsp;</li>
<li><a href="#package-description">Description</a>&nbsp;|&nbsp;</li>
<li>Related Packages&nbsp;|&nbsp;</li>
<li><a href="#class-summary">Classes and Interfaces</a></li>
</ul>
</div>
<div class="nav-list-search"><label for="search-input">SEARCH:</label>
<input type="text" id="search-input" value="search" disabled="disabled">
<input type="reset" id="reset-button" value="reset" disabled="disabled">
</div>
</div>
<!-- ========= END OF TOP NAVBAR ========= -->
<span class="skip-nav" id="skip-navbar-top"></span></nav>
</header>
<div class="flex-content">
<main role="main">
<div class="header">
<h1 title="Package smile.feature.importance" class="title">Package smile.feature.importance</h1>
</div>
<hr>
<div class="package-signature">package <span class="element-name">smile.feature.importance</span></div>
<section class="package-description" id="package-description">
<div class="block">Feature importance.
 <p>
 Global explanations tries to describe the model as whole, in terms of
 which variables/features influenced the general model the most. Two
 common methods for such an overall explanation is some kind of
 permutation feature importance or partial dependence plots.
 Permutation feature importance measures the increase in the prediction
 error of the model after permuting the feature's values, which breaks
 the relationship between the feature and the true outcome.
 The partial dependence plot (PDP) shows the marginal effect one or
 two features have on the predicted outcome of a machine learning model.
 A partial dependence plot can show whether the relationship between
 the target and a feature is linear, monotonic or more complex.
 <p>
 Local explanations tries to identify how the different input
 variables/features influenced a specific prediction/output from the model,
 and are often referred to as individual prediction explanation methods.
 Such explanations are particularly useful for complex models which behave
 rather different for different feature combinations, meaning that the
 global explanation is not representative for the local behavior.
 <p>
 Local explanation methods may further be divided into two categories:
 model-specific and model-agnostic (general) explanation methods.
 The model-agnostic methods usually try to explain individual predictions
 by learning simple, interpretable explanations of the model specifically
 for a given prediction. Three examples are Explanation Vectors, LIME,
 and Shapley values.</div>
</section>
<section class="summary">
<ul class="summary-list">
<li>
<div id="class-summary">
<div class="caption"><span>Interfaces</span></div>
<div class="summary-table two-column-summary">
<div class="table-header col-first">Class</div>
<div class="table-header col-last">Description</div>
<div class="col-first even-row-color class-summary class-summary-tab1"><a href="SHAP.html" title="interface in smile.feature.importance">SHAP</a>&lt;T&gt;</div>
<div class="col-last even-row-color class-summary class-summary-tab1">
<div class="block">SHAP (SHapley Additive exPlanations) is a game theoretic approach to
 explain the output of any machine learning model.</div>
</div>
<div class="col-first odd-row-color class-summary class-summary-tab1"><a href="TreeSHAP.html" title="interface in smile.feature.importance">TreeSHAP</a></div>
<div class="col-last odd-row-color class-summary class-summary-tab1">
<div class="block">SHAP of ensemble tree methods.</div>
</div>
</div>
</div>
</li>
</ul>
</section>
</main>
</div>
</div>
</body>
</html>
