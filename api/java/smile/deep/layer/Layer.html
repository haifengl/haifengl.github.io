<!DOCTYPE HTML>
<html lang="en">
<head>
<!-- Generated by javadoc (21) on Wed Apr 24 10:39:18 EDT 2024 -->
<title>Layer</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="dc.created" content="2024-04-24">
<meta name="description" content="declaration: package: smile.deep.layer, interface: Layer">
<meta name="generator" content="javadoc/ClassWriterImpl">
<link rel="stylesheet" type="text/css" href="../../../stylesheet.css" title="Style">
<link rel="stylesheet" type="text/css" href="../../../script-dir/jquery-ui.min.css" title="Style">
<script type="text/javascript" src="../../../script.js"></script>
<script type="text/javascript" src="../../../script-dir/jquery-3.6.1.min.js"></script>
<script type="text/javascript" src="../../../script-dir/jquery-ui.min.js"></script>
<script type="text/javascript" src="../../../script-dir/gtag.js"></script>
</head>
<body class="class-declaration-page">
<script type="text/javascript">var pathtoroot = "../../../";
loadScripts(document, 'script');</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<div class="flex-box">
<header role="banner" class="flex-header">
<nav role="navigation">
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="top-nav" id="navbar-top"><button id="navbar-toggle-button" aria-controls="navbar-top" aria-expanded="false" aria-label="Toggle navigation links"><span class="nav-bar-toggle-icon">&nbsp;</span><span class="nav-bar-toggle-icon">&nbsp;</span><span class="nav-bar-toggle-icon">&nbsp;</span></button>
<div class="skip-nav"><a href="#skip-navbar-top" title="Skip navigation links">Skip navigation links</a></div>
<ul id="navbar-top-firstrow" class="nav-list" title="Navigation">
<li><a href="../../../index.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="nav-bar-cell1-rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../index-all.html">Index</a></li>
<li><a href="../../../help-doc.html#class">Help</a></li>
</ul>
<ul class="sub-nav-list-small">
<li>
<p>Summary:</p>
<ul>
<li>Nested</li>
<li>Field</li>
<li>Constr</li>
<li><a href="#method-summary">Method</a></li>
</ul>
</li>
<li>
<p>Detail:</p>
<ul>
<li>Field</li>
<li>Constr</li>
<li><a href="#method-detail">Method</a></li>
</ul>
</li>
</ul>
</div>
<div class="sub-nav">
<div id="navbar-sub-list">
<ul class="sub-nav-list">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li>Constr&nbsp;|&nbsp;</li>
<li><a href="#method-summary">Method</a></li>
</ul>
<ul class="sub-nav-list">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li>Constr&nbsp;|&nbsp;</li>
<li><a href="#method-detail">Method</a></li>
</ul>
</div>
<div class="nav-list-search"><a href="../../../search.html">SEARCH</a>
<input type="text" id="search-input" disabled placeholder="Search">
<input type="reset" id="reset-button" disabled value="reset">
</div>
</div>
<!-- ========= END OF TOP NAVBAR ========= -->
<span class="skip-nav" id="skip-navbar-top"></span></nav>
</header>
<div class="flex-content">
<main role="main">
<!-- ======== START OF CLASS DATA ======== -->
<div class="header">
<div class="sub-title"><span class="package-label-in-type">Package</span>&nbsp;<a href="package-summary.html">smile.deep.layer</a></div>
<h1 title="Interface Layer" class="title">Interface Layer</h1>
</div>
<section class="class-description" id="class-description">
<dl class="notes">
<dt>All Superinterfaces:</dt>
<dd><code><a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/util/function/Function.html" title="class or interface in java.util.function" class="external-link">Function</a>&lt;<a href="../tensor/Tensor.html" title="class in smile.deep.tensor">Tensor</a>,<wbr><a href="../tensor/Tensor.html" title="class in smile.deep.tensor">Tensor</a>&gt;</code></dd>
</dl>
<dl class="notes">
<dt>All Known Implementing Classes:</dt>
<dd><code><a href="../activation/ActivationFunction.html" title="class in smile.deep.activation">ActivationFunction</a></code>, <code><a href="AdaptiveAvgPool2dLayer.html" title="class in smile.deep.layer">AdaptiveAvgPool2dLayer</a></code>, <code><a href="AvgPool2dLayer.html" title="class in smile.deep.layer">AvgPool2dLayer</a></code>, <code><a href="BatchNorm1dLayer.html" title="class in smile.deep.layer">BatchNorm1dLayer</a></code>, <code><a href="BatchNorm2dLayer.html" title="class in smile.deep.layer">BatchNorm2dLayer</a></code>, <code><a href="Conv2dLayer.html" title="class in smile.deep.layer">Conv2dLayer</a></code>, <code><a href="../../vision/layer/Conv2dNormActivation.html" title="class in smile.vision.layer">Conv2dNormActivation</a></code>, <code><a href="DropoutLayer.html" title="class in smile.deep.layer">DropoutLayer</a></code>, <code><a href="../../vision/EfficientNet.html" title="class in smile.vision">EfficientNet</a></code>, <code><a href="EmbeddingLayer.html" title="class in smile.deep.layer">EmbeddingLayer</a></code>, <code><a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></code>, <code><a href="../../vision/layer/FusedMBConv.html" title="class in smile.vision.layer">FusedMBConv</a></code>, <code><a href="../activation/GELU.html" title="class in smile.deep.activation">GELU</a></code>, <code><a href="../activation/GLU.html" title="class in smile.deep.activation">GLU</a></code>, <code><a href="GroupNormLayer.html" title="class in smile.deep.layer">GroupNormLayer</a></code>, <code><a href="../activation/HardShrink.html" title="class in smile.deep.activation">HardShrink</a></code>, <code><a href="LayerBlock.html" title="class in smile.deep.layer">LayerBlock</a></code>, <code><a href="../activation/LeakyReLU.html" title="class in smile.deep.activation">LeakyReLU</a></code>, <code><a href="../activation/LogSigmoid.html" title="class in smile.deep.activation">LogSigmoid</a></code>, <code><a href="../activation/LogSoftmax.html" title="class in smile.deep.activation">LogSoftmax</a></code>, <code><a href="MaxPool2dLayer.html" title="class in smile.deep.layer">MaxPool2dLayer</a></code>, <code><a href="../../vision/layer/MBConv.html" title="class in smile.vision.layer">MBConv</a></code>, <code><a href="../../llm/PositionalEncoding.html" title="class in smile.llm">PositionalEncoding</a></code>, <code><a href="../activation/ReLU.html" title="class in smile.deep.activation">ReLU</a></code>, <code><a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></code>, <code><a href="../activation/Sigmoid.html" title="class in smile.deep.activation">Sigmoid</a></code>, <code><a href="../activation/SiLU.html" title="class in smile.deep.activation">SiLU</a></code>, <code><a href="../activation/Softmax.html" title="class in smile.deep.activation">Softmax</a></code>, <code><a href="../activation/SoftShrink.html" title="class in smile.deep.activation">SoftShrink</a></code>, <code><a href="../../vision/layer/SqueezeExcitation.html" title="class in smile.vision.layer">SqueezeExcitation</a></code>, <code><a href="../../vision/layer/StochasticDepth.html" title="class in smile.vision.layer">StochasticDepth</a></code>, <code><a href="../activation/Tanh.html" title="class in smile.deep.activation">Tanh</a></code>, <code><a href="../activation/TanhShrink.html" title="class in smile.deep.activation">TanhShrink</a></code></dd>
</dl>
<hr>
<div class="type-signature"><span class="modifiers">public interface </span><span class="element-name type-name-label">Layer</span><span class="extends-implements">
extends <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/util/function/Function.html" title="class or interface in java.util.function" class="external-link">Function</a>&lt;<a href="../tensor/Tensor.html" title="class in smile.deep.tensor">Tensor</a>,<wbr><a href="../tensor/Tensor.html" title="class in smile.deep.tensor">Tensor</a>&gt;</span></div>
<div class="block">A layer in the neural network.</div>
</section>
<section class="summary">
<ul class="summary-list">
<!-- ========== METHOD SUMMARY =========== -->
<li>
<section class="method-summary" id="method-summary">
<h2>Method Summary</h2>
<div id="method-summary-table">
<div class="table-tabs" role="tablist" aria-orientation="horizontal"><button id="method-summary-table-tab0" role="tab" aria-selected="true" aria-controls="method-summary-table.tabpanel" tabindex="0" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table', 3)" class="active-table-tab">All Methods</button><button id="method-summary-table-tab1" role="tab" aria-selected="false" aria-controls="method-summary-table.tabpanel" tabindex="-1" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table-tab1', 3)" class="table-tab">Static Methods</button><button id="method-summary-table-tab2" role="tab" aria-selected="false" aria-controls="method-summary-table.tabpanel" tabindex="-1" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table-tab2', 3)" class="table-tab">Instance Methods</button><button id="method-summary-table-tab3" role="tab" aria-selected="false" aria-controls="method-summary-table.tabpanel" tabindex="-1" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table-tab3', 3)" class="table-tab">Abstract Methods</button><button id="method-summary-table-tab5" role="tab" aria-selected="false" aria-controls="method-summary-table.tabpanel" tabindex="-1" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table-tab5', 3)" class="table-tab">Default Methods</button></div>
<div id="method-summary-table.tabpanel" role="tabpanel">
<div class="summary-table three-column-summary" aria-labelledby="method-summary-table-tab0">
<div class="table-header col-first">Modifier and Type</div>
<div class="table-header col-second">Method</div>
<div class="table-header col-last">Description</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="AdaptiveAvgPool2dLayer.html" title="class in smile.deep.layer">AdaptiveAvgPool2dLayer</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#adaptiveAvgPool2d(int)" class="member-name-link">adaptiveAvgPool2d</a><wbr>(int&nbsp;size)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns an adaptive average pooling layer.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab5"><code>default <a href="../tensor/Tensor.html" title="class in smile.deep.tensor">Tensor</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab5"><code><a href="#apply(smile.deep.tensor.Tensor)" class="member-name-link">apply</a><wbr>(<a href="../tensor/Tensor.html" title="class in smile.deep.tensor">Tensor</a>&nbsp;input)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab5">&nbsp;</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code>org.bytedeco.pytorch.Module</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#asTorch()" class="member-name-link">asTorch</a>()</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">Returns the PyTorch Module object.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="AvgPool2dLayer.html" title="class in smile.deep.layer">AvgPool2dLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#avgPool2d(int)" class="member-name-link">avgPool2d</a><wbr>(int&nbsp;size)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns an average pooling layer that reduces a tensor by combining cells,
 and assigning the average value of the input cells to the output cell.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="BatchNorm1dLayer.html" title="class in smile.deep.layer">BatchNorm1dLayer</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#batchNorm1d(int)" class="member-name-link">batchNorm1d</a><wbr>(int&nbsp;in)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a normalization layer that re-centers and normalizes the output
 of one layer before feeding it to another.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="BatchNorm1dLayer.html" title="class in smile.deep.layer">BatchNorm1dLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#batchNorm1d(int,double,double,boolean)" class="member-name-link">batchNorm1d</a><wbr>(int&nbsp;in,
 double&nbsp;eps,
 double&nbsp;momentum,
 boolean&nbsp;affine)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a normalization layer that re-centers and normalizes the output
 of one layer before feeding it to another.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="BatchNorm2dLayer.html" title="class in smile.deep.layer">BatchNorm2dLayer</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#batchNorm2d(int)" class="member-name-link">batchNorm2d</a><wbr>(int&nbsp;in)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a normalization layer that re-centers and normalizes the output
 of one layer before feeding it to another.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="BatchNorm2dLayer.html" title="class in smile.deep.layer">BatchNorm2dLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#batchNorm2d(int,double,double,boolean)" class="member-name-link">batchNorm2d</a><wbr>(int&nbsp;in,
 double&nbsp;eps,
 double&nbsp;momentum,
 boolean&nbsp;affine)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a normalization layer that re-centers and normalizes the output
 of one layer before feeding it to another.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="Conv2dLayer.html" title="class in smile.deep.layer">Conv2dLayer</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#conv2d(int,int,int)" class="member-name-link">conv2d</a><wbr>(int&nbsp;in,
 int&nbsp;out,
 int&nbsp;kernel)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a convolutional layer.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="Conv2dLayer.html" title="class in smile.deep.layer">Conv2dLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#conv2d(int,int,int,int,int,int,int,boolean,java.lang.String)" class="member-name-link">conv2d</a><wbr>(int&nbsp;in,
 int&nbsp;out,
 int&nbsp;kernel,
 int&nbsp;stride,
 int&nbsp;padding,
 int&nbsp;dilation,
 int&nbsp;groups,
 boolean&nbsp;bias,
 <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/String.html" title="class or interface in java.lang" class="external-link">String</a>&nbsp;paddingMode)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a convolutional layer.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="Conv2dLayer.html" title="class in smile.deep.layer">Conv2dLayer</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#conv2d(int,int,int,int,java.lang.String,int,int,boolean,java.lang.String)" class="member-name-link">conv2d</a><wbr>(int&nbsp;in,
 int&nbsp;out,
 int&nbsp;size,
 int&nbsp;stride,
 <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/String.html" title="class or interface in java.lang" class="external-link">String</a>&nbsp;padding,
 int&nbsp;dilation,
 int&nbsp;groups,
 boolean&nbsp;bias,
 <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/String.html" title="class or interface in java.lang" class="external-link">String</a>&nbsp;paddingMode)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a convolutional layer.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="DropoutLayer.html" title="class in smile.deep.layer">DropoutLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#dropout(double)" class="member-name-link">dropout</a><wbr>(double&nbsp;p)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a dropout layer that randomly zeroes some of the elements of
 the input tensor with probability p during training.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="EmbeddingLayer.html" title="class in smile.deep.layer">EmbeddingLayer</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#embedding(int,int)" class="member-name-link">embedding</a><wbr>(int&nbsp;numTokens,
 int&nbsp;dim)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns an embedding layer that is a simple lookup table that stores
 embeddings of a fixed dictionary and size.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="EmbeddingLayer.html" title="class in smile.deep.layer">EmbeddingLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#embedding(int,int,double)" class="member-name-link">embedding</a><wbr>(int&nbsp;numTokens,
 int&nbsp;dim,
 double&nbsp;alpha)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns an embedding layer that is a simple lookup table that stores
 embeddings of a fixed dictionary and size.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="../tensor/Tensor.html" title="class in smile.deep.tensor">Tensor</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#forward(smile.deep.tensor.Tensor)" class="member-name-link">forward</a><wbr>(<a href="../tensor/Tensor.html" title="class in smile.deep.tensor">Tensor</a>&nbsp;input)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">Forward propagation (or forward pass) through the layer.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#gelu(int,int)" class="member-name-link">gelu</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with GELU activation function.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#gelu(int,int,double)" class="member-name-link">gelu</a><wbr>(int&nbsp;in,
 int&nbsp;out,
 double&nbsp;dropout)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with GELU activation function.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#hardShrink(int,int)" class="member-name-link">hardShrink</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with hard shrink activation function.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab5"><code>default boolean</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab5"><code><a href="#isTraining()" class="member-name-link">isTraining</a>()</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab5">
<div class="block">Returns true if the layer is in training mode.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#leaky(int,int,double)" class="member-name-link">leaky</a><wbr>(int&nbsp;in,
 int&nbsp;out,
 double&nbsp;negativeSlope)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with leaky ReLU activation function.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#leaky(int,int,double,double)" class="member-name-link">leaky</a><wbr>(int&nbsp;in,
 int&nbsp;out,
 double&nbsp;negativeSlope,
 double&nbsp;dropout)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with leaky ReLU activation function.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#linear(int,int)" class="member-name-link">linear</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a linear fully connected layer.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#logSigmoid(int,int)" class="member-name-link">logSigmoid</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with log sigmoid activation function.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#logSoftmax(int,int)" class="member-name-link">logSoftmax</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with log softmax activation function.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="MaxPool2dLayer.html" title="class in smile.deep.layer">MaxPool2dLayer</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#maxPool2d(int)" class="member-name-link">maxPool2d</a><wbr>(int&nbsp;size)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a max pooling layer that reduces a tensor by combining cells,
 and assigning the maximum value of the input cells to the output cell.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#relu(int,int)" class="member-name-link">relu</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with ReLU activation function.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#relu(int,int,double)" class="member-name-link">relu</a><wbr>(int&nbsp;in,
 int&nbsp;out,
 double&nbsp;dropout)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with ReLU activation function.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#sigmoid(int,int)" class="member-name-link">sigmoid</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with sigmoid activation function.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#silu(int,int)" class="member-name-link">silu</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with SiLU activation function.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#silu(int,int,double)" class="member-name-link">silu</a><wbr>(int&nbsp;in,
 int&nbsp;out,
 double&nbsp;dropout)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with SiLU activation function.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#softmax(int,int)" class="member-name-link">softmax</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with softmax activation function.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#softShrink(int,int)" class="member-name-link">softShrink</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with soft shrink activation function.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#tanh(int,int)" class="member-name-link">tanh</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with tanh activation function.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#tanhShrink(int,int)" class="member-name-link">tanhShrink</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with tanh shrink activation function.</div>
</div>
</div>
</div>
</div>
<div class="inherited-list">
<h3 id="methods-inherited-from-class-java.util.function.Function">Methods inherited from interface&nbsp;java.util.function.<a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/util/function/Function.html" title="class or interface in java.util.function" class="external-link">Function</a></h3>
<code><a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/util/function/Function.html#andThen(java.util.function.Function)" title="class or interface in java.util.function" class="external-link">andThen</a>, <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/util/function/Function.html#compose(java.util.function.Function)" title="class or interface in java.util.function" class="external-link">compose</a></code></div>
</section>
</li>
</ul>
</section>
<section class="details">
<ul class="details-list">
<!-- ============ METHOD DETAIL ========== -->
<li>
<section class="method-details" id="method-detail">
<h2>Method Details</h2>
<ul class="member-list">
<li>
<section class="detail" id="forward(smile.deep.tensor.Tensor)">
<h3>forward</h3>
<div class="member-signature"><span class="return-type"><a href="../tensor/Tensor.html" title="class in smile.deep.tensor">Tensor</a></span>&nbsp;<span class="element-name">forward</span><wbr><span class="parameters">(<a href="../tensor/Tensor.html" title="class in smile.deep.tensor">Tensor</a>&nbsp;input)</span></div>
<div class="block">Forward propagation (or forward pass) through the layer.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>input</code> - the input tensor.</dd>
<dt>Returns:</dt>
<dd>the output tensor.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="apply(smile.deep.tensor.Tensor)">
<h3>apply</h3>
<div class="member-signature"><span class="modifiers">default</span>&nbsp;<span class="return-type"><a href="../tensor/Tensor.html" title="class in smile.deep.tensor">Tensor</a></span>&nbsp;<span class="element-name">apply</span><wbr><span class="parameters">(<a href="../tensor/Tensor.html" title="class in smile.deep.tensor">Tensor</a>&nbsp;input)</span></div>
<dl class="notes">
<dt>Specified by:</dt>
<dd><code><a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/util/function/Function.html#apply(T)" title="class or interface in java.util.function" class="external-link">apply</a></code>&nbsp;in interface&nbsp;<code><a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/util/function/Function.html" title="class or interface in java.util.function" class="external-link">Function</a>&lt;<a href="../tensor/Tensor.html" title="class in smile.deep.tensor">Tensor</a>,<wbr><a href="../tensor/Tensor.html" title="class in smile.deep.tensor">Tensor</a>&gt;</code></dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="asTorch()">
<h3>asTorch</h3>
<div class="member-signature"><span class="return-type">org.bytedeco.pytorch.Module</span>&nbsp;<span class="element-name">asTorch</span>()</div>
<div class="block">Returns the PyTorch Module object.</div>
<dl class="notes">
<dt>Returns:</dt>
<dd>the PyTorch Module object.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="isTraining()">
<h3>isTraining</h3>
<div class="member-signature"><span class="modifiers">default</span>&nbsp;<span class="return-type">boolean</span>&nbsp;<span class="element-name">isTraining</span>()</div>
<div class="block">Returns true if the layer is in training mode.</div>
<dl class="notes">
<dt>Returns:</dt>
<dd>true if the layer is in training mode.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="linear(int,int)">
<h3>linear</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></span>&nbsp;<span class="element-name">linear</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a linear fully connected layer.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="relu(int,int)">
<h3>relu</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></span>&nbsp;<span class="element-name">relu</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with ReLU activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="relu(int,int,double)">
<h3>relu</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></span>&nbsp;<span class="element-name">relu</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out,
 double&nbsp;dropout)</span></div>
<div class="block">Returns a fully connected layer with ReLU activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dd><code>dropout</code> - the optional dropout probability.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="leaky(int,int,double)">
<h3>leaky</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></span>&nbsp;<span class="element-name">leaky</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out,
 double&nbsp;negativeSlope)</span></div>
<div class="block">Returns a fully connected layer with leaky ReLU activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dd><code>negativeSlope</code> - Controls the angle of the negative slope in leaky ReLU,
                     which is used for negative input values.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="leaky(int,int,double,double)">
<h3>leaky</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></span>&nbsp;<span class="element-name">leaky</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out,
 double&nbsp;negativeSlope,
 double&nbsp;dropout)</span></div>
<div class="block">Returns a fully connected layer with leaky ReLU activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dd><code>negativeSlope</code> - Controls the angle of the negative slope in leaky ReLU,
                     which is used for negative input values.</dd>
<dd><code>dropout</code> - the optional dropout probability.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="gelu(int,int)">
<h3>gelu</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></span>&nbsp;<span class="element-name">gelu</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with GELU activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="gelu(int,int,double)">
<h3>gelu</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></span>&nbsp;<span class="element-name">gelu</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out,
 double&nbsp;dropout)</span></div>
<div class="block">Returns a fully connected layer with GELU activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dd><code>dropout</code> - the optional dropout probability.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="silu(int,int)">
<h3>silu</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></span>&nbsp;<span class="element-name">silu</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with SiLU activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="silu(int,int,double)">
<h3>silu</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></span>&nbsp;<span class="element-name">silu</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out,
 double&nbsp;dropout)</span></div>
<div class="block">Returns a fully connected layer with SiLU activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dd><code>dropout</code> - the optional dropout probability.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="tanh(int,int)">
<h3>tanh</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></span>&nbsp;<span class="element-name">tanh</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with tanh activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="sigmoid(int,int)">
<h3>sigmoid</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></span>&nbsp;<span class="element-name">sigmoid</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with sigmoid activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="logSigmoid(int,int)">
<h3>logSigmoid</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></span>&nbsp;<span class="element-name">logSigmoid</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with log sigmoid activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="softmax(int,int)">
<h3>softmax</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></span>&nbsp;<span class="element-name">softmax</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with softmax activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="logSoftmax(int,int)">
<h3>logSoftmax</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></span>&nbsp;<span class="element-name">logSoftmax</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with log softmax activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="tanhShrink(int,int)">
<h3>tanhShrink</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></span>&nbsp;<span class="element-name">tanhShrink</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with tanh shrink activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="softShrink(int,int)">
<h3>softShrink</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></span>&nbsp;<span class="element-name">softShrink</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with soft shrink activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="hardShrink(int,int)">
<h3>hardShrink</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="SequentialBlock.html" title="class in smile.deep.layer">SequentialBlock</a></span>&nbsp;<span class="element-name">hardShrink</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with hard shrink activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="conv2d(int,int,int)">
<h3>conv2d</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="Conv2dLayer.html" title="class in smile.deep.layer">Conv2dLayer</a></span>&nbsp;<span class="element-name">conv2d</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out,
 int&nbsp;kernel)</span></div>
<div class="block">Returns a convolutional layer.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input channels.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dd><code>kernel</code> - the window/kernel size.</dd>
<dt>Returns:</dt>
<dd>a convolutional layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="conv2d(int,int,int,int,int,int,int,boolean,java.lang.String)">
<h3>conv2d</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="Conv2dLayer.html" title="class in smile.deep.layer">Conv2dLayer</a></span>&nbsp;<span class="element-name">conv2d</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out,
 int&nbsp;kernel,
 int&nbsp;stride,
 int&nbsp;padding,
 int&nbsp;dilation,
 int&nbsp;groups,
 boolean&nbsp;bias,
 <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/String.html" title="class or interface in java.lang" class="external-link">String</a>&nbsp;paddingMode)</span></div>
<div class="block">Returns a convolutional layer.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input channels.</dd>
<dd><code>out</code> - the number of output channels/features.</dd>
<dd><code>kernel</code> - the window/kernel size.</dd>
<dd><code>stride</code> - controls the stride for the cross-correlation.</dd>
<dd><code>padding</code> - controls the amount of padding applied on both sides.</dd>
<dd><code>dilation</code> - controls the spacing between the kernel points.</dd>
<dd><code>groups</code> - controls the connections between inputs and outputs.
              The in channels and out channels must both be divisible by groups.</dd>
<dd><code>bias</code> - If true, adds a learnable bias to the output.</dd>
<dd><code>paddingMode</code> - "zeros", "reflect", "replicate" or "circular".</dd>
<dt>Returns:</dt>
<dd>a convolutional layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="conv2d(int,int,int,int,java.lang.String,int,int,boolean,java.lang.String)">
<h3>conv2d</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="Conv2dLayer.html" title="class in smile.deep.layer">Conv2dLayer</a></span>&nbsp;<span class="element-name">conv2d</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out,
 int&nbsp;size,
 int&nbsp;stride,
 <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/String.html" title="class or interface in java.lang" class="external-link">String</a>&nbsp;padding,
 int&nbsp;dilation,
 int&nbsp;groups,
 boolean&nbsp;bias,
 <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/lang/String.html" title="class or interface in java.lang" class="external-link">String</a>&nbsp;paddingMode)</span></div>
<div class="block">Returns a convolutional layer.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input channels.</dd>
<dd><code>out</code> - the number of output channels/features.</dd>
<dd><code>size</code> - the window/kernel size.</dd>
<dd><code>stride</code> - controls the stride for the cross-correlation.</dd>
<dd><code>padding</code> - "valid" or "same". With "valid" padding, there's no
               "made-up" padding inputs. It drops the right-most columns
               (or bottom-most rows). "same" tries to pad evenly left
               and right, but if the amount of columns to be added
               is odd, it will add the extra column to the right.
               If stride is 1, the layer's outputs will have the
               same spatial dimensions as its inputs.</dd>
<dd><code>dilation</code> - controls the spacing between the kernel points.</dd>
<dd><code>groups</code> - controls the connections between inputs and outputs.
              The in channels and out channels must both be divisible by groups.</dd>
<dd><code>bias</code> - If true, adds a learnable bias to the output.</dd>
<dd><code>paddingMode</code> - "zeros", "reflect", "replicate" or "circular".</dd>
<dt>Returns:</dt>
<dd>a convolutional layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="maxPool2d(int)">
<h3>maxPool2d</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="MaxPool2dLayer.html" title="class in smile.deep.layer">MaxPool2dLayer</a></span>&nbsp;<span class="element-name">maxPool2d</span><wbr><span class="parameters">(int&nbsp;size)</span></div>
<div class="block">Returns a max pooling layer that reduces a tensor by combining cells,
 and assigning the maximum value of the input cells to the output cell.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>size</code> - the window/kernel size.</dd>
<dt>Returns:</dt>
<dd>a max pooling layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="avgPool2d(int)">
<h3>avgPool2d</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="AvgPool2dLayer.html" title="class in smile.deep.layer">AvgPool2dLayer</a></span>&nbsp;<span class="element-name">avgPool2d</span><wbr><span class="parameters">(int&nbsp;size)</span></div>
<div class="block">Returns an average pooling layer that reduces a tensor by combining cells,
 and assigning the average value of the input cells to the output cell.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>size</code> - the window/kernel size.</dd>
<dt>Returns:</dt>
<dd>a max pooling layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="adaptiveAvgPool2d(int)">
<h3>adaptiveAvgPool2d</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="AdaptiveAvgPool2dLayer.html" title="class in smile.deep.layer">AdaptiveAvgPool2dLayer</a></span>&nbsp;<span class="element-name">adaptiveAvgPool2d</span><wbr><span class="parameters">(int&nbsp;size)</span></div>
<div class="block">Returns an adaptive average pooling layer.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>size</code> - the output size.</dd>
<dt>Returns:</dt>
<dd>an adaptive average pooling layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="batchNorm1d(int)">
<h3>batchNorm1d</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="BatchNorm1dLayer.html" title="class in smile.deep.layer">BatchNorm1dLayer</a></span>&nbsp;<span class="element-name">batchNorm1d</span><wbr><span class="parameters">(int&nbsp;in)</span></div>
<div class="block">Returns a normalization layer that re-centers and normalizes the output
 of one layer before feeding it to another. Centering and scaling the
 intermediate tensors has a number of beneficial effects, such as allowing
 higher learning rates without exploding/vanishing gradients.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dt>Returns:</dt>
<dd>a normalization layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="batchNorm1d(int,double,double,boolean)">
<h3>batchNorm1d</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="BatchNorm1dLayer.html" title="class in smile.deep.layer">BatchNorm1dLayer</a></span>&nbsp;<span class="element-name">batchNorm1d</span><wbr><span class="parameters">(int&nbsp;in,
 double&nbsp;eps,
 double&nbsp;momentum,
 boolean&nbsp;affine)</span></div>
<div class="block">Returns a normalization layer that re-centers and normalizes the output
 of one layer before feeding it to another. Centering and scaling the
 intermediate tensors has a number of beneficial effects, such as allowing
 higher learning rates without exploding/vanishing gradients.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>eps</code> - a value added to the denominator for numerical stability.</dd>
<dd><code>momentum</code> - the value used for the running_mean and running_var
                computation. Can be set to 0.0 for cumulative moving average
                (i.e. simple average).</dd>
<dd><code>affine</code> - when set to true, this layer has learnable affine parameters.</dd>
<dt>Returns:</dt>
<dd>a normalization layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="batchNorm2d(int)">
<h3>batchNorm2d</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="BatchNorm2dLayer.html" title="class in smile.deep.layer">BatchNorm2dLayer</a></span>&nbsp;<span class="element-name">batchNorm2d</span><wbr><span class="parameters">(int&nbsp;in)</span></div>
<div class="block">Returns a normalization layer that re-centers and normalizes the output
 of one layer before feeding it to another. Centering and scaling the
 intermediate tensors has a number of beneficial effects, such as allowing
 higher learning rates without exploding/vanishing gradients.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dt>Returns:</dt>
<dd>a normalization layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="batchNorm2d(int,double,double,boolean)">
<h3>batchNorm2d</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="BatchNorm2dLayer.html" title="class in smile.deep.layer">BatchNorm2dLayer</a></span>&nbsp;<span class="element-name">batchNorm2d</span><wbr><span class="parameters">(int&nbsp;in,
 double&nbsp;eps,
 double&nbsp;momentum,
 boolean&nbsp;affine)</span></div>
<div class="block">Returns a normalization layer that re-centers and normalizes the output
 of one layer before feeding it to another. Centering and scaling the
 intermediate tensors has a number of beneficial effects, such as allowing
 higher learning rates without exploding/vanishing gradients.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>eps</code> - a value added to the denominator for numerical stability.</dd>
<dd><code>momentum</code> - the value used for the running_mean and running_var
                computation. Can be set to 0.0 for cumulative moving average
                (i.e. simple average).</dd>
<dd><code>affine</code> - when set to true, this layer has learnable affine parameters.</dd>
<dt>Returns:</dt>
<dd>a normalization layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="dropout(double)">
<h3>dropout</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="DropoutLayer.html" title="class in smile.deep.layer">DropoutLayer</a></span>&nbsp;<span class="element-name">dropout</span><wbr><span class="parameters">(double&nbsp;p)</span></div>
<div class="block">Returns a dropout layer that randomly zeroes some of the elements of
 the input tensor with probability p during training. The zeroed
 elements are chosen independently for each forward call and are
 sampled from a Bernoulli distribution. Each channel will be zeroed
 out independently on every forward call.

 This has proven to be an effective technique for regularization
 and preventing the co-adaptation of neurons as described in the
 paper "Improving Neural Networks by Preventing Co-adaptation
 of Feature Detectors".</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>p</code> - the probability of an element to be zeroed.</dd>
<dt>Returns:</dt>
<dd>a dropout layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="embedding(int,int)">
<h3>embedding</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="EmbeddingLayer.html" title="class in smile.deep.layer">EmbeddingLayer</a></span>&nbsp;<span class="element-name">embedding</span><wbr><span class="parameters">(int&nbsp;numTokens,
 int&nbsp;dim)</span></div>
<div class="block">Returns an embedding layer that is a simple lookup table that stores
 embeddings of a fixed dictionary and size.

 This layer is often used to store word embeddings and retrieve them
 using indices. The input to the module is a list of indices, and the
 output is the corresponding word embeddings.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>numTokens</code> - the size of the dictionary of embeddings.</dd>
<dd><code>dim</code> - the size of each embedding vector.</dd>
<dt>Returns:</dt>
<dd>a dropout layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="embedding(int,int,double)">
<h3>embedding</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="EmbeddingLayer.html" title="class in smile.deep.layer">EmbeddingLayer</a></span>&nbsp;<span class="element-name">embedding</span><wbr><span class="parameters">(int&nbsp;numTokens,
 int&nbsp;dim,
 double&nbsp;alpha)</span></div>
<div class="block">Returns an embedding layer that is a simple lookup table that stores
 embeddings of a fixed dictionary and size.

 This layer is often used to store word embeddings and retrieve them
 using indices. The input to the module is a list of indices, and the
 output is the corresponding word embeddings.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>numTokens</code> - the size of the dictionary of embeddings.</dd>
<dd><code>dim</code> - the size of each embedding vector.</dd>
<dd><code>alpha</code> - optional scaling factor.</dd>
<dt>Returns:</dt>
<dd>a dropout layer.</dd>
</dl>
</section>
</li>
</ul>
</section>
</li>
</ul>
</section>
<!-- ========= END OF CLASS DATA ========= -->
</main>
<footer role="contentinfo">
<hr>
<p class="legal-copy"><small>Copyright &copy; 2010-2024 Haifeng Li. All rights reserved.
Use is subject to <a href="https://raw.githubusercontent.com/haifengl/smile/master/LICENSE">license terms.</a>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-57GD08QCML"></script></small></p>
</footer>
</div>
</div>
</body>
</html>
