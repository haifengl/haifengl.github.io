<!DOCTYPE HTML>
<html lang="en">
<head>
<!-- Generated by javadoc (17) on Sat Mar 30 12:01:19 EDT 2024 -->
<title>Layer</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="dc.created" content="2024-03-30">
<meta name="description" content="declaration: package: smile.deep.layer, interface: Layer">
<meta name="generator" content="javadoc/ClassWriterImpl">
<link rel="stylesheet" type="text/css" href="../../../stylesheet.css" title="Style">
<link rel="stylesheet" type="text/css" href="../../../script-dir/jquery-ui.min.css" title="Style">
<link rel="stylesheet" type="text/css" href="../../../jquery-ui.overrides.css" title="Style">
<script type="text/javascript" src="../../../script.js"></script>
<script type="text/javascript" src="../../../script-dir/jquery-3.5.1.min.js"></script>
<script type="text/javascript" src="../../../script-dir/jquery-ui.min.js"></script>
</head>
<body class="class-declaration-page">
<script type="text/javascript">var evenRowColor = "even-row-color";
var oddRowColor = "odd-row-color";
var tableTab = "table-tab";
var activeTableTab = "active-table-tab";
var pathtoroot = "../../../";
loadScripts(document, 'script');</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<div class="flex-box">
<header role="banner" class="flex-header">
<nav role="navigation">
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="top-nav" id="navbar-top">
<div class="skip-nav"><a href="#skip-navbar-top" title="Skip navigation links">Skip navigation links</a></div>
<ul id="navbar-top-firstrow" class="nav-list" title="Navigation">
<li><a href="../../../index.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="nav-bar-cell1-rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../index-all.html">Index</a></li>
<li><a href="../../../help-doc.html#class">Help</a></li>
</ul>
</div>
<div class="sub-nav">
<div>
<ul class="sub-nav-list">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li>Constr&nbsp;|&nbsp;</li>
<li><a href="#method-summary">Method</a></li>
</ul>
<ul class="sub-nav-list">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li>Constr&nbsp;|&nbsp;</li>
<li><a href="#method-detail">Method</a></li>
</ul>
</div>
<div class="nav-list-search"><label for="search-input">SEARCH:</label>
<input type="text" id="search-input" value="search" disabled="disabled">
<input type="reset" id="reset-button" value="reset" disabled="disabled">
</div>
</div>
<!-- ========= END OF TOP NAVBAR ========= -->
<span class="skip-nav" id="skip-navbar-top"></span></nav>
</header>
<div class="flex-content">
<main role="main">
<!-- ======== START OF CLASS DATA ======== -->
<div class="header">
<div class="sub-title"><span class="package-label-in-type">Package</span>&nbsp;<a href="package-summary.html">smile.deep.layer</a></div>
<h1 title="Interface Layer" class="title">Interface Layer</h1>
</div>
<section class="class-description" id="class-description">
<dl class="notes">
<dt>All Known Implementing Classes:</dt>
<dd><code><a href="BatchNorm1dLayer.html" title="class in smile.deep.layer">BatchNorm1dLayer</a></code>, <code><a href="BatchNorm2dLayer.html" title="class in smile.deep.layer">BatchNorm2dLayer</a></code>, <code><a href="Conv2dLayer.html" title="class in smile.deep.layer">Conv2dLayer</a></code>, <code><a href="DropoutLayer.html" title="class in smile.deep.layer">DropoutLayer</a></code>, <code><a href="EmbeddingLayer.html" title="class in smile.deep.layer">EmbeddingLayer</a></code>, <code><a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></code>, <code><a href="MaxPool2dLayer.html" title="class in smile.deep.layer">MaxPool2dLayer</a></code>, <code><a href="../Model.html" title="class in smile.deep">Model</a></code>, <code><a href="../../llm/PositionalEncoding.html" title="class in smile.llm">PositionalEncoding</a></code></dd>
</dl>
<hr>
<div class="type-signature"><span class="modifiers">public interface </span><span class="element-name type-name-label">Layer</span></div>
<div class="block">A layer in the neural network.</div>
</section>
<section class="summary">
<ul class="summary-list">
<!-- ========== METHOD SUMMARY =========== -->
<li>
<section class="method-summary" id="method-summary">
<h2>Method Summary</h2>
<div id="method-summary-table">
<div class="table-tabs" role="tablist" aria-orientation="horizontal"><button id="method-summary-table-tab0" role="tab" aria-selected="true" aria-controls="method-summary-table.tabpanel" tabindex="0" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table', 3)" class="active-table-tab">All Methods</button><button id="method-summary-table-tab1" role="tab" aria-selected="false" aria-controls="method-summary-table.tabpanel" tabindex="-1" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table-tab1', 3)" class="table-tab">Static Methods</button><button id="method-summary-table-tab2" role="tab" aria-selected="false" aria-controls="method-summary-table.tabpanel" tabindex="-1" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table-tab2', 3)" class="table-tab">Instance Methods</button><button id="method-summary-table-tab3" role="tab" aria-selected="false" aria-controls="method-summary-table.tabpanel" tabindex="-1" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table-tab3', 3)" class="table-tab">Abstract Methods</button></div>
<div id="method-summary-table.tabpanel" role="tabpanel">
<div class="summary-table three-column-summary" aria-labelledby="method-summary-table-tab0">
<div class="table-header col-first">Modifier and Type</div>
<div class="table-header col-second">Method</div>
<div class="table-header col-last">Description</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code>org.bytedeco.pytorch.Module</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#asTorch()" class="member-name-link">asTorch</a>()</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">Returns the PyTorch Module object.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="BatchNorm1dLayer.html" title="class in smile.deep.layer">BatchNorm1dLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#batchNorm1d(int)" class="member-name-link">batchNorm1d</a><wbr>(int&nbsp;in)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a normalization layer that re-centers and normalizes the output
 of one layer before feeding it to another.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="BatchNorm1dLayer.html" title="class in smile.deep.layer">BatchNorm1dLayer</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#batchNorm1d(int,double,double,boolean)" class="member-name-link">batchNorm1d</a><wbr>(int&nbsp;in,
 double&nbsp;eps,
 double&nbsp;momentum,
 boolean&nbsp;affine)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a normalization layer that re-centers and normalizes the output
 of one layer before feeding it to another.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="BatchNorm2dLayer.html" title="class in smile.deep.layer">BatchNorm2dLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#batchNorm2d(int)" class="member-name-link">batchNorm2d</a><wbr>(int&nbsp;in)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a normalization layer that re-centers and normalizes the output
 of one layer before feeding it to another.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="BatchNorm2dLayer.html" title="class in smile.deep.layer">BatchNorm2dLayer</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#batchNorm2d(int,double,double,boolean)" class="member-name-link">batchNorm2d</a><wbr>(int&nbsp;in,
 double&nbsp;eps,
 double&nbsp;momentum,
 boolean&nbsp;affine)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a normalization layer that re-centers and normalizes the output
 of one layer before feeding it to another.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="Conv2dLayer.html" title="class in smile.deep.layer">Conv2dLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#conv2d(int,int,int)" class="member-name-link">conv2d</a><wbr>(int&nbsp;in,
 int&nbsp;out,
 int&nbsp;size)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a convolutional layer.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="Conv2dLayer.html" title="class in smile.deep.layer">Conv2dLayer</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#conv2d(int,int,int,int,int,int,boolean)" class="member-name-link">conv2d</a><wbr>(int&nbsp;in,
 int&nbsp;out,
 int&nbsp;size,
 int&nbsp;stride,
 int&nbsp;dilation,
 int&nbsp;groups,
 boolean&nbsp;bias)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a convolutional layer.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="DropoutLayer.html" title="class in smile.deep.layer">DropoutLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#dropout(double)" class="member-name-link">dropout</a><wbr>(double&nbsp;p)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a dropout layer that randomly zeroes some of the elements of
 the input tensor with probability p during training.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="EmbeddingLayer.html" title="class in smile.deep.layer">EmbeddingLayer</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#embedding(int,int)" class="member-name-link">embedding</a><wbr>(int&nbsp;numTokens,
 int&nbsp;dim)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns an embedding layer that is a simple lookup table that stores
 embeddings of a fixed dictionary and size.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="EmbeddingLayer.html" title="class in smile.deep.layer">EmbeddingLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#embedding(int,int,double)" class="member-name-link">embedding</a><wbr>(int&nbsp;numTokens,
 int&nbsp;dim,
 double&nbsp;alpha)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns an embedding layer that is a simple lookup table that stores
 embeddings of a fixed dictionary and size.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="../tensor/Tensor.html" title="class in smile.deep.tensor">Tensor</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#forward(smile.deep.tensor.Tensor)" class="member-name-link">forward</a><wbr>(<a href="../tensor/Tensor.html" title="class in smile.deep.tensor">Tensor</a>&nbsp;input)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">Forward propagation (or forward pass) through the layer.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#gelu(int,int)" class="member-name-link">gelu</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with GELU activation function.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#gelu(int,int,double)" class="member-name-link">gelu</a><wbr>(int&nbsp;in,
 int&nbsp;out,
 double&nbsp;dropout)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with GELU activation function.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#hardShrink(int,int)" class="member-name-link">hardShrink</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with hard shrink activation function.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#leaky(int,int)" class="member-name-link">leaky</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with leaky ReLU activation function.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#leaky(int,int,double)" class="member-name-link">leaky</a><wbr>(int&nbsp;in,
 int&nbsp;out,
 double&nbsp;dropout)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with leaky ReLU activation function.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#linear(int,int)" class="member-name-link">linear</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a linear fully connected layer.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#logSigmoid(int,int)" class="member-name-link">logSigmoid</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with log sigmoid activation function.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#logSoftmax(int,int)" class="member-name-link">logSoftmax</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with log softmax activation function.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="MaxPool2dLayer.html" title="class in smile.deep.layer">MaxPool2dLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#maxPool2d(int)" class="member-name-link">maxPool2d</a><wbr>(int&nbsp;size)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a max pooling layer that reduces a tensor by combining cells,
 and assigning the maximum value of the input cells to the output cell.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code>void</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#register(java.lang.String,smile.deep.layer.Layer)" class="member-name-link">register</a><wbr>(<a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/lang/String.html" title="class or interface in java.lang" class="external-link">String</a>&nbsp;name,
 <a href="Layer.html" title="interface in smile.deep.layer">Layer</a>&nbsp;parent)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">Registers this layer to a neural network.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#relu(int,int)" class="member-name-link">relu</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with ReLU activation function.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#relu(int,int,double)" class="member-name-link">relu</a><wbr>(int&nbsp;in,
 int&nbsp;out,
 double&nbsp;dropout)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with ReLU activation function.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#sigmoid(int,int)" class="member-name-link">sigmoid</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with sigmoid activation function.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#silu(int,int)" class="member-name-link">silu</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with SiLU activation function.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#silu(int,int,double)" class="member-name-link">silu</a><wbr>(int&nbsp;in,
 int&nbsp;out,
 double&nbsp;dropout)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with SiLU activation function.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#softmax(int,int)" class="member-name-link">softmax</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with softmax activation function.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#softShrink(int,int)" class="member-name-link">softShrink</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with soft shrink activation function.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab1"><code>static <a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab1"><code><a href="#tanh(int,int)" class="member-name-link">tanh</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with tanh activation function.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab1"><code>static <a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab1"><code><a href="#tanhShrink(int,int)" class="member-name-link">tanhShrink</a><wbr>(int&nbsp;in,
 int&nbsp;out)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab1">
<div class="block">Returns a fully connected layer with tanh shrink activation function.</div>
</div>
</div>
</div>
</div>
</section>
</li>
</ul>
</section>
<section class="details">
<ul class="details-list">
<!-- ============ METHOD DETAIL ========== -->
<li>
<section class="method-details" id="method-detail">
<h2>Method Details</h2>
<ul class="member-list">
<li>
<section class="detail" id="register(java.lang.String,smile.deep.layer.Layer)">
<h3>register</h3>
<div class="member-signature"><span class="return-type">void</span>&nbsp;<span class="element-name">register</span><wbr><span class="parameters">(<a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/lang/String.html" title="class or interface in java.lang" class="external-link">String</a>&nbsp;name,
 <a href="Layer.html" title="interface in smile.deep.layer">Layer</a>&nbsp;parent)</span></div>
<div class="block">Registers this layer to a neural network.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>name</code> - the name of this layer.</dd>
<dd><code>parent</code> - the parent layer that this layer is registered to.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="forward(smile.deep.tensor.Tensor)">
<h3>forward</h3>
<div class="member-signature"><span class="return-type"><a href="../tensor/Tensor.html" title="class in smile.deep.tensor">Tensor</a></span>&nbsp;<span class="element-name">forward</span><wbr><span class="parameters">(<a href="../tensor/Tensor.html" title="class in smile.deep.tensor">Tensor</a>&nbsp;input)</span></div>
<div class="block">Forward propagation (or forward pass) through the layer.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>input</code> - the input tensor.</dd>
<dt>Returns:</dt>
<dd>the output tensor.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="asTorch()">
<h3>asTorch</h3>
<div class="member-signature"><span class="return-type">org.bytedeco.pytorch.Module</span>&nbsp;<span class="element-name">asTorch</span>()</div>
<div class="block">Returns the PyTorch Module object.</div>
<dl class="notes">
<dt>Returns:</dt>
<dd>the PyTorch Module object.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="linear(int,int)">
<h3>linear</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></span>&nbsp;<span class="element-name">linear</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a linear fully connected layer.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="relu(int,int)">
<h3>relu</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></span>&nbsp;<span class="element-name">relu</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with ReLU activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="relu(int,int,double)">
<h3>relu</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></span>&nbsp;<span class="element-name">relu</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out,
 double&nbsp;dropout)</span></div>
<div class="block">Returns a fully connected layer with ReLU activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dd><code>dropout</code> - the optional dropout probability.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="leaky(int,int)">
<h3>leaky</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></span>&nbsp;<span class="element-name">leaky</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with leaky ReLU activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="leaky(int,int,double)">
<h3>leaky</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></span>&nbsp;<span class="element-name">leaky</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out,
 double&nbsp;dropout)</span></div>
<div class="block">Returns a fully connected layer with leaky ReLU activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dd><code>dropout</code> - the optional dropout probability.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="gelu(int,int)">
<h3>gelu</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></span>&nbsp;<span class="element-name">gelu</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with GELU activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="gelu(int,int,double)">
<h3>gelu</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></span>&nbsp;<span class="element-name">gelu</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out,
 double&nbsp;dropout)</span></div>
<div class="block">Returns a fully connected layer with GELU activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dd><code>dropout</code> - the optional dropout probability.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="silu(int,int)">
<h3>silu</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></span>&nbsp;<span class="element-name">silu</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with SiLU activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="silu(int,int,double)">
<h3>silu</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></span>&nbsp;<span class="element-name">silu</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out,
 double&nbsp;dropout)</span></div>
<div class="block">Returns a fully connected layer with SiLU activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dd><code>dropout</code> - the optional dropout probability.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="tanh(int,int)">
<h3>tanh</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></span>&nbsp;<span class="element-name">tanh</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with tanh activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="sigmoid(int,int)">
<h3>sigmoid</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></span>&nbsp;<span class="element-name">sigmoid</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with sigmoid activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="logSigmoid(int,int)">
<h3>logSigmoid</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></span>&nbsp;<span class="element-name">logSigmoid</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with log sigmoid activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="softmax(int,int)">
<h3>softmax</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></span>&nbsp;<span class="element-name">softmax</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with softmax activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="logSoftmax(int,int)">
<h3>logSoftmax</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></span>&nbsp;<span class="element-name">logSoftmax</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with log softmax activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="tanhShrink(int,int)">
<h3>tanhShrink</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></span>&nbsp;<span class="element-name">tanhShrink</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with tanh shrink activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="softShrink(int,int)">
<h3>softShrink</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></span>&nbsp;<span class="element-name">softShrink</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with soft shrink activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="hardShrink(int,int)">
<h3>hardShrink</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="FullyConnectedLayer.html" title="class in smile.deep.layer">FullyConnectedLayer</a></span>&nbsp;<span class="element-name">hardShrink</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out)</span></div>
<div class="block">Returns a fully connected layer with hard shrink activation function.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dt>Returns:</dt>
<dd>a fully connected layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="conv2d(int,int,int)">
<h3>conv2d</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="Conv2dLayer.html" title="class in smile.deep.layer">Conv2dLayer</a></span>&nbsp;<span class="element-name">conv2d</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out,
 int&nbsp;size)</span></div>
<div class="block">Returns a convolutional layer.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input channels.</dd>
<dd><code>out</code> - the number of output features.</dd>
<dd><code>size</code> - the window size.</dd>
<dt>Returns:</dt>
<dd>a convolutional layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="conv2d(int,int,int,int,int,int,boolean)">
<h3>conv2d</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="Conv2dLayer.html" title="class in smile.deep.layer">Conv2dLayer</a></span>&nbsp;<span class="element-name">conv2d</span><wbr><span class="parameters">(int&nbsp;in,
 int&nbsp;out,
 int&nbsp;size,
 int&nbsp;stride,
 int&nbsp;dilation,
 int&nbsp;groups,
 boolean&nbsp;bias)</span></div>
<div class="block">Returns a convolutional layer.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input channels.</dd>
<dd><code>out</code> - the number of output channels/features.</dd>
<dd><code>size</code> - the window size.</dd>
<dd><code>stride</code> - controls the stride for the cross-correlation.</dd>
<dd><code>dilation</code> - controls the spacing between the kernel points.
                It is harder to describe, but this link has a nice
                visualization of what dilation does.</dd>
<dd><code>groups</code> - controls the connections between inputs and outputs.
              The in channels and out channels must both be divisible by groups.</dd>
<dd><code>bias</code> - If true, adds a learnable bias to the output.</dd>
<dt>Returns:</dt>
<dd>a convolutional layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="maxPool2d(int)">
<h3>maxPool2d</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="MaxPool2dLayer.html" title="class in smile.deep.layer">MaxPool2dLayer</a></span>&nbsp;<span class="element-name">maxPool2d</span><wbr><span class="parameters">(int&nbsp;size)</span></div>
<div class="block">Returns a max pooling layer that reduces a tensor by combining cells,
 and assigning the maximum value of the input cells to the output cell.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>size</code> - the window/kernel size.</dd>
<dt>Returns:</dt>
<dd>a max pooling layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="batchNorm1d(int)">
<h3>batchNorm1d</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="BatchNorm1dLayer.html" title="class in smile.deep.layer">BatchNorm1dLayer</a></span>&nbsp;<span class="element-name">batchNorm1d</span><wbr><span class="parameters">(int&nbsp;in)</span></div>
<div class="block">Returns a normalization layer that re-centers and normalizes the output
 of one layer before feeding it to another. Centering and scaling the
 intermediate tensors has a number of beneficial effects, such as allowing
 higher learning rates without exploding/vanishing gradients.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dt>Returns:</dt>
<dd>a normalization layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="batchNorm1d(int,double,double,boolean)">
<h3>batchNorm1d</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="BatchNorm1dLayer.html" title="class in smile.deep.layer">BatchNorm1dLayer</a></span>&nbsp;<span class="element-name">batchNorm1d</span><wbr><span class="parameters">(int&nbsp;in,
 double&nbsp;eps,
 double&nbsp;momentum,
 boolean&nbsp;affine)</span></div>
<div class="block">Returns a normalization layer that re-centers and normalizes the output
 of one layer before feeding it to another. Centering and scaling the
 intermediate tensors has a number of beneficial effects, such as allowing
 higher learning rates without exploding/vanishing gradients.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>eps</code> - a value added to the denominator for numerical stability.</dd>
<dd><code>momentum</code> - the value used for the running_mean and running_var
                computation. Can be set to 0.0 for cumulative moving average
                (i.e. simple average).</dd>
<dd><code>affine</code> - when set to true, this layer has learnable affine parameters.</dd>
<dt>Returns:</dt>
<dd>a normalization layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="batchNorm2d(int)">
<h3>batchNorm2d</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="BatchNorm2dLayer.html" title="class in smile.deep.layer">BatchNorm2dLayer</a></span>&nbsp;<span class="element-name">batchNorm2d</span><wbr><span class="parameters">(int&nbsp;in)</span></div>
<div class="block">Returns a normalization layer that re-centers and normalizes the output
 of one layer before feeding it to another. Centering and scaling the
 intermediate tensors has a number of beneficial effects, such as allowing
 higher learning rates without exploding/vanishing gradients.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dt>Returns:</dt>
<dd>a normalization layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="batchNorm2d(int,double,double,boolean)">
<h3>batchNorm2d</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="BatchNorm2dLayer.html" title="class in smile.deep.layer">BatchNorm2dLayer</a></span>&nbsp;<span class="element-name">batchNorm2d</span><wbr><span class="parameters">(int&nbsp;in,
 double&nbsp;eps,
 double&nbsp;momentum,
 boolean&nbsp;affine)</span></div>
<div class="block">Returns a normalization layer that re-centers and normalizes the output
 of one layer before feeding it to another. Centering and scaling the
 intermediate tensors has a number of beneficial effects, such as allowing
 higher learning rates without exploding/vanishing gradients.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>in</code> - the number of input features.</dd>
<dd><code>eps</code> - a value added to the denominator for numerical stability.</dd>
<dd><code>momentum</code> - the value used for the running_mean and running_var
                computation. Can be set to 0.0 for cumulative moving average
                (i.e. simple average).</dd>
<dd><code>affine</code> - when set to true, this layer has learnable affine parameters.</dd>
<dt>Returns:</dt>
<dd>a normalization layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="dropout(double)">
<h3>dropout</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="DropoutLayer.html" title="class in smile.deep.layer">DropoutLayer</a></span>&nbsp;<span class="element-name">dropout</span><wbr><span class="parameters">(double&nbsp;p)</span></div>
<div class="block">Returns a dropout layer that randomly zeroes some of the elements of
 the input tensor with probability p during training. The zeroed
 elements are chosen independently for each forward call and are
 sampled from a Bernoulli distribution. Each channel will be zeroed
 out independently on every forward call.

 This has proven to be an effective technique for regularization
 and preventing the co-adaptation of neurons as described in the
 paper "Improving Neural Networks by Preventing Co-adaptation
 of Feature Detectors".</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>p</code> - the probability of an element to be zeroed.</dd>
<dt>Returns:</dt>
<dd>a dropout layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="embedding(int,int)">
<h3>embedding</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="EmbeddingLayer.html" title="class in smile.deep.layer">EmbeddingLayer</a></span>&nbsp;<span class="element-name">embedding</span><wbr><span class="parameters">(int&nbsp;numTokens,
 int&nbsp;dim)</span></div>
<div class="block">Returns an embedding layer that is a simple lookup table that stores
 embeddings of a fixed dictionary and size.

 This layer is often used to store word embeddings and retrieve them
 using indices. The input to the module is a list of indices, and the
 output is the corresponding word embeddings.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>numTokens</code> - the size of the dictionary of embeddings.</dd>
<dd><code>dim</code> - the size of each embedding vector.</dd>
<dt>Returns:</dt>
<dd>a dropout layer.</dd>
</dl>
</section>
</li>
<li>
<section class="detail" id="embedding(int,int,double)">
<h3>embedding</h3>
<div class="member-signature"><span class="modifiers">static</span>&nbsp;<span class="return-type"><a href="EmbeddingLayer.html" title="class in smile.deep.layer">EmbeddingLayer</a></span>&nbsp;<span class="element-name">embedding</span><wbr><span class="parameters">(int&nbsp;numTokens,
 int&nbsp;dim,
 double&nbsp;alpha)</span></div>
<div class="block">Returns an embedding layer that is a simple lookup table that stores
 embeddings of a fixed dictionary and size.

 This layer is often used to store word embeddings and retrieve them
 using indices. The input to the module is a list of indices, and the
 output is the corresponding word embeddings.</div>
<dl class="notes">
<dt>Parameters:</dt>
<dd><code>numTokens</code> - the size of the dictionary of embeddings.</dd>
<dd><code>dim</code> - the size of each embedding vector.</dd>
<dd><code>alpha</code> - optional scaling factor.</dd>
<dt>Returns:</dt>
<dd>a dropout layer.</dd>
</dl>
</section>
</li>
</ul>
</section>
</li>
</ul>
</section>
<!-- ========= END OF CLASS DATA ========= -->
</main>
</div>
</div>
</body>
</html>
