<HTML>
<HEAD>
<meta charset="UTF-8">
<title>lasso - smile-kotlin</title>
<link rel="stylesheet" href="../../style.css">
</HEAD>
<BODY>
<a href="../index.html">smile-kotlin</a>&nbsp;/&nbsp;<a href="index.html">smile.regression</a>&nbsp;/&nbsp;<a href="./lasso.html">lasso</a><br/>
<br/>
<h1>lasso</h1>
<a name="smile.regression$lasso(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Double, kotlin.Double, kotlin.Int)"></a>
<code><span class="keyword">fun </span><span class="identifier">lasso</span><span class="symbol">(</span><span class="identifier" id="smile.regression$lasso(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Double, kotlin.Double, kotlin.Int)/formula">formula</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/data/formula/Formula.html"><span class="identifier">Formula</span></a><span class="symbol">, </span><span class="identifier" id="smile.regression$lasso(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Double, kotlin.Double, kotlin.Int)/data">data</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/data/DataFrame.html"><span class="identifier">DataFrame</span></a><span class="symbol">, </span><span class="identifier" id="smile.regression$lasso(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Double, kotlin.Double, kotlin.Int)/lambda">lambda</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a><span class="symbol">, </span><span class="identifier" id="smile.regression$lasso(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Double, kotlin.Double, kotlin.Int)/tol">tol</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;1E-3<span class="symbol">, </span><span class="identifier" id="smile.regression$lasso(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Double, kotlin.Double, kotlin.Int)/maxIter">maxIter</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;5000<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/regression/LinearModel.html"><span class="identifier">LinearModel</span></a></code>
<p>Least absolute shrinkage and selection operator.
The Lasso is a shrinkage and selection method for linear regression.
It minimizes the usual sum of squared errors, with a bound on the sum
of the absolute values of the coefficients (i.e. L1-regularized).
It has connections to soft-thresholding of wavelet coefficients, forward
stage-wise regression, and boosting methods.</p>
<p>The Lasso typically yields a sparse solution, of which the parameter
vector &beta; has relatively few nonzero coefficients. In contrast, the
solution of L2-regularized least squares (i.e. ridge regression)
typically has all coefficients nonzero. Because it effectively
reduces the number of variables, the Lasso is useful in some contexts.</p>
<p>For over-determined systems (more instances than variables, commonly in
machine learning), we normalize variables with mean 0 and standard deviation</p>
<ol><li>For under-determined systems (less instances than variables, e.g.
compressed sensing), we assume white noise (i.e. no intercept in the linear
model) and do not perform normalization. Note that the solution
is not unique in this case.</li>
</ol>
<p>There is no analytic formula or expression for the optimal solution to the
L1-regularized least squares problems. Therefore, its solution
must be computed numerically. The objective function in the
L1-regularized least squares is convex but not differentiable,
so solving it is more of a computational challenge than solving the
L2-regularized least squares. The Lasso may be solved using
quadratic programming or more general convex optimization methods, as well
as by specific algorithms such as the least angle regression algorithm.</p>
<p>====References:====</p>
<ul><li>R. Tibshirani. Regression shrinkage and selection via the lasso. J. Royal. Statist. Soc B., 58(1):267-288, 1996.</li>
<li>B. Efron, I. Johnstone, T. Hastie, and R. Tibshirani. Least angle regression. Annals of Statistics, 2003</li>
<li>Seung-Jean Kim, K. Koh, M. Lustig, Stephen Boyd, and Dimitry Gorinevsky. An Interior-Point Method for Large-Scale L1-Regularized Least Squares. IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 1, NO. 4, 2007.</li>
</ul>
<h3>Parameters</h3>
<p><a name="formula"></a>
<code>formula</code> - a symbolic description of the model to be fitted.</p>
<p><a name="data"></a>
<code>data</code> - the data frame of the explanatory and response variables.</p>
<p><a name="lambda"></a>
<code>lambda</code> - the shrinkage/regularization parameter.</p>
<p><a name="tol"></a>
<code>tol</code> - the tolerance for stopping iterations (relative target duality gap).</p>
<p><a name="maxIter"></a>
<code>maxIter</code> - the maximum number of iterations.</p>
</BODY>
</HTML>
