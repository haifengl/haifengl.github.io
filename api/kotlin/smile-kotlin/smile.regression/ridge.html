<HTML>
<HEAD>
<meta charset="UTF-8">
<title>ridge - smile-kotlin</title>
<link rel="stylesheet" href="../../style.css">
</HEAD>
<BODY>
<a href="../index.html">smile-kotlin</a>&nbsp;/&nbsp;<a href="index.html">smile.regression</a>&nbsp;/&nbsp;<a href="./ridge.html">ridge</a><br/>
<br/>
<h1>ridge</h1>
<a name="smile.regression$ridge(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Double)"></a>
<code><span class="keyword">fun </span><span class="identifier">ridge</span><span class="symbol">(</span><span class="identifier" id="smile.regression$ridge(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Double)/formula">formula</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/data/formula/Formula.html"><span class="identifier">Formula</span></a><span class="symbol">, </span><span class="identifier" id="smile.regression$ridge(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Double)/data">data</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/data/DataFrame.html"><span class="identifier">DataFrame</span></a><span class="symbol">, </span><span class="identifier" id="smile.regression$ridge(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Double)/lambda">lambda</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a><span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/regression/LinearModel.html"><span class="identifier">LinearModel</span></a></code>
<p>Ridge Regression. When the predictor variables are highly correlated amongst
themselves, the coefficients of the resulting least squares fit may be very
imprecise. By allowing a small amount of bias in the estimates, more
reasonable coefficients may often be obtained. Ridge regression is one
method to address these issues. Often, small amounts of bias lead to
dramatic reductions in the variance of the estimated model coefficients.
Ridge regression is such a technique which shrinks the regression
coefficients by imposing a penalty on their size. Ridge regression was
originally developed to overcome the singularity of the X'X matrix.
This matrix is perturbed so as to make its determinant appreciably
different from 0.</p>
<p>Ridge regression is a kind of Tikhonov regularization, which is the most
commonly used method of regularization of ill-posed problems. Another
interpretation of ridge regression is available through Bayesian estimation.
In this setting the belief that weight should be small is coded into a prior
distribution.</p>
<h3>Parameters</h3>
<p><a name="formula"></a>
<code>formula</code> - a symbolic description of the model to be fitted.</p>
<p><a name="data"></a>
<code>data</code> - the data frame of the explanatory and response variables.</p>
<p><a name="lambda"></a>
<code>lambda</code> - the shrinkage/regularization parameter.</p>
</BODY>
</HTML>
