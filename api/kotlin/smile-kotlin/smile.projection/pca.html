<HTML>
<HEAD>
<meta charset="UTF-8">
<title>pca - smile-kotlin</title>
<link rel="stylesheet" href="../../style.css">
</HEAD>
<BODY>
<a href="../index.html">smile-kotlin</a>&nbsp;/&nbsp;<a href="index.html">smile.projection</a>&nbsp;/&nbsp;<a href="./pca.html">pca</a><br/>
<br/>
<h1>pca</h1>
<a name="smile.projection$pca(kotlin.Array((kotlin.DoubleArray)), kotlin.Boolean)"></a>
<code><span class="keyword">fun </span><span class="identifier">pca</span><span class="symbol">(</span><span class="identifier" id="smile.projection$pca(kotlin.Array((kotlin.DoubleArray)), kotlin.Boolean)/data">data</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.projection$pca(kotlin.Array((kotlin.DoubleArray)), kotlin.Boolean)/cor">cor</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-boolean/index.html"><span class="identifier">Boolean</span></a>&nbsp;<span class="symbol">=</span>&nbsp;false<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/projection/PCA.html"><span class="identifier">PCA</span></a></code>
<p>Principal component analysis. PCA is an orthogonal
linear transformation that transforms a number of possibly correlated
variables into a smaller number of uncorrelated variables called principal
components. The first principal component accounts for as much of the
variability in the data as possible, and each succeeding component accounts
for as much of the remaining variability as possible. PCA is theoretically
the optimum transform for given data in least square terms.
PCA can be thought of as revealing the internal structure of the data in
a way which best explains the variance in the data. If a multivariate
dataset is visualized as a set of coordinates in a high-dimensional data
space, PCA supplies the user with a lower-dimensional picture when viewed
from its (in some sense) most informative viewpoint.</p>
<p>PCA is mostly used as a tool in exploratory data analysis and for making
predictive models. PCA involves the calculation of the eigenvalue
decomposition of a data covariance matrix or singular value decomposition
of a data matrix, usually after mean centering the data for each attribute.
The results of a PCA are usually discussed in terms of component scores and
loadings.</p>
<p>As a linear technique, PCA is built for several purposes: first, it enables us to
decorrelate the original variables; second, to carry out data compression,
where we pay decreasing attention to the numerical accuracy by which we
encode the sequence of principal components; third, to reconstruct the
original input data using a reduced number of variables according to a
least-squares criterion; and fourth, to identify potential clusters in the data.</p>
<p>In certain applications, PCA can be misleading. PCA is heavily influenced
when there are outliers in the data. In other situations, the linearity
of PCA may be an obstacle to successful data reduction and compression.</p>
<h3>Parameters</h3>
<p><a name="data"></a>
<code>data</code> - training data. If the sample size
    is larger than the data dimension and cor = false, SVD is employed for
    efficiency. Otherwise, eigen decomposition on covariance or correlation
    matrix is performed.</p>
<p><a name="cor"></a>
<code>cor</code> - true if use correlation matrix instead of covariance matrix if ture.</p>
</BODY>
</HTML>
