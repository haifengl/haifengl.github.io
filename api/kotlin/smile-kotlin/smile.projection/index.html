<HTML>
<HEAD>
<meta charset="UTF-8">
<title>smile.projection - smile-kotlin</title>
<link rel="stylesheet" href="../../style.css">
</HEAD>
<BODY>
<a href="../index.html">smile-kotlin</a>&nbsp;/&nbsp;<a href="./index.html">smile.projection</a><br/>
<br/>
<h2>Package smile.projection</h2>
<p>Feature extraction.</p>
<p>Feature extraction transforms the data in the
high-dimensional space to a space of fewer dimensions. The data
transformation may be linear, as in principal component analysis (PCA),
but many nonlinear dimensionality reduction techniques also exist.</p>
<p>The main linear technique for dimensionality reduction, principal component
analysis, performs a linear mapping of the data to a lower dimensional
space in such a way that the variance of the data in the low-dimensional
representation is maximized. In practice, the correlation matrix of the
data is constructed and the eigenvectors on this matrix are computed.
The eigenvectors that correspond to the largest eigenvalues (the principal
components) can now be used to reconstruct a large fraction of the variance
of the original data. Moreover, the first few eigenvectors can often be
interpreted in terms of the large-scale physical behavior of the system.
The original space has been reduced (with data loss, but hopefully
retaining the most important variance) to the space spanned by a few
eigenvectors.</p>
<p>Compared to regular batch PCA algorithm, the generalized Hebbian algorithm
is an adaptive method to find the largest k eigenvectors of the covariance
matrix, assuming that the associated eigenvalues are distinct. GHA works
with an arbitrarily large sample size and the storage requirement is modest.
Another attractive feature is that, in a nonstationary environment, it
has an inherent ability to track gradual changes in the optimal solution
in an inexpensive way.</p>
<p>Random projection is a promising linear dimensionality reduction technique
for learning mixtures of Gaussians. The key idea of random projection arises
from the Johnson-Lindenstrauss lemma: if points in a vector space are
projected onto a randomly selected subspace of suitably high dimension,
then the distances between the points are approximately preserved.</p>
<p>Principal component analysis can be employed in a nonlinear way by means
of the kernel trick. The resulting technique is capable of constructing
nonlinear mappings that maximize the variance in the data. The resulting
technique is entitled Kernel PCA. Other prominent nonlinear techniques
include manifold learning techniques such as locally linear embedding
(LLE), Hessian LLE, Laplacian eigenmaps, and LTSA. These techniques
construct a low-dimensional data representation using a cost function
that retains local properties of the data, and can be viewed as defining
a graph-based kernel for Kernel PCA. More recently, techniques have been
proposed that, instead of defining a fixed kernel, try to learn the kernel
using semidefinite programming. The most prominent example of such a
technique is maximum variance unfolding (MVU). The central idea of MVU
is to exactly preserve all pairwise distances between nearest neighbors
(in the inner product space), while maximizing the distances between points
that are not nearest neighbors.</p>
<p>An alternative approach to neighborhood preservation is through the
minimization of a cost function that measures differences between
distances in the input and output spaces. Important examples of such
techniques include classical multidimensional scaling (which is identical
to PCA), Isomap (which uses geodesic distances in the data space), diffusion
maps (which uses diffusion distances in the data space), t-SNE (which
minimizes the divergence between distributions over pairs of points),
and curvilinear component analysis.</p>
<p>A different approach to nonlinear dimensionality reduction is through the
use of autoencoders, a special kind of feed-forward neural networks with
a bottle-neck hidden layer. The training of deep encoders is typically
performed using a greedy layer-wise pre-training (e.g., using a stack of
Restricted Boltzmann machines) that is followed by a finetuning stage based
on backpropagation.</p>
  
<h3>Functions</h3>
<table>
<tbody>
<tr>
<td>
<h4><a href="gha.html">gha</a></h4>
</td>
<td>
<p>Generalized Hebbian Algorithm. GHA is a linear feed-forward neural
network model for unsupervised learning with applications primarily in
principal components analysis. It is single-layer process -- that is, a
synaptic weight changes only depending on the response of the inputs and
outputs of that layer.</p>
<code><span class="keyword">fun </span><span class="identifier">gha</span><span class="symbol">(</span><span class="identifier" id="smile.projection$gha(kotlin.Array((kotlin.DoubleArray)), kotlin.Array((kotlin.DoubleArray)), kotlin.Double)/data">data</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.projection$gha(kotlin.Array((kotlin.DoubleArray)), kotlin.Array((kotlin.DoubleArray)), kotlin.Double)/w">w</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.projection$gha(kotlin.Array((kotlin.DoubleArray)), kotlin.Array((kotlin.DoubleArray)), kotlin.Double)/r">r</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a><span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/projection/GHA.html"><span class="identifier">GHA</span></a></code>
<p>Generalized Hebbian Algorithm with random initial projection matrix.</p>
<code><span class="keyword">fun </span><span class="identifier">gha</span><span class="symbol">(</span><span class="identifier" id="smile.projection$gha(kotlin.Array((kotlin.DoubleArray)), kotlin.Int, kotlin.Double)/data">data</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.projection$gha(kotlin.Array((kotlin.DoubleArray)), kotlin.Int, kotlin.Double)/k">k</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a><span class="symbol">, </span><span class="identifier" id="smile.projection$gha(kotlin.Array((kotlin.DoubleArray)), kotlin.Int, kotlin.Double)/r">r</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a><span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/projection/GHA.html"><span class="identifier">GHA</span></a></code></td>
</tr>
<tr>
<td>
<h4><a href="kpca.html">kpca</a></h4>
</td>
<td>
<p>Kernel principal component analysis. Kernel PCA is an extension of
principal component analysis (PCA) using techniques of kernel methods.
Using a kernel, the originally linear operations of PCA are done in a
reproducing kernel Hilbert space with a non-linear mapping.</p>
<code><span class="keyword">fun </span><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span> <span class="identifier">kpca</span><span class="symbol">(</span><span class="identifier" id="smile.projection$kpca(kotlin.Array((smile.projection.kpca.T)), smile.math.kernel.MercerKernel((smile.projection.kpca.T)), kotlin.Int, kotlin.Double)/data">data</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.projection$kpca(kotlin.Array((smile.projection.kpca.T)), smile.math.kernel.MercerKernel((smile.projection.kpca.T)), kotlin.Int, kotlin.Double)/kernel">kernel</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/math/kernel/MercerKernel.html"><span class="identifier">MercerKernel</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.projection$kpca(kotlin.Array((smile.projection.kpca.T)), smile.math.kernel.MercerKernel((smile.projection.kpca.T)), kotlin.Int, kotlin.Double)/k">k</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a><span class="symbol">, </span><span class="identifier" id="smile.projection$kpca(kotlin.Array((smile.projection.kpca.T)), smile.math.kernel.MercerKernel((smile.projection.kpca.T)), kotlin.Int, kotlin.Double)/threshold">threshold</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;0.0001<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/projection/KPCA.html"><span class="identifier">KPCA</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span></code></td>
</tr>
<tr>
<td>
<h4><a href="pca.html">pca</a></h4>
</td>
<td>
<p>Principal component analysis. PCA is an orthogonal
linear transformation that transforms a number of possibly correlated
variables into a smaller number of uncorrelated variables called principal
components. The first principal component accounts for as much of the
variability in the data as possible, and each succeeding component accounts
for as much of the remaining variability as possible. PCA is theoretically
the optimum transform for given data in least square terms.
PCA can be thought of as revealing the internal structure of the data in
a way which best explains the variance in the data. If a multivariate
dataset is visualized as a set of coordinates in a high-dimensional data
space, PCA supplies the user with a lower-dimensional picture when viewed
from its (in some sense) most informative viewpoint.</p>
<code><span class="keyword">fun </span><span class="identifier">pca</span><span class="symbol">(</span><span class="identifier" id="smile.projection$pca(kotlin.Array((kotlin.DoubleArray)), kotlin.Boolean)/data">data</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.projection$pca(kotlin.Array((kotlin.DoubleArray)), kotlin.Boolean)/cor">cor</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-boolean/index.html"><span class="identifier">Boolean</span></a>&nbsp;<span class="symbol">=</span>&nbsp;false<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/projection/PCA.html"><span class="identifier">PCA</span></a></code></td>
</tr>
<tr>
<td>
<h4><a href="ppca.html">ppca</a></h4>
</td>
<td>
<p>Probabilistic principal component analysis. PPCA is a simplified factor analysis
that employs a latent variable model with linear relationship:</p>
<code><span class="keyword">fun </span><span class="identifier">ppca</span><span class="symbol">(</span><span class="identifier" id="smile.projection$ppca(kotlin.Array((kotlin.DoubleArray)), kotlin.Int)/data">data</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.projection$ppca(kotlin.Array((kotlin.DoubleArray)), kotlin.Int)/k">k</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a><span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/projection/PPCA.html"><span class="identifier">PPCA</span></a></code></td>
</tr>
</tbody>
</table>
</BODY>
</HTML>
