<HTML>
<HEAD>
<meta charset="UTF-8">
<title>mlp - smile-kotlin</title>
<link rel="stylesheet" href="../../style.css">
</HEAD>
<BODY>
<a href="../index.html">smile-kotlin</a>&nbsp;/&nbsp;<a href="index.html">smile.classification</a>&nbsp;/&nbsp;<a href="./mlp.html">mlp</a><br/>
<br/>
<h1>mlp</h1>
<a name="smile.classification$mlp(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Array((smile.base.mlp.LayerBuilder)), kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Double)"></a>
<code><span class="keyword">fun </span><span class="identifier">mlp</span><span class="symbol">(</span><span class="identifier" id="smile.classification$mlp(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Array((smile.base.mlp.LayerBuilder)), kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Double)/x">x</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$mlp(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Array((smile.base.mlp.LayerBuilder)), kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Double)/y">y</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$mlp(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Array((smile.base.mlp.LayerBuilder)), kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Double)/builders">builders</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="http://haifengl.github.io/api/java/smile/base/mlp/LayerBuilder.html"><span class="identifier">LayerBuilder</span></a><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$mlp(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Array((smile.base.mlp.LayerBuilder)), kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Double)/epochs">epochs</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;10<span class="symbol">, </span><span class="identifier" id="smile.classification$mlp(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Array((smile.base.mlp.LayerBuilder)), kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Double)/eta">eta</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;0.1<span class="symbol">, </span><span class="identifier" id="smile.classification$mlp(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Array((smile.base.mlp.LayerBuilder)), kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Double)/alpha">alpha</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;0.0<span class="symbol">, </span><span class="identifier" id="smile.classification$mlp(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Array((smile.base.mlp.LayerBuilder)), kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Double)/lambda">lambda</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;0.0<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/MLP.html"><span class="identifier">MLP</span></a></code>
<p>Multilayer perceptron neural network.
An MLP consists of several layers of nodes, interconnected through weighted
acyclic arcs from each preceding layer to the following, without lateral or
feedback connections. Each node calculates a transformed weighted linear
combination of its inputs (output activations from the preceding layer), with
one of the weights acting as a trainable bias connected to a constant input.
The transformation, called activation function, is a bounded non-decreasing
(non-linear) function, such as the sigmoid functions (ranges from 0 to 1).
Another popular activation function is hyperbolic tangent which is actually
equivalent to the sigmoid function in shape but ranges from -1 to 1.
More specialized activation functions include radial basis functions which
are used in RBF networks.</p>
<p>The representational capabilities of a MLP are determined by the range of
mappings it may implement through weight variation. Single layer perceptrons
are capable of solving only linearly separable problems. With the sigmoid
function as activation function, the single-layer network is identical
to the logistic regression model.</p>
<p>The universal approximation theorem for neural networks states that every
continuous function that maps intervals of real numbers to some output
interval of real numbers can be approximated arbitrarily closely by a
multi-layer perceptron with just one hidden layer. This result holds only
for restricted classes of activation functions, which are extremely complex
and NOT smooth for subtle mathematical reasons. On the other hand, smoothness
is important for gradient descent learning. Besides, the proof is not
constructive regarding the number of neurons required or the settings of
the weights. Therefore, complex systems will have more layers of neurons
with some having increased layers of input neurons and output neurons
in practice.</p>
<p>The most popular algorithm to train MLPs is back-propagation, which is a
gradient descent method. Based on chain rule, the algorithm propagates the
error back through the network and adjusts the weights of each connection in
order to reduce the value of the error function by some small amount.
For this reason, back-propagation can only be applied on networks with
differentiable activation functions.</p>
<p>During error back propagation, we usually times the gradient with a small
number &eta;, called learning rate, which is carefully selected to ensure
that the network converges to a local minimum of the error function
fast enough, without producing oscillations. One way to avoid oscillation
at large &eta;, is to make the change in weight dependent on the past weight
change by adding a momentum term.</p>
<p>Although the back-propagation algorithm may performs gradient
descent on the total error of all instances in a batch way,
the learning rule is often applied to each instance separately in an online
way or stochastic way. There exists empirical indication that the stochastic
way results in faster convergence.</p>
<p>In practice, the problem of over-fitting has emerged. This arises in
convoluted or over-specified systems when the capacity of the network
significantly exceeds the needed free parameters. There are two general
approaches for avoiding this problem: The first is to use cross-validation
and similar techniques to check for the presence of over-fitting and
optimally select hyper-parameters such as to minimize the generalization
error. The second is to use some form of regularization, which emerges
naturally in a Bayesian framework, where the regularization can be
performed by selecting a larger prior probability over simpler models;
but also in statistical learning theory, where the goal is to minimize over
the "empirical risk" and the "structural risk".</p>
<p>For neural networks, the input patterns usually should be scaled/standardized.
Commonly, each input variable is scaled into interval <code>[0, 1]</code> or to have
mean 0 and standard deviation 1.</p>
<p>For penalty functions and output units, the following natural pairings are
recommended:</p>
<ul><li>linear output units and a least squares penalty function.</li>
<li>a two-class cross-entropy penalty function and a logistic
    activation function.</li>
<li>a multi-class cross-entropy penalty function and a softmax
    activation function.</li>
</ul>
<p>By assigning a softmax activation function on the output layer of
the neural network for categorical target variables, the outputs
can be interpreted as posterior probabilities, which are very useful.</p>
<h3>Parameters</h3>
<p><a name="x"></a>
<code>x</code> - training samples.</p>
<p><a name="y"></a>
<code>y</code> - training labels in [0, k), where k is the number of classes.</p>
<p><a name="builders"></a>
<code>builders</code> - the builders of layers from bottom to top.</p>
<p><a name="epochs"></a>
<code>epochs</code> - the number of epochs of stochastic learning.</p>
<p><a name="eta"></a>
<code>eta</code> - the learning rate.</p>
<p><a name="alpha"></a>
<code>alpha</code> - the momentum factor.</p>
<p><a name="lambda"></a>
<code>lambda</code> - the weight decay for regularization.</p>
</BODY>
</HTML>
