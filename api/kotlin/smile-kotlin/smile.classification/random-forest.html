<HTML>
<HEAD>
<meta charset="UTF-8">
<title>randomForest - smile-kotlin</title>
<link rel="stylesheet" href="../../style.css">
</HEAD>
<BODY>
<a href="../index.html">smile-kotlin</a>&nbsp;/&nbsp;<a href="index.html">smile.classification</a>&nbsp;/&nbsp;<a href="./random-forest.html">randomForest</a><br/>
<br/>
<h1>randomForest</h1>
<a name="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)"></a>
<code><span class="keyword">fun </span><span class="identifier">randomForest</span><span class="symbol">(</span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/formula">formula</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/data/formula/Formula.html"><span class="identifier">Formula</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/data">data</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/data/DataFrame.html"><span class="identifier">DataFrame</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/ntrees">ntrees</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;500<span class="symbol">, </span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/mtry">mtry</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;0<span class="symbol">, </span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/splitRule">splitRule</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/base/cart/SplitRule.html"><span class="identifier">SplitRule</span></a>&nbsp;<span class="symbol">=</span>&nbsp;SplitRule.GINI<span class="symbol">, </span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/maxDepth">maxDepth</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;20<span class="symbol">, </span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/maxNodes">maxNodes</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;500<span class="symbol">, </span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/nodeSize">nodeSize</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;1<span class="symbol">, </span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/subsample">subsample</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;1.0<span class="symbol">, </span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/classWeight">classWeight</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">?</span>&nbsp;<span class="symbol">=</span>&nbsp;null<span class="symbol">, </span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/seeds">seeds</span><span class="symbol">:</span>&nbsp;<span class="identifier">LongStream</span><span class="symbol">?</span>&nbsp;<span class="symbol">=</span>&nbsp;null<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/RandomForest.html"><span class="identifier">RandomForest</span></a></code>
<p>Random forest for classification. Random forest is an ensemble classifier
that consists of many decision trees and outputs the majority vote of
individual trees. The method combines bagging idea and the random
selection of features.</p>
<p>Each tree is constructed using the following algorithm:</p>
<ol><li>If the number of cases in the training set is N, randomly sample N cases
with replacement from the original data. This sample will
be the training set for growing the tree.</li>
<li>If there are M input variables, a number m &amp;lt;&amp;lt; M is specified such
that at each node, m variables are selected at random out of the M and
the best split on these m is used to split the node. The value of m is
held constant during the forest growing.</li>
<li>Each tree is grown to the largest extent possible. There is no pruning.</li>
</ol>
<p>The advantages of random forest are:</p>
<ul><li>For many data sets, it produces a highly accurate classifier.</li>
<li>It runs efficiently on large data sets.</li>
<li>It can handle thousands of input variables without variable deletion.</li>
<li>It gives estimates of what variables are important in the classification.</li>
<li>It generates an internal unbiased estimate of the generalization error
as the forest building progresses.</li>
<li>It has an effective method for estimating missing data and maintains
accuracy when a large proportion of the data are missing.</li>
</ul>
<p>The disadvantages are</p>
<ul><li>Random forests are prone to over-fitting for some datasets. This is
even more pronounced on noisy data.</li>
<li>For data including categorical variables with different number of
levels, random forests are biased in favor of those attributes with more
levels. Therefore, the variable importance scores from random forest are
not reliable for this type of data.</li>
</ul>
<h3>Parameters</h3>
<p><a name="formula"></a>
<code>formula</code> - a symbolic description of the model to be fitted.</p>
<p><a name="data"></a>
<code>data</code> - the data frame of the explanatory and response variables.</p>
<p><a name="ntrees"></a>
<code>ntrees</code> - the number of trees.</p>
<p><a name="mtry"></a>
<code>mtry</code> - the number of random selected features to be used to determine
    the decision at a node of the tree. floor(sqrt(dim)) seems to give
    generally good performance, where dim is the number of variables.</p>
<p><a name="maxDepth"></a>
<code>maxDepth</code> - the maximum depth of the tree.</p>
<p><a name="maxNodes"></a>
<code>maxNodes</code> - the maximum number of leaf nodes in the tree.</p>
<p><a name="nodeSize"></a>
<code>nodeSize</code> - the minimum size of leaf nodes.</p>
<p><a name="subsample"></a>
<code>subsample</code> - the sampling rate for training tree. 1.0 means sampling with replacement.
    &lt; 1.0 means sampling without replacement.</p>
<p><a name="splitRule"></a>
<code>splitRule</code> - Decision tree node split rule.</p>
<p><strong>Return</strong><br/>
Random forest classification model.</p>
</BODY>
</HTML>
