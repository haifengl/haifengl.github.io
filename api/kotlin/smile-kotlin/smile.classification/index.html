<HTML>
<HEAD>
<meta charset="UTF-8">
<title>smile.classification - smile-kotlin</title>
<link rel="stylesheet" href="../../style.css">
</HEAD>
<BODY>
<a href="../index.html">smile-kotlin</a>&nbsp;/&nbsp;<a href="./index.html">smile.classification</a><br/>
<br/>
<h2>Package smile.classification</h2>
<p>Classification algorithms.</p>
 
<p>In machine learning and pattern recognition,
classification refers to an algorithmic procedure for assigning a given
input object into one of a given number of categories. The input
object is formally termed an instance, and the categories are termed classes.</p>
<p>The instance is usually described by a vector of features, which together
constitute a description of all known characteristics of the instance.
Typically, features are either categorical (also known as nominal, i.e.
consisting of one of a set of unordered items, such as a gender of "male"
or "female", or a blood type of "A", "B", "AB" or "O"), ordinal (consisting
of one of a set of ordered items, e.g. "large", "medium" or "small"),
integer-valued (e.g. a count of the number of occurrences of a particular
word in an email) or real-valued (e.g. a measurement of blood pressure).</p>
<p>Classification normally refers to a supervised procedure, i.e. a procedure
that produces an inferred function to predict the output value of new
instances based on a training set of pairs consisting of an input object
and a desired output value. The inferred function is called a classifier
if the output is discrete or a regression function if the output is
continuous.</p>
<p>The inferred function should predict the correct output value for any valid
input object. This requires the learning algorithm to generalize from the
training data to unseen situations in a "reasonable" way.</p>
<p>A wide range of supervised learning algorithms is available, each with
its strengths and weaknesses. There is no single learning algorithm that
works best on all supervised learning problems. The most widely used
learning algorithms are AdaBoost and gradient boosting, support vector
machines, linear regression, linear discriminant analysis, logistic
regression, naive Bayes, decision trees, k-nearest neighbor algorithm,
and neural networks (multilayer perceptron).</p>
<p>If the feature vectors include features of many different kinds (discrete,
discrete ordered, counts, continuous values), some algorithms cannot be
easily applied. Many algorithms, including linear regression, logistic
regression, neural networks, and nearest neighbor methods, require that
the input features be numerical and scaled to similar ranges (e.g., to
the <code>[-1,1]</code> interval). Methods that employ a distance function, such as
nearest neighbor methods and support vector machines with Gaussian kernels,
are particularly sensitive to this. An advantage of decision trees (and
boosting algorithms based on decision trees) is that they easily handle
heterogeneous data.</p>
<p>If the input features contain redundant information (e.g., highly correlated
features), some learning algorithms (e.g., linear regression, logistic
regression, and distance based methods) will perform poorly because of
numerical instabilities. These problems can often be solved by imposing
some form of regularization.</p>
<p>If each of the features makes an independent contribution to the output,
then algorithms based on linear functions (e.g., linear regression,
logistic regression, linear support vector machines, naive Bayes) generally
perform well. However, if there are complex interactions among features,
then algorithms such as nonlinear support vector machines, decision trees
and neural networks work better. Linear methods can also be applied, but
the engineer must manually specify the interactions when using them.</p>
<p>There are several major issues to consider in supervised learning:</p>
<ul><li><strong>Features</strong>: The accuracy of the inferred function depends strongly on how the input
object is represented. Typically, the input object is transformed into
a feature vector, which contains a number of features that are descriptive
of the object. The number of features should not be too large, because of
the curse of dimensionality; but should contain enough information to
accurately predict the output.
There are many algorithms for feature selection that seek to identify
the relevant features and discard the irrelevant ones. More generally,
dimensionality reduction may seek to map the input data into a lower
dimensional space prior to running the supervised learning algorithm.</li>

<li><strong>Overfitting</strong>:
Overfitting occurs when a statistical model describes random error
or noise instead of the underlying relationship. Overfitting generally
occurs when a model is excessively complex, such as having too many
parameters relative to the number of observations. A model which has
been overfit will generally have poor predictive performance, as it can
exaggerate minor fluctuations in the data.
The potential for overfitting depends not only on the number of parameters
and data but also the conformability of the model structure with the data
shape, and the magnitude of model error compared to the expected level
of noise or error in the data.
In order to avoid overfitting, it is necessary to use additional techniques
(e.g. cross-validation, regularization, early stopping, pruning, Bayesian
priors on parameters or model comparison), that can indicate when further
training is not resulting in better generalization. The basis of some
techniques is either (1) to explicitly penalize overly complex models,
or (2) to test the model's ability to generalize by evaluating its
performance on a set of data not used for training, which is assumed to
approximate the typical unseen data that a model will encounter.</li>

<li><strong>Regularization</strong>:
Regularization involves introducing additional information in order
to solve an ill-posed problem or to prevent over-fitting. This information
is usually of the form of a penalty for complexity, such as restrictions
for smoothness or bounds on the vector space norm.
A theoretical justification for regularization is that it attempts to impose
Occam's razor on the solution. From a Bayesian point of view, many
regularization techniques correspond to imposing certain prior distributions
on model parameters.</li>

<li><strong>Bias-variance tradeoff</strong>:
Mean squared error (MSE) can be broken down into two components:
variance and squared bias, known as the bias-variance decomposition.
Thus in order to minimize the MSE, we need to minimize both the bias and
the variance. However, this is not trivial. Therefore, there is a tradeoff
between bias and variance.</li>
</ul>
<h3>Functions</h3>
<table>
<tbody>
<tr>
<td>
<h4><a href="adaboost.html">adaboost</a></h4>
</td>
<td>
<p>AdaBoost (Adaptive Boosting) classifier with decision trees. In principle,
AdaBoost is a meta-algorithm, and can be used in conjunction with many other
learning algorithms to improve their performance. In practice, AdaBoost with
decision trees is probably the most popular combination. AdaBoost is adaptive
in the sense that subsequent classifiers built are tweaked in favor of those
instances misclassified by previous classifiers. AdaBoost is sensitive to
noisy data and outliers. However in some problems it can be less susceptible
to the over-fitting problem than most learning algorithms.</p>
<code><span class="keyword">fun </span><span class="identifier">adaboost</span><span class="symbol">(</span><span class="identifier" id="smile.classification$adaboost(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Int)/formula">formula</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/data/formula/Formula.html"><span class="identifier">Formula</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$adaboost(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Int)/data">data</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/data/DataFrame.html"><span class="identifier">DataFrame</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$adaboost(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Int)/ntrees">ntrees</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;500<span class="symbol">, </span><span class="identifier" id="smile.classification$adaboost(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Int)/maxDepth">maxDepth</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;20<span class="symbol">, </span><span class="identifier" id="smile.classification$adaboost(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Int)/maxNodes">maxNodes</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;6<span class="symbol">, </span><span class="identifier" id="smile.classification$adaboost(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Int)/nodeSize">nodeSize</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;1<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/AdaBoost.html"><span class="identifier">AdaBoost</span></a></code></td>
</tr>
<tr>
<td>
<h4><a href="cart.html">cart</a></h4>
</td>
<td>
<p>Decision tree. A classification/regression tree can be learned by
splitting the training set into subsets based on an attribute value
test. This process is repeated on each derived subset in a recursive
manner called recursive partitioning. The recursion is completed when
the subset at a node all has the same value of the target variable,
or when splitting no longer adds value to the predictions.</p>
<code><span class="keyword">fun </span><span class="identifier">cart</span><span class="symbol">(</span><span class="identifier" id="smile.classification$cart(smile.data.formula.Formula, smile.data.DataFrame, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int)/formula">formula</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/data/formula/Formula.html"><span class="identifier">Formula</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$cart(smile.data.formula.Formula, smile.data.DataFrame, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int)/data">data</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/data/DataFrame.html"><span class="identifier">DataFrame</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$cart(smile.data.formula.Formula, smile.data.DataFrame, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int)/splitRule">splitRule</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/base/cart/SplitRule.html"><span class="identifier">SplitRule</span></a>&nbsp;<span class="symbol">=</span>&nbsp;SplitRule.GINI<span class="symbol">, </span><span class="identifier" id="smile.classification$cart(smile.data.formula.Formula, smile.data.DataFrame, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int)/maxDepth">maxDepth</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;20<span class="symbol">, </span><span class="identifier" id="smile.classification$cart(smile.data.formula.Formula, smile.data.DataFrame, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int)/maxNodes">maxNodes</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;0<span class="symbol">, </span><span class="identifier" id="smile.classification$cart(smile.data.formula.Formula, smile.data.DataFrame, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int)/nodeSize">nodeSize</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;5<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/DecisionTree.html"><span class="identifier">DecisionTree</span></a></code></td>
</tr>
<tr>
<td>
<h4><a href="fisher.html">fisher</a></h4>
</td>
<td>
<p>Fisher's linear discriminant. Fisher defined the separation between two
distributions to be the ratio of the variance between the classes to
the variance within the classes, which is, in some sense, a measure
of the signal-to-noise ratio for the class labeling. FLD finds a linear
combination of features which maximizes the separation after the projection.
The resulting combination may be used for dimensionality reduction
before later classification.</p>
<code><span class="keyword">fun </span><span class="identifier">fisher</span><span class="symbol">(</span><span class="identifier" id="smile.classification$fisher(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Int, kotlin.Double)/x">x</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$fisher(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Int, kotlin.Double)/y">y</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$fisher(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Int, kotlin.Double)/L">L</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;-1<span class="symbol">, </span><span class="identifier" id="smile.classification$fisher(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Int, kotlin.Double)/tol">tol</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;0.0001<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/FLD.html"><span class="identifier">FLD</span></a></code></td>
</tr>
<tr>
<td>
<h4><a href="gbm.html">gbm</a></h4>
</td>
<td>
<p>Gradient boosted classification trees.</p>
<code><span class="keyword">fun </span><span class="identifier">gbm</span><span class="symbol">(</span><span class="identifier" id="smile.classification$gbm(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.Double)/formula">formula</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/data/formula/Formula.html"><span class="identifier">Formula</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$gbm(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.Double)/data">data</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/data/DataFrame.html"><span class="identifier">DataFrame</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$gbm(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.Double)/ntrees">ntrees</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;500<span class="symbol">, </span><span class="identifier" id="smile.classification$gbm(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.Double)/maxDepth">maxDepth</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;20<span class="symbol">, </span><span class="identifier" id="smile.classification$gbm(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.Double)/maxNodes">maxNodes</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;6<span class="symbol">, </span><span class="identifier" id="smile.classification$gbm(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.Double)/nodeSize">nodeSize</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;5<span class="symbol">, </span><span class="identifier" id="smile.classification$gbm(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.Double)/shrinkage">shrinkage</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;0.05<span class="symbol">, </span><span class="identifier" id="smile.classification$gbm(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.Double)/subsample">subsample</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;0.7<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/GradientTreeBoost.html"><span class="identifier">GradientTreeBoost</span></a></code></td>
</tr>
<tr>
<td>
<h4><a href="knn.html">knn</a></h4>
</td>
<td>
<p>K-nearest neighbor classifier.
The k-nearest neighbor algorithm (k-NN) is
a method for classifying objects by a majority vote of its neighbors,
with the object being assigned to the class most common amongst its k
nearest neighbors (k is a positive integer, typically small).
k-NN is a type of instance-based learning, or lazy learning where the
function is only approximated locally and all computation
is deferred until classification.</p>
<code><span class="keyword">fun </span><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span> <span class="identifier">knn</span><span class="symbol">(</span><span class="identifier" id="smile.classification$knn(smile.neighbor.KNNSearch((smile.classification.knn.T, )), kotlin.IntArray, kotlin.Int)/x">x</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/neighbor/KNNSearch.html"><span class="identifier">KNNSearch</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">,</span>&nbsp;<span class="identifier">T</span><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$knn(smile.neighbor.KNNSearch((smile.classification.knn.T, )), kotlin.IntArray, kotlin.Int)/y">y</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$knn(smile.neighbor.KNNSearch((smile.classification.knn.T, )), kotlin.IntArray, kotlin.Int)/k">k</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a><span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/KNN.html"><span class="identifier">KNN</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span></code>
<p>K-nearest neighbor classifier.</p>
<code><span class="keyword">fun </span><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span> <span class="identifier">knn</span><span class="symbol">(</span><span class="identifier" id="smile.classification$knn(kotlin.Array((smile.classification.knn.T)), kotlin.IntArray, kotlin.Int, smile.math.distance.Distance((smile.classification.knn.T)))/x">x</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$knn(kotlin.Array((smile.classification.knn.T)), kotlin.IntArray, kotlin.Int, smile.math.distance.Distance((smile.classification.knn.T)))/y">y</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$knn(kotlin.Array((smile.classification.knn.T)), kotlin.IntArray, kotlin.Int, smile.math.distance.Distance((smile.classification.knn.T)))/k">k</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$knn(kotlin.Array((smile.classification.knn.T)), kotlin.IntArray, kotlin.Int, smile.math.distance.Distance((smile.classification.knn.T)))/distance">distance</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/math/distance/Distance.html"><span class="identifier">Distance</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span><span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/KNN.html"><span class="identifier">KNN</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span></code>
<p>K-nearest neighbor classifier with Euclidean distance as the similarity measure.</p>
<code><span class="keyword">fun </span><span class="identifier">knn</span><span class="symbol">(</span><span class="identifier" id="smile.classification$knn(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Int)/x">x</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$knn(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Int)/y">y</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$knn(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Int)/k">k</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a><span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/KNN.html"><span class="identifier">KNN</span></a><span class="symbol">&lt;</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">&gt;</span></code></td>
</tr>
<tr>
<td>
<h4><a href="lda.html">lda</a></h4>
</td>
<td>
<p>Linear discriminant analysis. LDA is based on the Bayes decision theory
and assumes that the conditional probability density functions are normally
distributed. LDA also makes the simplifying homoscedastic assumption (i.e.
that the class covariances are identical) and that the covariances have full
rank. With these assumptions, the discriminant function of an input being
in a class is purely a function of this linear combination of independent
variables.</p>
<code><span class="keyword">fun </span><span class="identifier">lda</span><span class="symbol">(</span><span class="identifier" id="smile.classification$lda(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.DoubleArray, kotlin.Double)/x">x</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$lda(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.DoubleArray, kotlin.Double)/y">y</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$lda(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.DoubleArray, kotlin.Double)/priori">priori</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">?</span>&nbsp;<span class="symbol">=</span>&nbsp;null<span class="symbol">, </span><span class="identifier" id="smile.classification$lda(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.DoubleArray, kotlin.Double)/tol">tol</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;0.0001<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/LDA.html"><span class="identifier">LDA</span></a></code></td>
</tr>
<tr>
<td>
<h4><a href="logit.html">logit</a></h4>
</td>
<td>
<p>Logistic regression.
Logistic regression (logit model) is a generalized
linear model used for binomial regression. Logistic regression applies
maximum likelihood estimation after transforming the dependent into
a logit variable. A logit is the natural log of the odds of the dependent
equaling a certain value or not (usually 1 in binary logistic models,
the highest value in multinomial models). In this way, logistic regression
estimates the odds of a certain event (value) occurring.</p>
<code><span class="keyword">fun </span><span class="identifier">logit</span><span class="symbol">(</span><span class="identifier" id="smile.classification$logit(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Double, kotlin.Double, kotlin.Int)/x">x</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$logit(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Double, kotlin.Double, kotlin.Int)/y">y</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$logit(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Double, kotlin.Double, kotlin.Int)/lambda">lambda</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;0.0<span class="symbol">, </span><span class="identifier" id="smile.classification$logit(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Double, kotlin.Double, kotlin.Int)/tol">tol</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;1E-5<span class="symbol">, </span><span class="identifier" id="smile.classification$logit(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Double, kotlin.Double, kotlin.Int)/maxIter">maxIter</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;500<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/LogisticRegression.html"><span class="identifier">LogisticRegression</span></a></code></td>
</tr>
<tr>
<td>
<h4><a href="maxent.html">maxent</a></h4>
</td>
<td>
<p>Maximum entropy classifier.
Maximum entropy is a technique for learning
probability distributions from data. In maximum entropy models, the
observed data itself is assumed to be the testable information. Maximum
entropy models don't assume anything about the probability distribution
other than what have been observed and always choose the most uniform
distribution subject to the observed constraints.</p>
<code><span class="keyword">fun </span><span class="identifier">maxent</span><span class="symbol">(</span><span class="identifier" id="smile.classification$maxent(kotlin.Array((kotlin.IntArray)), kotlin.IntArray, kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Int)/x">x</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$maxent(kotlin.Array((kotlin.IntArray)), kotlin.IntArray, kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Int)/y">y</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$maxent(kotlin.Array((kotlin.IntArray)), kotlin.IntArray, kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Int)/p">p</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$maxent(kotlin.Array((kotlin.IntArray)), kotlin.IntArray, kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Int)/lambda">lambda</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;0.1<span class="symbol">, </span><span class="identifier" id="smile.classification$maxent(kotlin.Array((kotlin.IntArray)), kotlin.IntArray, kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Int)/tol">tol</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;1E-5<span class="symbol">, </span><span class="identifier" id="smile.classification$maxent(kotlin.Array((kotlin.IntArray)), kotlin.IntArray, kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Int)/maxIter">maxIter</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;500<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/Maxent.html"><span class="identifier">Maxent</span></a></code></td>
</tr>
<tr>
<td>
<h4><a href="mlp.html">mlp</a></h4>
</td>
<td>
<p>Multilayer perceptron neural network.
An MLP consists of several layers of nodes, interconnected through weighted
acyclic arcs from each preceding layer to the following, without lateral or
feedback connections. Each node calculates a transformed weighted linear
combination of its inputs (output activations from the preceding layer), with
one of the weights acting as a trainable bias connected to a constant input.
The transformation, called activation function, is a bounded non-decreasing
(non-linear) function, such as the sigmoid functions (ranges from 0 to 1).
Another popular activation function is hyperbolic tangent which is actually
equivalent to the sigmoid function in shape but ranges from -1 to 1.
More specialized activation functions include radial basis functions which
are used in RBF networks.</p>
<code><span class="keyword">fun </span><span class="identifier">mlp</span><span class="symbol">(</span><span class="identifier" id="smile.classification$mlp(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Array((smile.base.mlp.LayerBuilder)), kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Double)/x">x</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$mlp(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Array((smile.base.mlp.LayerBuilder)), kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Double)/y">y</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$mlp(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Array((smile.base.mlp.LayerBuilder)), kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Double)/builders">builders</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="http://haifengl.github.io/api/java/smile/base/mlp/LayerBuilder.html"><span class="identifier">LayerBuilder</span></a><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$mlp(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Array((smile.base.mlp.LayerBuilder)), kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Double)/epochs">epochs</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;10<span class="symbol">, </span><span class="identifier" id="smile.classification$mlp(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Array((smile.base.mlp.LayerBuilder)), kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Double)/eta">eta</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;0.1<span class="symbol">, </span><span class="identifier" id="smile.classification$mlp(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Array((smile.base.mlp.LayerBuilder)), kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Double)/alpha">alpha</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;0.0<span class="symbol">, </span><span class="identifier" id="smile.classification$mlp(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Array((smile.base.mlp.LayerBuilder)), kotlin.Int, kotlin.Double, kotlin.Double, kotlin.Double)/lambda">lambda</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;0.0<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/MLP.html"><span class="identifier">MLP</span></a></code></td>
</tr>
<tr>
<td>
<h4><a href="naive-bayes.html">naiveBayes</a></h4>
</td>
<td>
<p>Creates a naive Bayes classifier for document classification.
Add-k smoothing.</p>
<code><span class="keyword">fun </span><span class="identifier">naiveBayes</span><span class="symbol">(</span><span class="identifier" id="smile.classification$naiveBayes(kotlin.Array((kotlin.IntArray)), kotlin.IntArray, smile.classification.DiscreteNaiveBayes.Model, kotlin.DoubleArray, kotlin.Double)/x">x</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$naiveBayes(kotlin.Array((kotlin.IntArray)), kotlin.IntArray, smile.classification.DiscreteNaiveBayes.Model, kotlin.DoubleArray, kotlin.Double)/y">y</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$naiveBayes(kotlin.Array((kotlin.IntArray)), kotlin.IntArray, smile.classification.DiscreteNaiveBayes.Model, kotlin.DoubleArray, kotlin.Double)/model">model</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/classification/DiscreteNaiveBayes/Model.html"><span class="identifier">Model</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$naiveBayes(kotlin.Array((kotlin.IntArray)), kotlin.IntArray, smile.classification.DiscreteNaiveBayes.Model, kotlin.DoubleArray, kotlin.Double)/priori">priori</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">?</span>&nbsp;<span class="symbol">=</span>&nbsp;null<span class="symbol">, </span><span class="identifier" id="smile.classification$naiveBayes(kotlin.Array((kotlin.IntArray)), kotlin.IntArray, smile.classification.DiscreteNaiveBayes.Model, kotlin.DoubleArray, kotlin.Double)/sigma">sigma</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;1.0<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/DiscreteNaiveBayes.html"><span class="identifier">DiscreteNaiveBayes</span></a></code>
<p>Creates a general naive Bayes classifier.</p>
<code><span class="keyword">fun </span><span class="identifier">naiveBayes</span><span class="symbol">(</span><span class="identifier" id="smile.classification$naiveBayes(kotlin.DoubleArray, kotlin.Array((kotlin.Array((smile.stat.distribution.Distribution)))))/priori">priori</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$naiveBayes(kotlin.DoubleArray, kotlin.Array((kotlin.Array((smile.stat.distribution.Distribution)))))/condprob">condprob</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="http://haifengl.github.io/api/java/smile/stat/distribution/Distribution.html"><span class="identifier">Distribution</span></a><span class="symbol">&gt;</span><span class="symbol">&gt;</span><span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/NaiveBayes.html"><span class="identifier">NaiveBayes</span></a></code></td>
</tr>
<tr>
<td>
<h4><a href="ovo.html">ovo</a></h4>
</td>
<td>
<p>One-vs-one strategy for reducing the problem of
multiclass classification to multiple binary classification problems.
This approach trains K (K − 1) / 2 binary classifiers for a
K-way multiclass problem; each receives the samples of a pair of
classes from the original training set, and must learn to distinguish
these two classes. At prediction time, a voting scheme is applied:
all K (K − 1) / 2 classifiers are applied to an unseen sample and the
class that got the highest number of positive predictions gets predicted
by the combined classifier.
Like One-vs-rest, one-vs-one suffers from ambiguities in that some
regions of its input space may receive the same number of votes.</p>
<code><span class="keyword">fun </span><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span> <span class="identifier">ovo</span><span class="symbol">(</span><span class="identifier" id="smile.classification$ovo(kotlin.Array((smile.classification.ovo.T)), kotlin.IntArray, kotlin.Function2((kotlin.Array((smile.classification.ovo.T)), kotlin.IntArray, smile.classification.Classifier(()))))/x">x</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$ovo(kotlin.Array((smile.classification.ovo.T)), kotlin.IntArray, kotlin.Function2((kotlin.Array((smile.classification.ovo.T)), kotlin.IntArray, smile.classification.Classifier(()))))/y">y</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$ovo(kotlin.Array((smile.classification.ovo.T)), kotlin.IntArray, kotlin.Function2((kotlin.Array((smile.classification.ovo.T)), kotlin.IntArray, smile.classification.Classifier(()))))/trainer">trainer</span><span class="symbol">:</span>&nbsp;<span class="symbol">(</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span><span class="symbol">,</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">)</span>&nbsp;<span class="symbol">-&gt;</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/classification/Classifier.html"><span class="identifier">Classifier</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span><span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/OneVersusOne.html"><span class="identifier">OneVersusOne</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span></code></td>
</tr>
<tr>
<td>
<h4><a href="ovr.html">ovr</a></h4>
</td>
<td>
<p>One-vs-rest (or one-vs-all) strategy for reducing the problem of
multiclass classification to multiple binary classification problems.
It involves training a single classifier per class, with the samples
of that class as positive samples and all other samples as negatives.
This strategy requires the base classifiers to produce a real-valued
confidence score for its decision, rather than just a class label;
discrete class labels alone can lead to ambiguities, where multiple
classes are predicted for a single sample.</p>
<code><span class="keyword">fun </span><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span> <span class="identifier">ovr</span><span class="symbol">(</span><span class="identifier" id="smile.classification$ovr(kotlin.Array((smile.classification.ovr.T)), kotlin.IntArray, kotlin.Function2((kotlin.Array((smile.classification.ovr.T)), kotlin.IntArray, smile.classification.Classifier(()))))/x">x</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$ovr(kotlin.Array((smile.classification.ovr.T)), kotlin.IntArray, kotlin.Function2((kotlin.Array((smile.classification.ovr.T)), kotlin.IntArray, smile.classification.Classifier(()))))/y">y</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$ovr(kotlin.Array((smile.classification.ovr.T)), kotlin.IntArray, kotlin.Function2((kotlin.Array((smile.classification.ovr.T)), kotlin.IntArray, smile.classification.Classifier(()))))/trainer">trainer</span><span class="symbol">:</span>&nbsp;<span class="symbol">(</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span><span class="symbol">,</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">)</span>&nbsp;<span class="symbol">-&gt;</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/classification/Classifier.html"><span class="identifier">Classifier</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span><span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/OneVersusRest.html"><span class="identifier">OneVersusRest</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span></code></td>
</tr>
<tr>
<td>
<h4><a href="qda.html">qda</a></h4>
</td>
<td>
<p>Quadratic discriminant analysis. QDA is closely related to linear discriminant
analysis (LDA). Like LDA, QDA models the conditional probability density
functions as a Gaussian distribution, then uses the posterior distributions
to estimate the class for a given test data. Unlike LDA, however,
in QDA there is no assumption that the covariance of each of the classes
is identical. Therefore, the resulting separating surface between
the classes is quadratic.</p>
<code><span class="keyword">fun </span><span class="identifier">qda</span><span class="symbol">(</span><span class="identifier" id="smile.classification$qda(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.DoubleArray, kotlin.Double)/x">x</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$qda(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.DoubleArray, kotlin.Double)/y">y</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$qda(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.DoubleArray, kotlin.Double)/priori">priori</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">?</span>&nbsp;<span class="symbol">=</span>&nbsp;null<span class="symbol">, </span><span class="identifier" id="smile.classification$qda(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.DoubleArray, kotlin.Double)/tol">tol</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;0.0001<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/QDA.html"><span class="identifier">QDA</span></a></code></td>
</tr>
<tr>
<td>
<h4><a href="random-forest.html">randomForest</a></h4>
</td>
<td>
<p>Random forest for classification. Random forest is an ensemble classifier
that consists of many decision trees and outputs the majority vote of
individual trees. The method combines bagging idea and the random
selection of features.</p>
<code><span class="keyword">fun </span><span class="identifier">randomForest</span><span class="symbol">(</span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/formula">formula</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/data/formula/Formula.html"><span class="identifier">Formula</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/data">data</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/data/DataFrame.html"><span class="identifier">DataFrame</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/ntrees">ntrees</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;500<span class="symbol">, </span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/mtry">mtry</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;0<span class="symbol">, </span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/splitRule">splitRule</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/base/cart/SplitRule.html"><span class="identifier">SplitRule</span></a>&nbsp;<span class="symbol">=</span>&nbsp;SplitRule.GINI<span class="symbol">, </span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/maxDepth">maxDepth</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;20<span class="symbol">, </span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/maxNodes">maxNodes</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;500<span class="symbol">, </span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/nodeSize">nodeSize</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a>&nbsp;<span class="symbol">=</span>&nbsp;1<span class="symbol">, </span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/subsample">subsample</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;1.0<span class="symbol">, </span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/classWeight">classWeight</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">?</span>&nbsp;<span class="symbol">=</span>&nbsp;null<span class="symbol">, </span><span class="identifier" id="smile.classification$randomForest(smile.data.formula.Formula, smile.data.DataFrame, kotlin.Int, kotlin.Int, smile.base.cart.SplitRule, kotlin.Int, kotlin.Int, kotlin.Int, kotlin.Double, kotlin.IntArray, java.util.stream.LongStream)/seeds">seeds</span><span class="symbol">:</span>&nbsp;<span class="identifier">LongStream</span><span class="symbol">?</span>&nbsp;<span class="symbol">=</span>&nbsp;null<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/RandomForest.html"><span class="identifier">RandomForest</span></a></code></td>
</tr>
<tr>
<td>
<h4><a href="rbfnet.html">rbfnet</a></h4>
</td>
<td>
<p>Radial basis function networks.
A radial basis function network is an
artificial neural network that uses radial basis functions as activation
functions. It is a linear combination of radial basis functions. They are
used in function approximation, time series prediction, and control.</p>
<code><span class="keyword">fun </span><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span> <span class="identifier">rbfnet</span><span class="symbol">(</span><span class="identifier" id="smile.classification$rbfnet(kotlin.Array((smile.classification.rbfnet.T)), kotlin.IntArray, kotlin.Array((smile.base.rbf.RBF((smile.classification.rbfnet.T)))), kotlin.Boolean)/x">x</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$rbfnet(kotlin.Array((smile.classification.rbfnet.T)), kotlin.IntArray, kotlin.Array((smile.base.rbf.RBF((smile.classification.rbfnet.T)))), kotlin.Boolean)/y">y</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$rbfnet(kotlin.Array((smile.classification.rbfnet.T)), kotlin.IntArray, kotlin.Array((smile.base.rbf.RBF((smile.classification.rbfnet.T)))), kotlin.Boolean)/neurons">neurons</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="http://haifengl.github.io/api/java/smile/base/rbf/RBF.html"><span class="identifier">RBF</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$rbfnet(kotlin.Array((smile.classification.rbfnet.T)), kotlin.IntArray, kotlin.Array((smile.base.rbf.RBF((smile.classification.rbfnet.T)))), kotlin.Boolean)/normalized">normalized</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-boolean/index.html"><span class="identifier">Boolean</span></a>&nbsp;<span class="symbol">=</span>&nbsp;false<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/RBFNetwork.html"><span class="identifier">RBFNetwork</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span></code>
<p>Trains a Gaussian RBF network with k-means.</p>
<code><span class="keyword">fun </span><span class="identifier">rbfnet</span><span class="symbol">(</span><span class="identifier" id="smile.classification$rbfnet(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Int, kotlin.Boolean)/x">x</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$rbfnet(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Int, kotlin.Boolean)/y">y</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$rbfnet(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Int, kotlin.Boolean)/k">k</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int/index.html"><span class="identifier">Int</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$rbfnet(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Int, kotlin.Boolean)/normalized">normalized</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-boolean/index.html"><span class="identifier">Boolean</span></a>&nbsp;<span class="symbol">=</span>&nbsp;false<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/RBFNetwork.html"><span class="identifier">RBFNetwork</span></a><span class="symbol">&lt;</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">&gt;</span></code></td>
</tr>
<tr>
<td>
<h4><a href="rda.html">rda</a></h4>
</td>
<td>
<p>Regularized discriminant analysis. RDA is a compromise between LDA and QDA,
which allows one to shrink the separate covariances of QDA toward a common
variance as in LDA. This method is very similar in flavor to ridge regression.
The regularized covariance matrices of each class is
&Sigma;k(&alpha;) = &alpha; &Sigma;k + (1 - &alpha;) &Sigma;.
The quadratic discriminant function is defined using the shrunken covariance
matrices &Sigma;k(&alpha;). The parameter &alpha; in <code>[0, 1]</code>
controls the complexity of the model. When &alpha; is one, RDA becomes QDA.
While &alpha; is zero, RDA is equivalent to LDA. Therefore, the
regularization factor &alpha; allows a continuum of models between LDA and QDA.</p>
<code><span class="keyword">fun </span><span class="identifier">rda</span><span class="symbol">(</span><span class="identifier" id="smile.classification$rda(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Double, kotlin.DoubleArray, kotlin.Double)/x">x</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$rda(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Double, kotlin.DoubleArray, kotlin.Double)/y">y</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$rda(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Double, kotlin.DoubleArray, kotlin.Double)/alpha">alpha</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$rda(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Double, kotlin.DoubleArray, kotlin.Double)/priori">priori</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double-array/index.html"><span class="identifier">DoubleArray</span></a><span class="symbol">?</span>&nbsp;<span class="symbol">=</span>&nbsp;null<span class="symbol">, </span><span class="identifier" id="smile.classification$rda(kotlin.Array((kotlin.DoubleArray)), kotlin.IntArray, kotlin.Double, kotlin.DoubleArray, kotlin.Double)/tol">tol</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;0.0001<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/RDA.html"><span class="identifier">RDA</span></a></code></td>
</tr>
<tr>
<td>
<h4><a href="svm.html">svm</a></h4>
</td>
<td>
<p>Support vector machines for classification. The basic support vector machine
is a binary linear classifier which chooses the hyperplane that represents
the largest separation, or margin, between the two classes. If such a
hyperplane exists, it is known as the maximum-margin hyperplane and the
linear classifier it defines is known as a maximum margin classifier.</p>
<code><span class="keyword">fun </span><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span> <span class="identifier">svm</span><span class="symbol">(</span><span class="identifier" id="smile.classification$svm(kotlin.Array((smile.classification.svm.T)), kotlin.IntArray, smile.math.kernel.MercerKernel((smile.classification.svm.T)), kotlin.Double, kotlin.Double)/x">x</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-array/index.html"><span class="identifier">Array</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$svm(kotlin.Array((smile.classification.svm.T)), kotlin.IntArray, smile.math.kernel.MercerKernel((smile.classification.svm.T)), kotlin.Double, kotlin.Double)/y">y</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-int-array/index.html"><span class="identifier">IntArray</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$svm(kotlin.Array((smile.classification.svm.T)), kotlin.IntArray, smile.math.kernel.MercerKernel((smile.classification.svm.T)), kotlin.Double, kotlin.Double)/kernel">kernel</span><span class="symbol">:</span>&nbsp;<a href="http://haifengl.github.io/api/java/smile/math/kernel/MercerKernel.html"><span class="identifier">MercerKernel</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span><span class="symbol">, </span><span class="identifier" id="smile.classification$svm(kotlin.Array((smile.classification.svm.T)), kotlin.IntArray, smile.math.kernel.MercerKernel((smile.classification.svm.T)), kotlin.Double, kotlin.Double)/C">C</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a><span class="symbol">, </span><span class="identifier" id="smile.classification$svm(kotlin.Array((smile.classification.svm.T)), kotlin.IntArray, smile.math.kernel.MercerKernel((smile.classification.svm.T)), kotlin.Double, kotlin.Double)/tol">tol</span><span class="symbol">:</span>&nbsp;<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin/-double/index.html"><span class="identifier">Double</span></a>&nbsp;<span class="symbol">=</span>&nbsp;1E-3<span class="symbol">)</span><span class="symbol">: </span><a href="http://haifengl.github.io/api/java/smile/classification/SVM.html"><span class="identifier">SVM</span></a><span class="symbol">&lt;</span><span class="identifier">T</span><span class="symbol">&gt;</span></code></td>
</tr>
</tbody>
</table>
</BODY>
</HTML>
