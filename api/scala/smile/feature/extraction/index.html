<!DOCTYPE html ><html><head><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" name="viewport"/><title>Smile - Statistical Machine Intelligence and Learning Engine  - smile.feature.extraction</title><meta content="Smile - Statistical Machine Intelligence and Learning Engine - smile.feature.extraction" name="description"/><meta content="Smile Statistical Machine Intelligence and Learning Engine smile.feature.extraction" name="keywords"/><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><link href="../../../lib/index.css" media="screen" type="text/css" rel="stylesheet"/><link href="../../../lib/template.css" media="screen" type="text/css" rel="stylesheet"/><link href="../../../lib/print.css" media="print" type="text/css" rel="stylesheet"/><link href="../../../lib/diagrams.css" media="screen" type="text/css" rel="stylesheet" id="diagrams-css"/><script type="text/javascript" src="../../../lib/jquery.min.js"></script><script type="text/javascript" src="../../../lib/index.js"></script><script type="text/javascript" src="../../../index.js"></script><script type="text/javascript" src="../../../lib/scheduler.js"></script><script type="text/javascript" src="../../../lib/template.js"></script><script type="text/javascript">/* this variable can be used by the JS to determine the path to the root document */
var toRoot = '../../../';</script></head><body><div id="search"><span id="doc-title">Smile - Statistical Machine Intelligence and Learning Engine<span id="doc-version"></span></span> <span class="close-results"><span class="left">&lt;</span> Back</span><div id="textfilter"><span class="input"><input autocapitalize="none" placeholder="Search" id="index-input" type="text" accesskey="/"/><i class="clear material-icons"></i><i id="search-icon" class="material-icons"></i></span></div></div><div id="search-results"><div id="search-progress"><div id="progress-fill"></div></div><div id="results-content"><div id="entity-results"></div><div id="member-results"></div></div></div><div id="content-scroll-container" style="-webkit-overflow-scrolling: touch;"><div id="content-container" style="-webkit-overflow-scrolling: touch;"><div id="subpackage-spacer"><div id="packages"><h1>Packages</h1><ul><li class="indented0 " name="_root_.root" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="_root_" class="anchorToMember"></a><a id="root:_root_" class="anchorToMember"></a> <span class="permalink"><a href="../../../index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../../../index.html" title="Smile (Statistical Machine Intelligence and Learning Engine) is a fast and comprehensive machine learning, NLP, linear algebra, graph, interpolation, and visualization system in Java and Scala."><span class="name">root</span></a></span><p class="shortcomment cmt">Smile (Statistical Machine Intelligence and Learning Engine) is
a fast and comprehensive machine learning, NLP, linear algebra,
graph, interpolation, and visualization system in Java and Scala.</p><div class="fullcomment"><div class="comment cmt"><p>Smile (Statistical Machine Intelligence and Learning Engine) is
a fast and comprehensive machine learning, NLP, linear algebra,
graph, interpolation, and visualization system in Java and Scala.
With advanced data structures and algorithms,
Smile delivers state-of-art performance.</p><p>Smile covers every aspect of machine learning, including classification,
regression, clustering, association rule mining, feature selection,
manifold learning, multidimensional scaling, genetic algorithms,
missing value imputation, efficient nearest neighbor search, etc.</p></div><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../../index.html" name="_root_" id="_root_" class="extype">root</a></dd></dl></div></li><li class="indented1 " name="_root_.smile" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="smile" class="anchorToMember"></a><a id="smile:smile" class="anchorToMember"></a> <span class="permalink"><a href="../../../smile/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../../index.html" title=""><span class="name">smile</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../../index.html" name="_root_" id="_root_" class="extype">root</a></dd></dl></div></li><li class="indented2 " name="smile.feature" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="feature" class="anchorToMember"></a><a id="feature:feature" class="anchorToMember"></a> <span class="permalink"><a href="../../../smile/feature/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../index.html" title=""><span class="name">feature</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../index.html" name="smile" id="smile" class="extype">smile</a></dd></dl></div></li><li class="indented3 current" name="smile.feature.extraction" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="extraction" class="anchorToMember"></a><a id="extraction:extraction" class="anchorToMember"></a> <span class="permalink"><a href="../../../smile/feature/extraction/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><span class="name">extraction</span></span><p class="shortcomment cmt">Feature extraction.</p><div class="fullcomment"><div class="comment cmt"><p>Feature extraction. Feature extraction transforms the data in the
high-dimensional space to a space of fewer dimensions. The data
transformation may be linear, as in principal component analysis (PCA),
but many nonlinear dimensionality reduction techniques also exist.</p><p>The main linear technique for dimensionality reduction, principal component
analysis, performs a linear mapping of the data to a lower dimensional
space in such a way that the variance of the data in the low-dimensional
representation is maximized. In practice, the correlation matrix of the
data is constructed and the eigenvectors on this matrix are computed.
The eigenvectors that correspond to the largest eigenvalues (the principal
components) can now be used to reconstruct a large fraction of the variance
of the original data. Moreover, the first few eigenvectors can often be
interpreted in terms of the large-scale physical behavior of the system.
The original space has been reduced (with data loss, but hopefully
retaining the most important variance) to the space spanned by a few
eigenvectors.</p><p>Compared to regular batch PCA algorithm, the generalized Hebbian algorithm
is an adaptive method to find the largest k eigenvectors of the covariance
matrix, assuming that the associated eigenvalues are distinct. GHA works
with an arbitrarily large sample size and the storage requirement is modest.
Another attractive feature is that, in a nonstationary environment, it
has an inherent ability to track gradual changes in the optimal solution
in an inexpensive way.</p><p>Random projection is a promising linear dimensionality reduction technique
for learning mixtures of Gaussians. The key idea of random projection arises
from the Johnson-Lindenstrauss lemma: if points in a vector space are
projected onto a randomly selected subspace of suitably high dimension,
then the distances between the points are approximately preserved.</p><p>Principal component analysis can be employed in a nonlinear way by means
of the kernel trick. The resulting technique is capable of constructing
nonlinear mappings that maximize the variance in the data. The resulting
technique is entitled Kernel PCA. Other prominent nonlinear techniques
include manifold learning techniques such as locally linear embedding
(LLE), Hessian LLE, Laplacian eigenmaps, and LTSA. These techniques
construct a low-dimensional data representation using a cost function
that retains local properties of the data, and can be viewed as defining
a graph-based kernel for Kernel PCA. More recently, techniques have been
proposed that, instead of defining a fixed kernel, try to learn the kernel
using semidefinite programming. The most prominent example of such a
technique is maximum variance unfolding (MVU). The central idea of MVU
is to exactly preserve all pairwise distances between nearest neighbors
(in the inner product space), while maximizing the distances between points
that are not nearest neighbors.</p><p>An alternative approach to neighborhood preservation is through the
minimization of a cost function that measures differences between
distances in the input and output spaces. Important examples of such
techniques include classical multidimensional scaling (which is identical
to PCA), Isomap (which uses geodesic distances in the data space), diffusion
maps (which uses diffusion distances in the data space), t-SNE (which
minimizes the divergence between distributions over pairs of points),
and curvilinear component analysis.</p><p>A different approach to nonlinear dimensionality reduction is through the
use of autoencoders, a special kind of feed-forward neural networks with
a bottle-neck hidden layer. The training of deep encoders is typically
performed using a greedy layer-wise pre-training (e.g., using a stack of
Restricted Boltzmann machines) that is followed by a finetuning stage based
on backpropagation.
</p></div><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../index.html" name="smile.feature" id="smile.feature" class="extype">feature</a></dd></dl></div></li><li class="current-entities indented3"><span class="separator"></span> <a href="package$$$dummy$.html" title="Hacking scaladoc issue-8124." class="object"></a><a href="package$$$dummy$.html" title="Hacking scaladoc issue-8124.">$dummy</a></li></ul></div></div><div id="content"><body class="package value"><div id="definition"><div class="big-circle package">p</div><p id="owner"><a href="../../index.html" name="smile" id="smile" class="extype">smile</a>.<a href="../index.html" name="smile.feature" id="smile.feature" class="extype">feature</a></p><h1>extraction<span class="permalink"><a href="../../../smile/feature/extraction/index.html" title="Permalink"><i class="material-icons"></i></a></span></h1></div><h4 id="signature" class="signature"><span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><span class="name">extraction</span></span></h4><div id="comment" class="fullcommenttop"><div class="comment cmt"><p>Feature extraction. Feature extraction transforms the data in the
high-dimensional space to a space of fewer dimensions. The data
transformation may be linear, as in principal component analysis (PCA),
but many nonlinear dimensionality reduction techniques also exist.</p><p>The main linear technique for dimensionality reduction, principal component
analysis, performs a linear mapping of the data to a lower dimensional
space in such a way that the variance of the data in the low-dimensional
representation is maximized. In practice, the correlation matrix of the
data is constructed and the eigenvectors on this matrix are computed.
The eigenvectors that correspond to the largest eigenvalues (the principal
components) can now be used to reconstruct a large fraction of the variance
of the original data. Moreover, the first few eigenvectors can often be
interpreted in terms of the large-scale physical behavior of the system.
The original space has been reduced (with data loss, but hopefully
retaining the most important variance) to the space spanned by a few
eigenvectors.</p><p>Compared to regular batch PCA algorithm, the generalized Hebbian algorithm
is an adaptive method to find the largest k eigenvectors of the covariance
matrix, assuming that the associated eigenvalues are distinct. GHA works
with an arbitrarily large sample size and the storage requirement is modest.
Another attractive feature is that, in a nonstationary environment, it
has an inherent ability to track gradual changes in the optimal solution
in an inexpensive way.</p><p>Random projection is a promising linear dimensionality reduction technique
for learning mixtures of Gaussians. The key idea of random projection arises
from the Johnson-Lindenstrauss lemma: if points in a vector space are
projected onto a randomly selected subspace of suitably high dimension,
then the distances between the points are approximately preserved.</p><p>Principal component analysis can be employed in a nonlinear way by means
of the kernel trick. The resulting technique is capable of constructing
nonlinear mappings that maximize the variance in the data. The resulting
technique is entitled Kernel PCA. Other prominent nonlinear techniques
include manifold learning techniques such as locally linear embedding
(LLE), Hessian LLE, Laplacian eigenmaps, and LTSA. These techniques
construct a low-dimensional data representation using a cost function
that retains local properties of the data, and can be viewed as defining
a graph-based kernel for Kernel PCA. More recently, techniques have been
proposed that, instead of defining a fixed kernel, try to learn the kernel
using semidefinite programming. The most prominent example of such a
technique is maximum variance unfolding (MVU). The central idea of MVU
is to exactly preserve all pairwise distances between nearest neighbors
(in the inner product space), while maximizing the distances between points
that are not nearest neighbors.</p><p>An alternative approach to neighborhood preservation is through the
minimization of a cost function that measures differences between
distances in the input and output spaces. Important examples of such
techniques include classical multidimensional scaling (which is identical
to PCA), Isomap (which uses geodesic distances in the data space), diffusion
maps (which uses diffusion distances in the data space), t-SNE (which
minimizes the divergence between distributions over pairs of points),
and curvilinear component analysis.</p><p>A different approach to nonlinear dimensionality reduction is through the
use of autoencoders, a special kind of feed-forward neural networks with
a bottle-neck hidden layer. The training of deep encoders is typically
performed using a greedy layer-wise pre-training (e.g., using a stack of
Restricted Boltzmann machines) that is followed by a finetuning stage based
on backpropagation.
</p></div><div class="toggleContainer"><div class="toggle block"><span>Linear Supertypes</span><div class="superTypes hiddenContent"><a href="https://www.scala-lang.org/api/2.13.11/scala/AnyRef.html#scala.AnyRef" name="scala.AnyRef" id="scala.AnyRef" class="extype">AnyRef</a>, <a href="https://www.scala-lang.org/api/2.13.11/scala/Any.html#scala.Any" name="scala.Any" id="scala.Any" class="extype">Any</a></div></div></div></div><div id="mbrsel"><div class="toggle"></div><div id="memberfilter"><i class="material-icons arrow"></i><span class="input"><input placeholder="Filter all members" id="mbrsel-input" type="text" accesskey="/"/></span><i class="clear material-icons"></i></div><div id="filterby"><div id="order"><span class="filtertype">Ordering</span><ol><li class="alpha in"><span>Alphabetic</span></li><li class="inherit out"><span>By Inheritance</span></li></ol></div><div class="ancestors"><span class="filtertype">Inherited<br/></span><ol id="linearization"><li class="in" name="smile.feature.extraction"><span>extraction</span></li><li class="in" name="scala.AnyRef"><span>AnyRef</span></li><li class="in" name="scala.Any"><span>Any</span></li></ol></div><div class="ancestors"><span class="filtertype"></span><ol><li class="hideall out"><span>Hide All</span></li><li class="showall in"><span>Show All</span></li></ol></div><div id="visbl"><span class="filtertype">Visibility</span><ol><li class="public in"><span>Public</span></li><li class="protected out"><span>Protected</span></li></ol></div></div></div><div id="template"><div id="allMembers"><div class="values members"><h3>Value Members</h3><ol><li class="indented0 " name="smile.feature.extraction#gha" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="gha(data:Array[Array[Double]],k:Int,r:smile.math.TimeFunction):smile.feature.extraction.GHA" class="anchorToMember"></a><a id="gha(Array[Array[Double]],Int,TimeFunction):GHA" class="anchorToMember"></a> <span class="permalink"><a href="../../../smile/feature/extraction/index.html#gha(data:Array[Array[Double]],k:Int,r:smile.math.TimeFunction):smile.feature.extraction.GHA" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">gha</span><span class="params">(<span name="data">data: <a href="https://www.scala-lang.org/api/2.13.11/scala/Array.html#scala.Array" name="scala.Array" id="scala.Array" class="extype">Array</a>[<a href="https://www.scala-lang.org/api/2.13.11/scala/Array.html#scala.Array" name="scala.Array" id="scala.Array" class="extype">Array</a>[<a href="https://www.scala-lang.org/api/2.13.11/scala/Double.html#scala.Double" name="scala.Double" id="scala.Double" class="extype">Double</a>]]</span>, <span name="k">k: <a href="https://www.scala-lang.org/api/2.13.11/scala/Int.html#scala.Int" name="scala.Int" id="scala.Int" class="extype">Int</a></span>, <span name="r">r: <span name="smile.math.TimeFunction" class="extype">TimeFunction</span></span>)</span><span class="result">: <span name="smile.feature.extraction.GHA" class="extype">GHA</span></span></span><p class="shortcomment cmt">Generalized Hebbian Algorithm with random initial projection matrix.</p><div class="fullcomment"><div class="comment cmt"><p>Generalized Hebbian Algorithm with random initial projection matrix.
</p></div><dl class="paramcmts block"><dt class="param">data</dt><dd class="cmt"><p>training data.</p></dd><dt class="param">k</dt><dd class="cmt"><p>the dimension of feature space.</p></dd><dt class="param">r</dt><dd class="cmt"><p>the learning rate.</p></dd></dl></div></li><li class="indented0 " name="smile.feature.extraction#gha" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="gha(data:Array[Array[Double]],w:Array[Array[Double]],r:smile.math.TimeFunction):smile.feature.extraction.GHA" class="anchorToMember"></a><a id="gha(Array[Array[Double]],Array[Array[Double]],TimeFunction):GHA" class="anchorToMember"></a> <span class="permalink"><a href="../../../smile/feature/extraction/index.html#gha(data:Array[Array[Double]],w:Array[Array[Double]],r:smile.math.TimeFunction):smile.feature.extraction.GHA" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">gha</span><span class="params">(<span name="data">data: <a href="https://www.scala-lang.org/api/2.13.11/scala/Array.html#scala.Array" name="scala.Array" id="scala.Array" class="extype">Array</a>[<a href="https://www.scala-lang.org/api/2.13.11/scala/Array.html#scala.Array" name="scala.Array" id="scala.Array" class="extype">Array</a>[<a href="https://www.scala-lang.org/api/2.13.11/scala/Double.html#scala.Double" name="scala.Double" id="scala.Double" class="extype">Double</a>]]</span>, <span name="w">w: <a href="https://www.scala-lang.org/api/2.13.11/scala/Array.html#scala.Array" name="scala.Array" id="scala.Array" class="extype">Array</a>[<a href="https://www.scala-lang.org/api/2.13.11/scala/Array.html#scala.Array" name="scala.Array" id="scala.Array" class="extype">Array</a>[<a href="https://www.scala-lang.org/api/2.13.11/scala/Double.html#scala.Double" name="scala.Double" id="scala.Double" class="extype">Double</a>]]</span>, <span name="r">r: <span name="smile.math.TimeFunction" class="extype">TimeFunction</span></span>)</span><span class="result">: <span name="smile.feature.extraction.GHA" class="extype">GHA</span></span></span><p class="shortcomment cmt">Generalized Hebbian Algorithm.</p><div class="fullcomment"><div class="comment cmt"><p>Generalized Hebbian Algorithm. GHA is a linear feed-forward neural
network model for unsupervised learning with applications primarily in
principal components analysis. It is single-layer process -- that is, a
synaptic weight changes only depending on the response of the inputs and
outputs of that layer.</p><p>It guarantees that GHA finds the first k eigenvectors of the covariance matrix,
assuming that the associated eigenvalues are distinct. The convergence theorem
is forumulated in terms of a time-varying learning rate &eta;. In practice, the
learning rate &eta; is chosen to be a small constant, in which case convergence is
guaranteed with mean-squared error in synaptic weights of order &eta;.</p><p>It also has a simple and predictable trade-off between learning speed and
accuracy of convergence as set by the learning rate parameter &eta;. It was
shown that a larger learning rate &eta; leads to faster convergence
and larger asymptotic mean-square error, which is intuitively satisfying.</p><p>Compared to regular batch PCA algorithm based on eigen decomposition, GHA is
an adaptive method and works with an arbitrarily large sample size. The storage
requirement is modest. Another attractive feature is that, in a nonstationary
environment, it has an inherent ability to track gradual changes in the
optimal solution in an inexpensive way.</p><h6>References:</h6><ul><li>Terence D. Sanger. Optimal unsupervised learning in a single-layer linear feedforward neural network. Neural Networks 2(6):459-473, 1989.</li><li>Simon Haykin. Neural Networks: A Comprehensive Foundation (2 ed.). 1998.
</li></ul></div><dl class="paramcmts block"><dt class="param">data</dt><dd class="cmt"><p>training data.</p></dd><dt class="param">w</dt><dd class="cmt"><p>the initial projection matrix.</p></dd><dt class="param">r</dt><dd class="cmt"><p>the learning rate.</p></dd></dl></div></li><li class="indented0 " name="smile.feature.extraction#kpca" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="kpca(data:smile.data.DataFrame,kernel:smile.math.kernel.MercerKernel[Array[Double]],k:Int,threshold:Double):smile.feature.extraction.KernelPCA" class="anchorToMember"></a><a id="kpca(DataFrame,MercerKernel[Array[Double]],Int,Double):KernelPCA" class="anchorToMember"></a> <span class="permalink"><a href="../../../smile/feature/extraction/index.html#kpca(data:smile.data.DataFrame,kernel:smile.math.kernel.MercerKernel[Array[Double]],k:Int,threshold:Double):smile.feature.extraction.KernelPCA" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">kpca</span><span class="params">(<span name="data">data: <span name="smile.data.DataFrame" class="extype">DataFrame</span></span>, <span name="kernel">kernel: <span name="smile.math.kernel.MercerKernel" class="extype">MercerKernel</span>[<a href="https://www.scala-lang.org/api/2.13.11/scala/Array.html#scala.Array" name="scala.Array" id="scala.Array" class="extype">Array</a>[<a href="https://www.scala-lang.org/api/2.13.11/scala/Double.html#scala.Double" name="scala.Double" id="scala.Double" class="extype">Double</a>]]</span>, <span name="k">k: <a href="https://www.scala-lang.org/api/2.13.11/scala/Int.html#scala.Int" name="scala.Int" id="scala.Int" class="extype">Int</a></span>, <span name="threshold">threshold: <a href="https://www.scala-lang.org/api/2.13.11/scala/Double.html#scala.Double" name="scala.Double" id="scala.Double" class="extype">Double</a> = <span class="symbol">0.0001</span></span>)</span><span class="result">: <span name="smile.feature.extraction.KernelPCA" class="extype">KernelPCA</span></span></span><p class="shortcomment cmt">Kernel principal component analysis.</p><div class="fullcomment"><div class="comment cmt"><p>Kernel principal component analysis. Kernel PCA is an extension of
principal component analysis (PCA) using techniques of kernel methods.
Using a kernel, the originally linear operations of PCA are done in a
reproducing kernel Hilbert space with a non-linear mapping.</p><p>In practice, a large data set leads to a large Kernel/Gram matrix K, and
storing K may become a problem. One way to deal with this is to perform
clustering on your large dataset, and populate the kernel with the means
of those clusters. Since even this method may yield a relatively large K,
it is common to compute only the top P eigenvalues and eigenvectors of K.</p><p>Kernel PCA with an isotropic kernel function is closely related to metric MDS.
Carrying out metric MDS on the kernel matrix K produces an equivalent configuration
of points as the distance (2(1 - K(x<sub>i</sub>, x<sub>j</sub>)))<sup>1/2</sup>
computed in feature space.</p><p>Kernel PCA also has close connections with Isomap, LLE, and Laplacian eigenmaps.</p><h6>References:</h6><ul><li>Bernhard Scholkopf, Alexander Smola, and Klaus-Robert Muller. Nonlinear Component Analysis as a Kernel Eigenvalue Problem. Neural Computation, 1998.
</li></ul></div><dl class="paramcmts block"><dt class="param">data</dt><dd class="cmt"><p>training data.</p></dd><dt class="param">kernel</dt><dd class="cmt"><p>Mercer kernel to compute kernel matrix.</p></dd><dt class="param">k</dt><dd class="cmt"><p>choose top k principal components used for projection.</p></dd><dt class="param">threshold</dt><dd class="cmt"><p>only principal components with eigenvalues larger than
                 the given threshold will be kept.</p></dd></dl></div></li><li class="indented0 " name="smile.feature.extraction#pca" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="pca(data:smile.data.DataFrame,cor:Boolean):smile.feature.extraction.PCA" class="anchorToMember"></a><a id="pca(DataFrame,Boolean):PCA" class="anchorToMember"></a> <span class="permalink"><a href="../../../smile/feature/extraction/index.html#pca(data:smile.data.DataFrame,cor:Boolean):smile.feature.extraction.PCA" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">pca</span><span class="params">(<span name="data">data: <span name="smile.data.DataFrame" class="extype">DataFrame</span></span>, <span name="cor">cor: <a href="https://www.scala-lang.org/api/2.13.11/scala/Boolean.html#scala.Boolean" name="scala.Boolean" id="scala.Boolean" class="extype">Boolean</a> = <span class="symbol">false</span></span>)</span><span class="result">: <span name="smile.feature.extraction.PCA" class="extype">PCA</span></span></span><p class="shortcomment cmt">Principal component analysis.</p><div class="fullcomment"><div class="comment cmt"><p>Principal component analysis. PCA is an orthogonal
linear transformation that transforms a number of possibly correlated
variables into a smaller number of uncorrelated variables called principal
components. The first principal component accounts for as much of the
variability in the data as possible, and each succeeding component accounts
for as much of the remaining variability as possible. PCA is theoretically
the optimum transform for given data in least square terms.
PCA can be thought of as revealing the internal structure of the data in
a way which best explains the variance in the data. If a multivariate
dataset is visualized as a set of coordinates in a high-dimensional data
space, PCA supplies the user with a lower-dimensional picture when viewed
from its (in some sense) most informative viewpoint.</p><p>PCA is mostly used as a tool in exploratory data analysis and for making
predictive models. PCA involves the calculation of the eigenvalue
decomposition of a data covariance matrix or singular value decomposition
of a data matrix, usually after mean centering the data for each attribute.
The results of a PCA are usually discussed in terms of component scores and
loadings.</p><p>As a linear technique, PCA is built for several purposes: first, it enables us to
decorrelate the original variables; second, to carry out data compression,
where we pay decreasing attention to the numerical accuracy by which we
encode the sequence of principal components; third, to reconstruct the
original input data using a reduced number of variables according to a
least-squares criterion; and fourth, to identify potential clusters in the data.</p><p>In certain applications, PCA can be misleading. PCA is heavily influenced
when there are outliers in the data. In other situations, the linearity
of PCA may be an obstacle to successful data reduction and compression.
</p></div><dl class="paramcmts block"><dt class="param">data</dt><dd class="cmt"><p>training data. If the sample size
            is larger than the data dimension and cor = false, SVD is employed for
            efficiency. Otherwise, eigen decomposition on covariance or correlation
            matrix is performed.</p></dd><dt class="param">cor</dt><dd class="cmt"><p>true if use correlation matrix instead of covariance matrix if ture.</p></dd></dl></div></li><li class="indented0 " name="smile.feature.extraction#ppca" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="ppca(data:smile.data.DataFrame,k:Int):smile.feature.extraction.ProbabilisticPCA" class="anchorToMember"></a><a id="ppca(DataFrame,Int):ProbabilisticPCA" class="anchorToMember"></a> <span class="permalink"><a href="../../../smile/feature/extraction/index.html#ppca(data:smile.data.DataFrame,k:Int):smile.feature.extraction.ProbabilisticPCA" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">ppca</span><span class="params">(<span name="data">data: <span name="smile.data.DataFrame" class="extype">DataFrame</span></span>, <span name="k">k: <a href="https://www.scala-lang.org/api/2.13.11/scala/Int.html#scala.Int" name="scala.Int" id="scala.Int" class="extype">Int</a></span>)</span><span class="result">: <span name="smile.feature.extraction.ProbabilisticPCA" class="extype">ProbabilisticPCA</span></span></span><p class="shortcomment cmt">Probabilistic principal component analysis.</p><div class="fullcomment"><div class="comment cmt"><p>Probabilistic principal component analysis. PPCA is a simplified factor analysis
that employs a latent variable model with linear relationship:</p><pre>y &amp;sim; W * x + &amp;mu; + &amp;epsilon;</pre><p>where latent variables x &sim; N(0, I), error (or noise) &epsilon; &sim; N(0, &Psi;),
and &mu; is the location term (mean). In PPCA, an isotropic noise model is used,
i.e., noise variances constrained to be equal (&Psi;<sub>i</sub> = &sigma;<sup>2</sup>).
A close form of estimation of above parameters can be obtained
by maximum likelihood method.</p><h6>References:</h6><ul><li>Michael E. Tipping and Christopher M. Bishop. Probabilistic Principal Component Analysis. Journal of the Royal Statistical Society. Series B (Statistical Methodology) 61(3):611-622, 1999.
</li></ul></div><dl class="paramcmts block"><dt class="param">data</dt><dd class="cmt"><p>training data.</p></dd><dt class="param">k</dt><dd class="cmt"><p>the number of principal component to learn.</p></dd></dl></div></li><li class="indented0 " name="smile.feature.extraction.$dummy" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="$dummy" class="anchorToMember"></a><a id="$dummy:$dummy" class="anchorToMember"></a> <span class="permalink"><a href="../../../smile/feature/extraction/package$$$dummy$.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">object</span></span> <span class="symbol"><a href="package$$$dummy$.html" title="Hacking scaladoc issue-8124."><span class="name">$dummy</span></a></span><p class="shortcomment cmt">Hacking scaladoc <a href="https://github.com/scala/bug/issues/8124" target="_blank">issue-8124</a>.</p><div class="fullcomment"><div class="comment cmt"><p>Hacking scaladoc <a href="https://github.com/scala/bug/issues/8124" target="_blank">issue-8124</a>.
The user should ignore this object.</p></div></div></li></ol></div></div><div id="inheritedMembers"><div name="scala.AnyRef" class="parent"><h3>Inherited from <a href="https://www.scala-lang.org/api/2.13.11/scala/AnyRef.html#scala.AnyRef" name="scala.AnyRef" id="scala.AnyRef" class="extype">AnyRef</a></h3></div><div name="scala.Any" class="parent"><h3>Inherited from <a href="https://www.scala-lang.org/api/2.13.11/scala/Any.html#scala.Any" name="scala.Any" id="scala.Any" class="extype">Any</a></h3></div></div><div id="groupedMembers"><div name="Ungrouped" class="group"><h3>Ungrouped</h3></div></div></div><div id="tooltip"></div><div id="footer"></div></body></div></div></div></body></html>
