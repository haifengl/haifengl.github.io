<!DOCTYPE html><html data-pathToRoot="../../" data-rawLocation="smile/feature/extraction" data-dynamicSideMenu="true"><head><meta charset="utf-8"></meta><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"></meta><title>smile.feature.extraction</title><link rel="shortcut icon" type="image/x-icon" href="../../favicon.ico"></link><script type="text/javascript" src="../../scripts/theme.js"></script><script type="text/javascript" src="../../scripts/searchData.js" defer="true"></script><script type="text/javascript" src="../../scripts/scastieConfiguration.js" defer="true"></script><link rel="stylesheet" href="../../styles/theme/bundle.css"></link><link rel="stylesheet" href="../../styles/theme/components/bundle.css"></link><link rel="stylesheet" href="../../styles/theme/components/button/bundle.css"></link><link rel="stylesheet" href="../../styles/theme/layout/bundle.css"></link><link rel="stylesheet" href="../../styles/nord-light.css"></link><link rel="stylesheet" href="../../styles/dotty-icons.css"></link><link rel="stylesheet" href="../../styles/filter-bar.css"></link><link rel="stylesheet" href="../../styles/code-snippets.css"></link><link rel="stylesheet" href="../../styles/searchbar.css"></link><link rel="stylesheet" href="../../styles/social-links.css"></link><link rel="stylesheet" href="../../styles/versions-dropdown.css"></link><link rel="stylesheet" href="../../styles/content-contributors.css"></link><link rel="stylesheet" href="../../styles/fontawesome.css"></link><script type="text/javascript" src="../../hljs/highlight.pack.js" defer="true"></script><script type="text/javascript" src="../../scripts/hljs-scala3.js" defer="true"></script><script type="text/javascript" src="../../scripts/ux.js" defer="true"></script><script type="text/javascript" src="../../scripts/common/component.js" defer="true"></script><script type="text/javascript" src="../../scripts/common/utils.js" defer="true"></script><script type="text/javascript" src="../../scripts/components/FilterBar.js" defer="true"></script><script type="text/javascript" src="../../scripts/components/DocumentableList.js" defer="true"></script><script type="text/javascript" src="../../scripts/components/Input.js" defer="true"></script><script type="text/javascript" src="../../scripts/components/FilterGroup.js" defer="true"></script><script type="text/javascript" src="../../scripts/components/Filter.js" defer="true"></script><script type="text/javascript" src="../../scripts/scaladoc-scalajs.js" defer="true"></script><script type="text/javascript" src="../../scripts/contributors.js" defer="true"></script><script type="text/javascript" src="https://code.jquery.com/jquery-3.5.1.min.js" defer="true"></script><script type="text/javascript" src="https://d3js.org/d3.v6.min.js" defer="true"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/graphlib-dot@0.6.2/dist/graphlib-dot.min.js" defer="true"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/dagre-d3/0.6.1/dagre-d3.min.js" defer="true"></script><script type="text/javascript" src="https://scastie.scala-lang.org/embedded.js" defer="true"></script><script type="text/javascript" src="../../scripts/data.js" defer="true"></script><link rel="stylesheet" href="../../styles/apistyles.css"></link><script>var pathToRoot = "../../";</script></head><body><div id=""><div id="header" class="body-small"><div class="header-container-left"><a href="../../" class="logo-container"><span id="project-logo" class="project-logo"><img src="../../project-logo/smile.jpg"></img></span><span id="dark-project-logo" class="project-logo"><img src="../../project-logo/smile.jpg"></img></span><span class="project-name h300">Smile - Scala</span></a><span onclick="dropdownHandler(event)" class="text-button with-arrow" id="dropdown-trigger"><a><div class="projectVersion">4.3.0</div></a></span><div id="version-dropdown" class="dropdown-menu"></div></div><div class="header-container-right"><button id="search-toggle" class="icon-button"></button><span id="theme-toggle" class="icon-button"></span><span id="mobile-menu-toggle" class="icon-button hamburger"></span></div></div><div id="mobile-menu"><div class="mobile-menu-header body-small"><span class="mobile-menu-logo"><span id="project-logo" class="project-logo"><img src="../../project-logo/smile.jpg"></img></span><span id="dark-project-logo" class="project-logo"><img src="../../project-logo/smile.jpg"></img></span><span class="project-name h300">Smile - Scala</span></span><button id="mobile-menu-close" class="icon-button close"></button></div><div class="mobile-menu-container body-medium"><input id="mobile-scaladoc-searchbar-input" class="scaladoc-searchbar-input" type="search" placeholder="Find anything"></input><span id="mobile-theme-toggle" class="mobile-menu-item mode"></span></div></div><span id="mobile-sidebar-toggle" class="floating-button"></span><div id="leftColumn" class="body-small"></div><div id="footer" class="body-small"><div class="left-container">Generated with</div><div class="right-container"><div class="text">Copyright © 2010-2025 Haifeng Li. All rights reserved.
<script async src="https://www.googletagmanager.com/gtag/js?id=G-57GD08QCML"></script>
<script type="text/javascript" src="/api/java/script-dir/gtag.js"></script>
Use is subject to license terms.</div></div><div class="text-mobile">Copyright © 2010-2025 Haifeng Li. All rights reserved.
Use is subject to license terms.</div></div><div id="scaladoc-searchBar"></div><div id="main"><div class="breadcrumbs container"><a href="../../index.html">Smile - Scala</a>/<a href="../../smile.html">smile</a>/<a href="extraction.html">smile.feature.extraction</a></div><div id="content" class="body-medium"><div><div class="cover-header">
 <span class="icon"><span class="micon pa"></span></span>
 <h1 class="h600 single">smile.feature.extraction</h1>
</div>
<div class="fqname body-large">
 <span></span>
</div>
<div class="main-signature mono-small-block">
 <div class="signature">
  <span class="modifiers"></span><span class="kind"><span t="k">package </span></span><a href="extraction.html" t="n" class="documentableName ">smile.feature.extraction</a>
 </div>
</div>
<div class="cover">
 <div class="doc">
  <p>Feature extraction. Feature extraction transforms the data in the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist.</p>
  <p>The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, the correlation matrix of the data is constructed and the eigenvectors on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behavior of the system. The original space has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors.</p>
  <p>Compared to regular batch PCA algorithm, the generalized Hebbian algorithm is an adaptive method to find the largest k eigenvectors of the covariance matrix, assuming that the associated eigenvalues are distinct. GHA works with an arbitrarily large sample size and the storage requirement is modest. Another attractive feature is that, in a nonstationary environment, it has an inherent ability to track gradual changes in the optimal solution in an inexpensive way.</p>
  <p>Random projection is a promising linear dimensionality reduction technique for learning mixtures of Gaussians. The key idea of random projection arises from the Johnson-Lindenstrauss lemma: if points in a vector space are projected onto a randomly selected subspace of suitably high dimension, then the distances between the points are approximately preserved.</p>
  <p>Principal component analysis can be employed in a nonlinear way by means of the kernel trick. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data. The resulting technique is entitled Kernel PCA. Other prominent nonlinear techniques include manifold learning techniques such as locally linear embedding (LLE), Hessian LLE, Laplacian eigenmaps, and LTSA. These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data, and can be viewed as defining a graph-based kernel for Kernel PCA. More recently, techniques have been proposed that, instead of defining a fixed kernel, try to learn the kernel using semidefinite programming. The most prominent example of such a technique is maximum variance unfolding (MVU). The central idea of MVU is to exactly preserve all pairwise distances between nearest neighbors (in the inner product space), while maximizing the distances between points that are not nearest neighbors.</p>
  <p>An alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces. Important examples of such techniques include classical multidimensional scaling (which is identical to PCA), Isomap (which uses geodesic distances in the data space), diffusion maps (which uses diffusion distances in the data space), t-SNE (which minimizes the divergence between distributions over pairs of points), and curvilinear component analysis.</p>
  <p>A different approach to nonlinear dimensionality reduction is through the use of autoencoders, a special kind of feed-forward neural networks with a bottle-neck hidden layer. The training of deep encoders is typically performed using a greedy layer-wise pre-training (e.g., using a stack of Restricted Boltzmann machines) that is followed by a finetuning stage based on backpropagation.</p>
 </div>
 <section id="attributes">
  <h2 class="h500">Attributes</h2>
  <dl class="attributes"></dl>
 </section>
</div>
<section id="members-list">
 <h2 class="h500">Members list</h2>
 <div class="documentableFilter">
  <div class="filtersContainer"></div><input class="filterableInput" placeholder="Filter by any phrase" data-test-id="filterBarInput" type="search"><button class="clearButton label-only-button" data-test-id="filterBarClearButton">Clear all</button>
 </div>
 <div class="membersList expand">
  <div class="tabs">
   <div class="contents">
    <section id="Type-members">
     <div data-togglable="Type members" class="tab expand">
      <div class="member-group-header">
       <h3 data-togglable="Type members" class="h400">Type members</h3>
      </div>
      <section id="Classlikes">
       <div class="documentableList expand">
        <div class="documentableList-expander">
         <button class="icon-button show-content expand"></button>
         <h4 class="groupHeader h200">Classlikes</h4>
        </div>
        <div class="documentableElement">
         <div class="documentableElement-expander">
          <button class="icon-button ar show-content"></button>
          <div class="header monospace mono-medium">
           <div class="signature">
            <span class="modifiers"></span><span class="kind"><span t="k">object </span></span><a href="extraction/$dummy$.html" t="n" class="documentableName ">$dummy</a>
           </div>
          </div>
         </div>
         <div class="docs">
          <span class="modifiers"></span>
          <div>
           <div class="originInfo"></div>
           <div class="memberDocumentation">
            <div class="documentableBrief doc">
             <p>Hacking scaladoc <a href="https://github.com/scala/bug/issues/8124">issue-8124</a>. The user should ignore this object.</p>
            </div>
            <div class="cover">
             <div class="doc">
              <p>Hacking scaladoc <a href="https://github.com/scala/bug/issues/8124">issue-8124</a>. The user should ignore this object.</p>
             </div>
             <h2 class="h200">Attributes</h2>
             <dl class="attributes attributes-small">
              <dt class="body-small">
               Supertypes
              </dt>
              <dd class="body-medium">
               <div class="mono-small-block supertypes">
                <div>
                 class <span data-unresolved-link="" t="n">Object</span>
                </div>
                <div>
                 trait <span data-unresolved-link="" t="n">Matchable</span>
                </div>
                <div>
                 class <span data-unresolved-link="" t="n">Any</span>
                </div><span></span>
               </div>
              </dd>
              <dt class="body-small">
               Self type
              </dt>
              <dd class="body-medium">
               <div class="mono-small-block supertypes">
                <span></span>
                <div>
                 <a href="extraction/$dummy$.html" t="n">$dummy</a>.<span t="k">type</span>
                </div>
               </div>
              </dd>
             </dl>
            </div>
           </div>
          </div>
         </div>
        </div>
       </div>
      </section>
     </div>
    </section>
    <section id="Value-members">
     <div data-togglable="Value members" class="tab expand">
      <div class="member-group-header">
       <h3 data-togglable="Value members" class="h400">Value members</h3>
      </div>
      <section id="Concrete-methods">
       <div class="documentableList expand">
        <div class="documentableList-expander">
         <button class="icon-button show-content expand"></button>
         <h4 class="groupHeader h200">Concrete methods</h4>
        </div>
        <div class="documentableElement" id="gha-fffff6c6">
         <div class="documentableElement-expander">
          <button class="icon-button ar show-content"></button>
          <div class="header monospace mono-medium">
           <div class="signature">
            <span class="modifiers"></span><span class="kind"><span t="k">def </span></span><a href="extraction.html#gha-fffff6c6" t="n" class="documentableName ">gha</a>(<span t="k"></span><span t="k"></span><span data-unresolved-link="" t="n">data</span>: <a href="https://www.scala-lang.org/api/2.13.16/scala/Array.html" t="t">Array</a>[<a href="https://www.scala-lang.org/api/2.13.16/scala/Array.html" t="t">Array</a>[<a href="https://www.scala-lang.org/api/2.13.16/scala/Double.html" t="t">Double</a>]], <span t="k"></span><span data-unresolved-link="" t="n">w</span>: <a href="https://www.scala-lang.org/api/2.13.16/scala/Array.html" t="t">Array</a>[<a href="https://www.scala-lang.org/api/2.13.16/scala/Array.html" t="t">Array</a>[<a href="https://www.scala-lang.org/api/2.13.16/scala/Double.html" t="t">Double</a>]], <span t="k"></span><span data-unresolved-link="" t="n">r</span>: <span data-unresolved-link="" t="t">TimeFunction</span>): <span data-unresolved-link="" t="t">GHA</span>
           </div>
          </div>
         </div>
         <div class="docs">
          <span class="modifiers"></span>
          <div>
           <div class="originInfo"></div>
           <div class="memberDocumentation">
            <div class="documentableBrief doc">
             <p>Generalized Hebbian Algorithm. GHA is a linear feed-forward neural network model for unsupervised learning with applications primarily in principal components analysis. It is single-layer process -- that is, a synaptic weight changes only depending on the response of the inputs and outputs of that layer.</p>
            </div>
            <div class="cover">
             <div class="doc">
              <p>Generalized Hebbian Algorithm. GHA is a linear feed-forward neural network model for unsupervised learning with applications primarily in principal components analysis. It is single-layer process -- that is, a synaptic weight changes only depending on the response of the inputs and outputs of that layer.</p>
              <p>It guarantees that GHA finds the first k eigenvectors of the covariance matrix, assuming that the associated eigenvalues are distinct. The convergence theorem is formulated in terms of a time-varying learning rate η. In practice, the learning rate η is chosen to be a small constant, in which case convergence is guaranteed with mean-squared error in synaptic weights of order η.</p>
              <p>It also has a simple and predictable trade-off between learning speed and accuracy of convergence as set by the learning rate parameter η. It was shown that a larger learning rate η leads to faster convergence and larger asymptotic mean-square error, which is intuitively satisfying.</p>
              <p>Compared to regular batch PCA algorithm based on eigen decomposition, GHA is an adaptive method and works with an arbitrarily large sample size. The storage requirement is modest. Another attractive feature is that, in a nonstationary environment, it has an inherent ability to track gradual changes in the optimal solution in an inexpensive way.</p>
              <p>====References:====</p>
              <ul>
               <li>Terence D. Sanger. Optimal unsupervised learning in a single-layer linear feedforward neural network. Neural Networks 2(6):459-473, 1989.</li>
               <li>Simon Haykin. Neural Networks: A Comprehensive Foundation (2 ed.). 1998.</li>
              </ul>
             </div>
             <h2 class="h200">Value parameters</h2>
             <dl class="attributes attributes-small">
              <dt class="body-small">
               data
              </dt>
              <dd class="body-medium">
               <p>training data.</p>
              </dd>
              <dt class="body-small">
               r
              </dt>
              <dd class="body-medium">
               <p>the learning rate.</p>
              </dd>
              <dt class="body-small">
               w
              </dt>
              <dd class="body-medium">
               <p>the initial projection matrix.</p>
              </dd>
             </dl>
             <h2 class="h200">Attributes</h2>
             <dl class="attributes attributes-small"></dl>
            </div>
           </div>
          </div>
         </div>
        </div>
        <div class="documentableElement" id="gha-fffffe76">
         <div class="documentableElement-expander">
          <button class="icon-button ar show-content"></button>
          <div class="header monospace mono-medium">
           <div class="signature">
            <span class="modifiers"></span><span class="kind"><span t="k">def </span></span><a href="extraction.html#gha-fffffe76" t="n" class="documentableName ">gha</a>(<span t="k"></span><span t="k"></span><span data-unresolved-link="" t="n">data</span>: <a href="https://www.scala-lang.org/api/2.13.16/scala/Array.html" t="t">Array</a>[<a href="https://www.scala-lang.org/api/2.13.16/scala/Array.html" t="t">Array</a>[<a href="https://www.scala-lang.org/api/2.13.16/scala/Double.html" t="t">Double</a>]], <span t="k"></span><span data-unresolved-link="" t="n">k</span>: <a href="https://www.scala-lang.org/api/2.13.16/scala/Int.html" t="t">Int</a>, <span t="k"></span><span data-unresolved-link="" t="n">r</span>: <span data-unresolved-link="" t="t">TimeFunction</span>): <span data-unresolved-link="" t="t">GHA</span>
           </div>
          </div>
         </div>
         <div class="docs">
          <span class="modifiers"></span>
          <div>
           <div class="originInfo"></div>
           <div class="memberDocumentation">
            <div class="documentableBrief doc">
             <p>Generalized Hebbian Algorithm with random initial projection matrix.</p>
            </div>
            <div class="cover">
             <div class="doc">
              <p>Generalized Hebbian Algorithm with random initial projection matrix.</p>
             </div>
             <h2 class="h200">Value parameters</h2>
             <dl class="attributes attributes-small">
              <dt class="body-small">
               data
              </dt>
              <dd class="body-medium">
               <p>training data.</p>
              </dd>
              <dt class="body-small">
               k
              </dt>
              <dd class="body-medium">
               <p>the dimension of feature space.</p>
              </dd>
              <dt class="body-small">
               r
              </dt>
              <dd class="body-medium">
               <p>the learning rate.</p>
              </dd>
             </dl>
             <h2 class="h200">Attributes</h2>
             <dl class="attributes attributes-small"></dl>
            </div>
           </div>
          </div>
         </div>
        </div>
        <div class="documentableElement" id="kpca-fffff93f">
         <div class="documentableElement-expander">
          <button class="icon-button ar show-content"></button>
          <div class="header monospace mono-medium">
           <div class="signature">
            <span class="modifiers"></span><span class="kind"><span t="k">def </span></span><a href="extraction.html#kpca-fffff93f" t="n" class="documentableName ">kpca</a>(<span t="k"></span><span t="k"></span><span data-unresolved-link="" t="n">data</span>: <span data-unresolved-link="" t="t">DataFrame</span>, <span t="k"></span><span data-unresolved-link="" t="n">kernel</span>: <span data-unresolved-link="" t="t">MercerKernel</span>[<a href="https://www.scala-lang.org/api/2.13.16/scala/Array.html" t="t">Array</a>[<a href="https://www.scala-lang.org/api/2.13.16/scala/Double.html" t="t">Double</a>]], <span t="k"></span><span data-unresolved-link="" t="n">k</span>: <a href="https://www.scala-lang.org/api/2.13.16/scala/Int.html" t="t">Int</a>, <span t="k"></span><span data-unresolved-link="" t="n">threshold</span>: <a href="https://www.scala-lang.org/api/2.13.16/scala/Double.html" t="t">Double</a>): <span data-unresolved-link="" t="t">KernelPCA</span>
           </div>
          </div>
         </div>
         <div class="docs">
          <span class="modifiers"></span>
          <div>
           <div class="originInfo"></div>
           <div class="memberDocumentation">
            <div class="documentableBrief doc">
             <p>Kernel principal component analysis. Kernel PCA is an extension of principal component analysis (PCA) using techniques of kernel methods. Using a kernel, the originally linear operations of PCA are done in a reproducing kernel Hilbert space with a non-linear mapping.</p>
            </div>
            <div class="cover">
             <div class="doc">
              <p>Kernel principal component analysis. Kernel PCA is an extension of principal component analysis (PCA) using techniques of kernel methods. Using a kernel, the originally linear operations of PCA are done in a reproducing kernel Hilbert space with a non-linear mapping.</p>
              <p>In practice, a large data set leads to a large Kernel/Gram matrix K, and storing K may become a problem. One way to deal with this is to perform clustering on your large dataset, and populate the kernel with the means of those clusters. Since even this method may yield a relatively large K, it is common to compute only the top P eigenvalues and eigenvectors of K.</p>
              <p>Kernel PCA with an isotropic kernel function is closely related to metric MDS. Carrying out metric MDS on the kernel matrix K produces an equivalent configuration of points as the distance (2(1 - K(x<sub>i</sub>, x<sub>j</sub>)))<sup>1/2</sup> computed in feature space.</p>
              <p>Kernel PCA also has close connections with Isomap, LLE, and Laplacian eigenmaps.</p>
              <p>====References:====</p>
              <ul>
               <li>Bernhard Scholkopf, Alexander Smola, and Klaus-Robert Muller. Nonlinear Component Analysis as a Kernel Eigenvalue Problem. Neural Computation, 1998.</li>
              </ul>
             </div>
             <h2 class="h200">Value parameters</h2>
             <dl class="attributes attributes-small">
              <dt class="body-small">
               data
              </dt>
              <dd class="body-medium">
               <p>training data.</p>
              </dd>
              <dt class="body-small">
               k
              </dt>
              <dd class="body-medium">
               <p>choose top k principal components used for projection.</p>
              </dd>
              <dt class="body-small">
               kernel
              </dt>
              <dd class="body-medium">
               <p>Mercer kernel to compute kernel matrix.</p>
              </dd>
              <dt class="body-small">
               threshold
              </dt>
              <dd class="body-medium">
               <p>only principal components with eigenvalues larger than the given threshold will be kept.</p>
              </dd>
             </dl>
             <h2 class="h200">Attributes</h2>
             <dl class="attributes attributes-small"></dl>
            </div>
           </div>
          </div>
         </div>
        </div>
        <div class="documentableElement" id="pca-8a0">
         <div class="documentableElement-expander">
          <button class="icon-button ar show-content"></button>
          <div class="header monospace mono-medium">
           <div class="signature">
            <span class="modifiers"></span><span class="kind"><span t="k">def </span></span><a href="extraction.html#pca-8a0" t="n" class="documentableName ">pca</a>(<span t="k"></span><span t="k"></span><span data-unresolved-link="" t="n">data</span>: <span data-unresolved-link="" t="t">DataFrame</span>, <span t="k"></span><span data-unresolved-link="" t="n">cor</span>: <a href="https://www.scala-lang.org/api/2.13.16/scala/Boolean.html" t="t">Boolean</a>): <span data-unresolved-link="" t="t">PCA</span>
           </div>
          </div>
         </div>
         <div class="docs">
          <span class="modifiers"></span>
          <div>
           <div class="originInfo"></div>
           <div class="memberDocumentation">
            <div class="documentableBrief doc">
             <p>Principal component analysis. PCA is an orthogonal linear transformation that transforms a number of possibly correlated variables into a smaller number of uncorrelated variables called principal components. The first principal component accounts for as much of the variability in the data as possible, and each succeeding component accounts for as much of the remaining variability as possible. PCA is theoretically the optimum transform for given data in least square terms. PCA can be thought of as revealing the internal structure of the data in a way which best explains the variance in the data. If a multivariate dataset is visualized as a set of coordinates in a high-dimensional data space, PCA supplies the user with a lower-dimensional picture when viewed from its (in some sense) most informative viewpoint.</p>
            </div>
            <div class="cover">
             <div class="doc">
              <p>Principal component analysis. PCA is an orthogonal linear transformation that transforms a number of possibly correlated variables into a smaller number of uncorrelated variables called principal components. The first principal component accounts for as much of the variability in the data as possible, and each succeeding component accounts for as much of the remaining variability as possible. PCA is theoretically the optimum transform for given data in least square terms. PCA can be thought of as revealing the internal structure of the data in a way which best explains the variance in the data. If a multivariate dataset is visualized as a set of coordinates in a high-dimensional data space, PCA supplies the user with a lower-dimensional picture when viewed from its (in some sense) most informative viewpoint.</p>
              <p>PCA is mostly used as a tool in exploratory data analysis and for making predictive models. PCA involves the calculation of the eigenvalue decomposition of a data covariance matrix or singular value decomposition of a data matrix, usually after mean centering the data for each attribute. The results of a PCA are usually discussed in terms of component scores and loadings.</p>
              <p>As a linear technique, PCA is built for several purposes: first, it enables us to decorrelate the original variables; second, to carry out data compression, where we pay decreasing attention to the numerical accuracy by which we encode the sequence of principal components; third, to reconstruct the original input data using a reduced number of variables according to a least-squares criterion; and fourth, to identify potential clusters in the data.</p>
              <p>In certain applications, PCA can be misleading. PCA is heavily influenced when there are outliers in the data. In other situations, the linearity of PCA may be an obstacle to successful data reduction and compression.</p>
             </div>
             <h2 class="h200">Value parameters</h2>
             <dl class="attributes attributes-small">
              <dt class="body-small">
               cor
              </dt>
              <dd class="body-medium">
               <p>true if use correlation matrix instead of covariance matrix if ture.</p>
              </dd>
              <dt class="body-small">
               data
              </dt>
              <dd class="body-medium">
               <p>training data. If the sample size is larger than the data dimension and cor = false, SVD is employed for efficiency. Otherwise, eigen decomposition on covariance or correlation matrix is performed.</p>
              </dd>
             </dl>
             <h2 class="h200">Attributes</h2>
             <dl class="attributes attributes-small"></dl>
            </div>
           </div>
          </div>
         </div>
        </div>
        <div class="documentableElement" id="ppca-bea">
         <div class="documentableElement-expander">
          <button class="icon-button ar show-content"></button>
          <div class="header monospace mono-medium">
           <div class="signature">
            <span class="modifiers"></span><span class="kind"><span t="k">def </span></span><a href="extraction.html#ppca-bea" t="n" class="documentableName ">ppca</a>(<span t="k"></span><span t="k"></span><span data-unresolved-link="" t="n">data</span>: <span data-unresolved-link="" t="t">DataFrame</span>, <span t="k"></span><span data-unresolved-link="" t="n">k</span>: <a href="https://www.scala-lang.org/api/2.13.16/scala/Int.html" t="t">Int</a>): <span data-unresolved-link="" t="t">ProbabilisticPCA</span>
           </div>
          </div>
         </div>
         <div class="docs">
          <span class="modifiers"></span>
          <div>
           <div class="originInfo"></div>
           <div class="memberDocumentation">
            <div class="documentableBrief doc">
             <p>Probabilistic principal component analysis. PPCA is a simplified factor analysis that employs a latent variable model with linear relationship:</p>
            </div>
            <div class="cover">
             <div class="doc">
              <p>Probabilistic principal component analysis. PPCA is a simplified factor analysis that employs a latent variable model with linear relationship:</p>
              <div class="snippet mono-small-block">
               <pre><code class="language-"><span line-number="1" class=""><span class="tooltip-container"></span><span class="hideable">   </span>y &amp;sim; W * x + &amp;mu; + &amp;epsilon;
</span></code></pre>
               <div class="buttons"></div>
              </div>
              <p>where latent variables x ∼ N(0, I), error (or noise) ε ∼ N(0, Ψ), and μ is the location term (mean). In PPCA, an isotropic noise model is used, i.e., noise variances constrained to be equal (Ψ<sub>i</sub> = σ<sup>2</sup>). A close form of estimation of above parameters can be obtained by maximum likelihood method.</p>
              <p>====References:====</p>
              <ul>
               <li>Michael E. Tipping and Christopher M. Bishop. Probabilistic Principal Component Analysis. Journal of the Royal Statistical Society. Series B (Statistical Methodology) 61(3):611-622, 1999.</li>
              </ul>
             </div>
             <h2 class="h200">Value parameters</h2>
             <dl class="attributes attributes-small">
              <dt class="body-small">
               data
              </dt>
              <dd class="body-medium">
               <p>training data.</p>
              </dd>
              <dt class="body-small">
               k
              </dt>
              <dd class="body-medium">
               <p>the number of principal component to learn.</p>
              </dd>
             </dl>
             <h2 class="h200">Attributes</h2>
             <dl class="attributes attributes-small"></dl>
            </div>
           </div>
          </div>
         </div>
        </div>
       </div>
      </section>
     </div>
    </section>
   </div>
  </div>
 </div>
</section></div><div id="toc" class="body-small"><div id="toc-container"><span class="toc-title h200">In this article</span><nav class="toc-nav"><ul class="toc-list"><li><a href="#attributes">Attributes</a></li><li><a href="#members-list">Members list</a><ul><li><a href="#Type-members">Type members</a><ul><li><a href="#Classlikes">Classlikes</a></li></ul></li><li><a href="#Value-members">Value members</a><ul><li><a href="#Concrete-methods">Concrete methods</a></li></ul></li></ul></li></ul></nav></div></div></div><div id="footer" class="body-small mobile-footer"><div class="left-container">Generated with</div><div class="right-container"><div class="text">Copyright © 2010-2025 Haifeng Li. All rights reserved.
Use is subject to license terms.</div></div><div class="text-mobile">Copyright © 2010-2025 Haifeng Li. All rights reserved.
Use is subject to license terms.</div></div></div></div></body></html>