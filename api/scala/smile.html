<!DOCTYPE html><html data-pathToRoot="" data-rawLocation="smile" data-dynamicSideMenu="true"><head><meta charset="utf-8"></meta><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"></meta><title>smile</title><link rel="shortcut icon" type="image/x-icon" href="favicon.ico"></link><script type="text/javascript" src="scripts/theme.js"></script><script type="text/javascript" src="scripts/searchData.js" defer="true"></script><script type="text/javascript" src="scripts/scastieConfiguration.js" defer="true"></script><link rel="stylesheet" href="styles/theme/bundle.css"></link><link rel="stylesheet" href="styles/theme/components/bundle.css"></link><link rel="stylesheet" href="styles/theme/components/button/bundle.css"></link><link rel="stylesheet" href="styles/theme/layout/bundle.css"></link><link rel="stylesheet" href="styles/nord-light.css"></link><link rel="stylesheet" href="styles/dotty-icons.css"></link><link rel="stylesheet" href="styles/filter-bar.css"></link><link rel="stylesheet" href="styles/code-snippets.css"></link><link rel="stylesheet" href="styles/searchbar.css"></link><link rel="stylesheet" href="styles/social-links.css"></link><link rel="stylesheet" href="styles/versions-dropdown.css"></link><link rel="stylesheet" href="styles/content-contributors.css"></link><link rel="stylesheet" href="styles/fontawesome.css"></link><script type="text/javascript" src="hljs/highlight.pack.js" defer="true"></script><script type="text/javascript" src="scripts/hljs-scala3.js" defer="true"></script><script type="text/javascript" src="scripts/ux.js" defer="true"></script><script type="text/javascript" src="scripts/common/component.js" defer="true"></script><script type="text/javascript" src="scripts/common/utils.js" defer="true"></script><script type="text/javascript" src="scripts/components/FilterBar.js" defer="true"></script><script type="text/javascript" src="scripts/components/DocumentableList.js" defer="true"></script><script type="text/javascript" src="scripts/components/Input.js" defer="true"></script><script type="text/javascript" src="scripts/components/FilterGroup.js" defer="true"></script><script type="text/javascript" src="scripts/components/Filter.js" defer="true"></script><script type="text/javascript" src="scripts/scaladoc-scalajs.js" defer="true"></script><script type="text/javascript" src="scripts/contributors.js" defer="true"></script><script type="text/javascript" src="https://code.jquery.com/jquery-3.5.1.min.js" defer="true"></script><script type="text/javascript" src="https://d3js.org/d3.v6.min.js" defer="true"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/graphlib-dot@0.6.2/dist/graphlib-dot.min.js" defer="true"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/dagre-d3/0.6.1/dagre-d3.min.js" defer="true"></script><script type="text/javascript" src="https://scastie.scala-lang.org/embedded.js" defer="true"></script><script type="text/javascript" src="scripts/data.js" defer="true"></script><link rel="stylesheet" href="styles/apistyles.css"></link><script>var pathToRoot = "";</script></head><body><div id=""><div id="header" class="body-small"><div class="header-container-left"><a href="" class="logo-container"><span id="project-logo" class="project-logo"><img src="project-logo/smile.jpg"></img></span><span id="dark-project-logo" class="project-logo"><img src="project-logo/smile.jpg"></img></span><span class="project-name h300">Smile - Scala</span></a><span onclick="dropdownHandler(event)" class="text-button with-arrow" id="dropdown-trigger"><a><div class="projectVersion">4.3.0</div></a></span><div id="version-dropdown" class="dropdown-menu"></div></div><div class="header-container-right"><button id="search-toggle" class="icon-button"></button><span id="theme-toggle" class="icon-button"></span><span id="mobile-menu-toggle" class="icon-button hamburger"></span></div></div><div id="mobile-menu"><div class="mobile-menu-header body-small"><span class="mobile-menu-logo"><span id="project-logo" class="project-logo"><img src="project-logo/smile.jpg"></img></span><span id="dark-project-logo" class="project-logo"><img src="project-logo/smile.jpg"></img></span><span class="project-name h300">Smile - Scala</span></span><button id="mobile-menu-close" class="icon-button close"></button></div><div class="mobile-menu-container body-medium"><input id="mobile-scaladoc-searchbar-input" class="scaladoc-searchbar-input" type="search" placeholder="Find anything"></input><span id="mobile-theme-toggle" class="mobile-menu-item mode"></span></div></div><span id="mobile-sidebar-toggle" class="floating-button"></span><div id="leftColumn" class="body-small"></div><div id="footer" class="body-small"><div class="left-container">Generated with</div><div class="right-container"><div class="text">Copyright © 2010-2025 Haifeng Li. All rights reserved.
<script async src="https://www.googletagmanager.com/gtag/js?id=G-57GD08QCML"></script>
<script type="text/javascript" src="/api/java/script-dir/gtag.js"></script>
Use is subject to license terms.</div></div><div class="text-mobile">Copyright © 2010-2025 Haifeng Li. All rights reserved.
Use is subject to license terms.</div></div><div id="scaladoc-searchBar"></div><div id="main"><div class="breadcrumbs container"><a href="index.html">Smile - Scala</a>/<a href="smile.html">smile</a></div><div id="content" class="body-medium"><div><div class="cover-header">
 <span class="icon"><span class="micon pa"></span></span>
 <h1 class="h600 single">smile</h1>
</div>
<div class="fqname body-large">
 <span></span>
</div>
<div class="main-signature mono-small-block">
 <div class="signature">
  <span class="modifiers"></span><span class="kind"><span t="k">package </span></span><a href="smile.html" t="n" class="documentableName ">smile</a>
 </div>
</div>
<section id="members-list">
 <h2 class="h500">Members list</h2>
 <div class="documentableFilter">
  <div class="filtersContainer"></div><input class="filterableInput" placeholder="Filter by any phrase" data-test-id="filterBarInput" type="search"><button class="clearButton label-only-button" data-test-id="filterBarClearButton">Clear all</button>
 </div>
 <div class="membersList expand">
  <div class="tabs">
   <div class="contents">
    <section id="Packages">
     <div data-togglable="packages" class="tab expand">
      <div class="documentableList expand">
       <div class="documentableList-expander">
        <button class="icon-button show-content expand"></button>
        <h3 class="groupHeader h400">Packages</h3>
       </div>
       <div class="documentableElement">
        <div class="documentableElement-expander">
         <button class="icon-button ar show-content"></button>
         <div class="header monospace mono-medium">
          <div class="signature">
           <span class="modifiers"></span><span class="kind"><span t="k">package </span></span><a href="smile/association.html" t="n" class="documentableName ">smile.association</a>
          </div>
         </div>
        </div>
        <div class="docs">
         <span class="modifiers"></span>
         <div>
          <div class="originInfo"></div>
          <div class="memberDocumentation">
           <div class="documentableBrief doc">
            <p>Frequent item set mining and association rule mining. Association rule learning is a popular and well researched method for discovering interesting relations between variables in large databases. Let I = {i<sub>1</sub>, i<sub>2</sub>,..., i<sub>n</sub>} be a set of n binary attributes called items. Let D = {t<sub>1</sub>, t<sub>2</sub>,..., t<sub>m</sub>} be a set of transactions called the database. Each transaction in D has a unique transaction ID and contains a subset of the items in I. An association rule is defined as an implication of the form X ⇒ Y where X, Y ⊆ I and X ∩ Y = Ø. The item sets X and Y are called antecedent (left-hand-side or LHS) and consequent (right-hand-side or RHS) of the rule, respectively. The support supp(X) of an item set X is defined as the proportion of transactions in the database which contain the item set. Note that the support of an association rule X ⇒ Y is supp(X ∪ Y). The confidence of a rule is defined conf(X ⇒ Y) = supp(X ∪ Y) / supp(X). Confidence can be interpreted as an estimate of the probability P(Y | X), the probability of finding the RHS of the rule in transactions under the condition that these transactions also contain the LHS.</p>
           </div>
           <div class="cover">
            <div class="doc">
             <p>Frequent item set mining and association rule mining. Association rule learning is a popular and well researched method for discovering interesting relations between variables in large databases. Let I = {i<sub>1</sub>, i<sub>2</sub>,..., i<sub>n</sub>} be a set of n binary attributes called items. Let D = {t<sub>1</sub>, t<sub>2</sub>,..., t<sub>m</sub>} be a set of transactions called the database. Each transaction in D has a unique transaction ID and contains a subset of the items in I. An association rule is defined as an implication of the form X ⇒ Y where X, Y ⊆ I and X ∩ Y = Ø. The item sets X and Y are called antecedent (left-hand-side or LHS) and consequent (right-hand-side or RHS) of the rule, respectively. The support supp(X) of an item set X is defined as the proportion of transactions in the database which contain the item set. Note that the support of an association rule X ⇒ Y is supp(X ∪ Y). The confidence of a rule is defined conf(X ⇒ Y) = supp(X ∪ Y) / supp(X). Confidence can be interpreted as an estimate of the probability P(Y | X), the probability of finding the RHS of the rule in transactions under the condition that these transactions also contain the LHS.</p>
             <p>For example, the rule {onions, potatoes} ⇒ {burger} found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, he or she is likely to also buy burger. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements.</p>
             <p>Association rules are usually required to satisfy a user-specified minimum support and a user-specified minimum confidence at the same time. Association rule generation is usually split up into two separate steps:</p>
             <ul>
              <li>First, minimum support is applied to find all frequent item sets in a database (i.e. frequent item set mining).</li>
              <li>Second, these frequent item sets and the minimum confidence constraint are used to form rules.</li>
             </ul>
             <p>Finding all frequent item sets in a database is difficult since it involves searching all possible item sets (item combinations). The set of possible item sets is the power set over I (the set of items) and has size 2<sup>n</sup> - 1 (excluding the empty set which is not a valid item set). Although the size of the power set grows exponentially in the number of items n in I, efficient search is possible using the downward-closure property of support (also called anti-monotonicity) which guarantees that for a frequent item set also all its subsets are frequent and thus for an infrequent item set, all its supersets must be infrequent.</p>
             <p>In practice, we may only consider the frequent item set that has the maximum number of items bypassing all the sub item sets. An item set is maximal frequent if none of its immediate supersets is frequent.</p>
             <p>For a maximal frequent item set, even though we know that all the sub item sets are frequent, we don't know the actual support of those sub item sets, which are very important to find the association rules within the item sets. If the final goal is association rule mining, we would like to discover closed frequent item sets. An item set is closed if none of its immediate supersets has the same support as the item set.</p>
             <p>Some well known algorithms of frequent item set mining are Apriori, Eclat and FP-Growth. Apriori is the best-known algorithm to mine association rules. It uses a breadth-first search strategy to counting the support of item sets and uses a candidate generation function which exploits the downward closure property of support. Eclat is a depth-first search algorithm using set intersection.</p>
             <p>FP-growth (frequent pattern growth) uses an extended prefix-tree (FP-tree) structure to store the database in a compressed form. FP-growth adopts a divide-and-conquer approach to decompose both the mining tasks and the databases. It uses a pattern fragment growth method to avoid the costly process of candidate generation and testing used by Apriori.</p>
             <p>====References:====</p>
             <ul>
              <li>R. Agrawal, T. Imielinski and A. Swami. Mining Association Rules Between Sets of Items in Large Databases, SIGMOD, 207-216, 1993.</li>
              <li>Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules in large databases. VLDB, 487-499, 1994.</li>
              <li>Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12(3):372-390, 2000.</li>
              <li>Jiawei Han, Jian Pei, Yiwen Yin, and Runying Mao. Mining frequent patterns without candidate generation. Data Mining and Knowledge Discovery 8:53-87, 2004.</li>
             </ul>
            </div>
            <h2 class="h200">Attributes</h2>
            <dl class="attributes attributes-small"></dl>
           </div>
          </div>
         </div>
        </div>
       </div>
       <div class="documentableElement">
        <div class="documentableElement-expander">
         <button class="icon-button ar show-content"></button>
         <div class="header monospace mono-medium">
          <div class="signature">
           <span class="modifiers"></span><span class="kind"><span t="k">package </span></span><a href="smile/cas.html" t="n" class="documentableName ">smile.cas</a>
          </div>
         </div>
        </div>
        <div class="docs">
         <span class="modifiers"></span>
         <div>
          <div class="originInfo"></div>
          <div class="memberDocumentation">
           <div class="documentableBrief doc">
            <p>Computer algebra system. A computer algebra system (CAS) has the ability to manipulate mathematical expressions in a way similar to the traditional manual computations of mathematicians and scientists.</p>
           </div>
           <div class="cover">
            <div class="doc">
             <p>Computer algebra system. A computer algebra system (CAS) has the ability to manipulate mathematical expressions in a way similar to the traditional manual computations of mathematicians and scientists.</p>
             <p>The symbolic manipulations supported include:</p>
             <ul>
              <li>
               <p>simplification to a smaller expression or some standard form, including automatic simplification with assumptions and simplification with constraints</p></li>
              <li>
               <p>substitution of symbols or numeric values for certain expressions</p></li>
              <li>
               <p>change of form of expressions: expanding products and powers, partial and full factorization, rewriting as partial fractions, constraint satisfaction, rewriting trigonometric functions as exponentials, transforming logic expressions, etc.</p></li>
              <li>
               <p>partial and total differentiation</p></li>
              <li>
               <p>matrix operations including products, inverses, etc.</p></li>
             </ul>
            </div>
            <h2 class="h200">Attributes</h2>
            <dl class="attributes attributes-small"></dl>
           </div>
          </div>
         </div>
        </div>
       </div>
       <div class="documentableElement">
        <div class="documentableElement-expander">
         <button class="icon-button ar show-content"></button>
         <div class="header monospace mono-medium">
          <div class="signature">
           <span class="modifiers"></span><span class="kind"><span t="k">package </span></span><a href="smile/classification.html" t="n" class="documentableName ">smile.classification</a>
          </div>
         </div>
        </div>
        <div class="docs">
         <span class="modifiers"></span>
         <div>
          <div class="originInfo"></div>
          <div class="memberDocumentation">
           <div class="documentableBrief doc">
            <p>Classification algorithms. In machine learning and pattern recognition, classification refers to an algorithmic procedure for assigning a given input object into one of a given number of categories. The input object is formally termed an instance, and the categories are termed classes.</p>
           </div>
           <div class="cover">
            <div class="doc">
             <p>Classification algorithms. In machine learning and pattern recognition, classification refers to an algorithmic procedure for assigning a given input object into one of a given number of categories. The input object is formally termed an instance, and the categories are termed classes.</p>
             <p>The instance is usually described by a vector of features, which together constitute a description of all known characteristics of the instance. Typically, features are either categorical (also known as nominal, i.e. consisting of one of a set of unordered items, such as a gender of "male" or "female", or a blood type of "A", "B", "AB" or "O"), ordinal (consisting of one of a set of ordered items, e.g. "large", "medium" or "small"), integer-valued (e.g. a count of the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure).</p>
             <p>Classification normally refers to a supervised procedure, i.e. a procedure that produces an inferred function to predict the output value of new instances based on a training set of pairs consisting of an input object and a desired output value. The inferred function is called a classifier if the output is discrete or a regression function if the output is continuous.</p>
             <p>The inferred function should predict the correct output value for any valid input object. This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way.</p>
             <p>A wide range of supervised learning algorithms is available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems. The most widely used learning algorithms are AdaBoost and gradient boosting, support vector machines, linear regression, linear discriminant analysis, logistic regression, naive Bayes, decision trees, k-nearest neighbor algorithm, and neural networks (multilayer perceptron).</p>
             <p>If the feature vectors include features of many different kinds (discrete, discrete ordered, counts, continuous values), some algorithms cannot be easily applied. Many algorithms, including linear regression, logistic regression, neural networks, and nearest neighbor methods, require that the input features be numerical and scaled to similar ranges (e.g., to the [-1,1] interval). Methods that employ a distance function, such as nearest neighbor methods and support vector machines with Gaussian kernels, are particularly sensitive to this. An advantage of decision trees (and boosting algorithms based on decision trees) is that they easily handle heterogeneous data.</p>
             <p>If the input features contain redundant information (e.g., highly correlated features), some learning algorithms (e.g., linear regression, logistic regression, and distance based methods) will perform poorly because of numerical instabilities. These problems can often be solved by imposing some form of regularization.</p>
             <p>If each of the features makes an independent contribution to the output, then algorithms based on linear functions (e.g., linear regression, logistic regression, linear support vector machines, naive Bayes) generally perform well. However, if there are complex interactions among features, then algorithms such as nonlinear support vector machines, decision trees and neural networks work better. Linear methods can also be applied, but the engineer must manually specify the interactions when using them.</p>
             <p>There are several major issues to consider in supervised learning:</p>
             <ul>
              <li>
               <p>'''Features:''' The accuracy of the inferred function depends strongly on how the input object is represented. Typically, the input object is transformed into a feature vector, which contains a number of features that are descriptive of the object. The number of features should not be too large, because of the curse of dimensionality; but should contain enough information to accurately predict the output. There are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones. More generally, dimensionality reduction may seek to map the input data into a lower dimensional space prior to running the supervised learning algorithm.</p></li>
              <li>
               <p>'''Overfitting:''' Overfitting occurs when a statistical model describes random error or noise instead of the underlying relationship. Overfitting generally occurs when a model is excessively complex, such as having too many parameters relative to the number of observations. A model which has been overfit will generally have poor predictive performance, as it can exaggerate minor fluctuations in the data. The potential for overfitting depends not only on the number of parameters and data but also the conformability of the model structure with the data shape, and the magnitude of model error compared to the expected level of noise or error in the data. In order to avoid overfitting, it is necessary to use additional techniques (e.g. cross-validation, regularization, early stopping, pruning, Bayesian priors on parameters or model comparison), that can indicate when further training is not resulting in better generalization. The basis of some techniques is either (1) to explicitly penalize overly complex models, or (2) to test the model's ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter.</p></li>
              <li>
               <p>'''Regularization:''' Regularization involves introducing additional information in order to solve an ill-posed problem or to prevent over-fitting. This information is usually of the form of a penalty for complexity, such as restrictions for smoothness or bounds on the vector space norm. A theoretical justification for regularization is that it attempts to impose Occam's razor on the solution. From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters.</p></li>
              <li>
               <p>'''Bias-variance tradeoff:''' Mean squared error (MSE) can be broken down into two components: variance and squared bias, known as the bias-variance decomposition. Thus in order to minimize the MSE, we need to minimize both the bias and the variance. However, this is not trivial. Therefore, there is a tradeoff between bias and variance.</p></li>
             </ul>
            </div>
            <h2 class="h200">Attributes</h2>
            <dl class="attributes attributes-small"></dl>
           </div>
          </div>
         </div>
        </div>
       </div>
       <div class="documentableElement">
        <div class="documentableElement-expander">
         <button class="icon-button ar show-content"></button>
         <div class="header monospace mono-medium">
          <div class="signature">
           <span class="modifiers"></span><span class="kind"><span t="k">package </span></span><a href="smile/clustering.html" t="n" class="documentableName ">smile.clustering</a>
          </div>
         </div>
        </div>
        <div class="docs">
         <span class="modifiers"></span>
         <div>
          <div class="originInfo"></div>
          <div class="memberDocumentation">
           <div class="documentableBrief doc">
            <p>Clustering analysis. Clustering is the assignment of a set of observations into subsets (called clusters) so that observations in the same cluster are similar in some sense. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis used in many fields.</p>
           </div>
           <div class="cover">
            <div class="doc">
             <p>Clustering analysis. Clustering is the assignment of a set of observations into subsets (called clusters) so that observations in the same cluster are similar in some sense. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis used in many fields.</p>
             <p>Hierarchical algorithms find successive clusters using previously established clusters. These algorithms usually are either agglomerative ("bottom-up") or divisive ("top-down"). Agglomerative algorithms begin with each element as a separate cluster and merge them into successively larger clusters. Divisive algorithms begin with the whole set and proceed to divide it into successively smaller clusters.</p>
             <p>Partitional algorithms typically determine all clusters at once, but can also be used as divisive algorithms in the hierarchical clustering. Many partitional clustering algorithms require the specification of the number of clusters to produce in the input data set, prior to execution of the algorithm. Barring knowledge of the proper value beforehand, the appropriate value must be determined, a problem on its own for which a number of techniques have been developed.</p>
             <p>Density-based clustering algorithms are devised to discover arbitrary-shaped clusters. In this approach, a cluster is regarded as a region in which the density of data objects exceeds a threshold.</p>
             <p>Subspace clustering methods look for clusters that can only be seen in a particular projection (subspace, manifold) of the data. These methods thus can ignore irrelevant attributes. The general problem is also known as Correlation clustering while the special case of axis-parallel subspaces is also known as two-way clustering, co-clustering or biclustering in bioinformatics: in these methods not only the objects are clustered but also the features of the objects, i.e., if the data is represented in a data matrix, the rows and columns are clustered simultaneously. They usually do not however work with arbitrary feature combinations as in general subspace methods.</p>
            </div>
            <h2 class="h200">Attributes</h2>
            <dl class="attributes attributes-small"></dl>
           </div>
          </div>
         </div>
        </div>
       </div>
       <div class="documentableElement">
        <div class="documentableElement-expander">
         <button class="icon-button ar show-content"></button>
         <div class="header monospace mono-medium">
          <div class="signature">
           <span class="modifiers"></span><span class="kind"><span t="k">package </span></span><a href="smile/data.html" t="n" class="documentableName ">smile.data</a>
          </div>
         </div>
        </div>
        <div class="docs">
         <span class="modifiers"></span>
         <div>
          <div class="originInfo"></div>
          <div class="memberDocumentation">
           <div class="documentableBrief doc">
            <p>Data manipulation functions.</p>
           </div>
           <div class="cover">
            <div class="doc">
             <p>Data manipulation functions.</p>
            </div>
            <h2 class="h200">Attributes</h2>
            <dl class="attributes attributes-small"></dl>
           </div>
          </div>
         </div>
        </div>
       </div>
       <div class="documentableElement">
        <div class="documentableElement-expander">
         <button class="icon-button ar show-content"></button>
         <div class="header monospace mono-medium">
          <div class="signature">
           <span class="modifiers"></span><span class="kind"><span t="k">package </span></span><a href="smile/feature/extraction.html" t="n" class="documentableName ">smile.feature.extraction</a>
          </div>
         </div>
        </div>
        <div class="docs">
         <span class="modifiers"></span>
         <div>
          <div class="originInfo"></div>
          <div class="memberDocumentation">
           <div class="documentableBrief doc">
            <p>Feature extraction. Feature extraction transforms the data in the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist.</p>
           </div>
           <div class="cover">
            <div class="doc">
             <p>Feature extraction. Feature extraction transforms the data in the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist.</p>
             <p>The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, the correlation matrix of the data is constructed and the eigenvectors on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behavior of the system. The original space has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors.</p>
             <p>Compared to regular batch PCA algorithm, the generalized Hebbian algorithm is an adaptive method to find the largest k eigenvectors of the covariance matrix, assuming that the associated eigenvalues are distinct. GHA works with an arbitrarily large sample size and the storage requirement is modest. Another attractive feature is that, in a nonstationary environment, it has an inherent ability to track gradual changes in the optimal solution in an inexpensive way.</p>
             <p>Random projection is a promising linear dimensionality reduction technique for learning mixtures of Gaussians. The key idea of random projection arises from the Johnson-Lindenstrauss lemma: if points in a vector space are projected onto a randomly selected subspace of suitably high dimension, then the distances between the points are approximately preserved.</p>
             <p>Principal component analysis can be employed in a nonlinear way by means of the kernel trick. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data. The resulting technique is entitled Kernel PCA. Other prominent nonlinear techniques include manifold learning techniques such as locally linear embedding (LLE), Hessian LLE, Laplacian eigenmaps, and LTSA. These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data, and can be viewed as defining a graph-based kernel for Kernel PCA. More recently, techniques have been proposed that, instead of defining a fixed kernel, try to learn the kernel using semidefinite programming. The most prominent example of such a technique is maximum variance unfolding (MVU). The central idea of MVU is to exactly preserve all pairwise distances between nearest neighbors (in the inner product space), while maximizing the distances between points that are not nearest neighbors.</p>
             <p>An alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces. Important examples of such techniques include classical multidimensional scaling (which is identical to PCA), Isomap (which uses geodesic distances in the data space), diffusion maps (which uses diffusion distances in the data space), t-SNE (which minimizes the divergence between distributions over pairs of points), and curvilinear component analysis.</p>
             <p>A different approach to nonlinear dimensionality reduction is through the use of autoencoders, a special kind of feed-forward neural networks with a bottle-neck hidden layer. The training of deep encoders is typically performed using a greedy layer-wise pre-training (e.g., using a stack of Restricted Boltzmann machines) that is followed by a finetuning stage based on backpropagation.</p>
            </div>
            <h2 class="h200">Attributes</h2>
            <dl class="attributes attributes-small"></dl>
           </div>
          </div>
         </div>
        </div>
       </div>
       <div class="documentableElement">
        <div class="documentableElement-expander">
         <button class="icon-button ar show-content"></button>
         <div class="header monospace mono-medium">
          <div class="signature">
           <span class="modifiers"></span><span class="kind"><span t="k">package </span></span><a href="smile/manifold.html" t="n" class="documentableName ">smile.manifold</a>
          </div>
         </div>
        </div>
        <div class="docs">
         <span class="modifiers"></span>
         <div>
          <div class="originInfo"></div>
          <div class="memberDocumentation">
           <div class="documentableBrief doc">
            <p>Manifold learning finds a low-dimensional basis for describing high-dimensional data. Manifold learning is a popular approach to nonlinear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high; though each data point consists of perhaps thousands of features, it may be described as a function of only a few underlying parameters. That is, the data points are actually samples from a low-dimensional manifold that is embedded in a high-dimensional space. Manifold learning algorithms attempt to uncover these parameters in order to find a low-dimensional representation of the data.</p>
           </div>
           <div class="cover">
            <div class="doc">
             <p>Manifold learning finds a low-dimensional basis for describing high-dimensional data. Manifold learning is a popular approach to nonlinear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high; though each data point consists of perhaps thousands of features, it may be described as a function of only a few underlying parameters. That is, the data points are actually samples from a low-dimensional manifold that is embedded in a high-dimensional space. Manifold learning algorithms attempt to uncover these parameters in order to find a low-dimensional representation of the data.</p>
             <p>Some prominent approaches are locally linear embedding (LLE), Hessian LLE, Laplacian eigenmaps, and LTSA. These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data, and can be viewed as defining a graph-based kernel for Kernel PCA. More recently, techniques have been proposed that, instead of defining a fixed kernel, try to learn the kernel using semidefinite programming. The most prominent example of such a technique is maximum variance unfolding (MVU). The central idea of MVU is to exactly preserve all pairwise distances between nearest neighbors (in the inner product space), while maximizing the distances between points that are not nearest neighbors.</p>
             <p>An alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces. Important examples of such techniques include classical multidimensional scaling (which is identical to PCA), Isomap (which uses geodesic distances in the data space), diffusion maps (which uses diffusion distances in the data space), t-SNE (which minimizes the divergence between distributions over pairs of points), and curvilinear component analysis.</p>
            </div>
            <h2 class="h200">Attributes</h2>
            <dl class="attributes attributes-small"></dl>
           </div>
          </div>
         </div>
        </div>
       </div>
       <div class="documentableElement">
        <div class="documentableElement-expander">
         <button class="icon-button ar show-content"></button>
         <div class="header monospace mono-medium">
          <div class="signature">
           <span class="modifiers"></span><span class="kind"><span t="k">package </span></span><a href="smile/math.html" t="n" class="documentableName ">smile.math</a>
          </div>
         </div>
        </div>
        <div class="docs">
         <span class="modifiers"></span>
         <div>
          <div class="originInfo"></div>
          <div class="memberDocumentation">
           <div class="documentableBrief doc">
            <p>Mathematical and statistical functions.</p>
           </div>
           <div class="cover">
            <div class="doc">
             <p>Mathematical and statistical functions.</p>
            </div>
            <h2 class="h200">Attributes</h2>
            <dl class="attributes attributes-small"></dl>
           </div>
          </div>
         </div>
        </div>
       </div>
       <div class="documentableElement">
        <div class="documentableElement-expander">
         <button class="icon-button ar show-content"></button>
         <div class="header monospace mono-medium">
          <div class="signature">
           <span class="modifiers"></span><span class="kind"><span t="k">package </span></span><a href="smile/nlp.html" t="n" class="documentableName ">smile.nlp</a>
          </div>
         </div>
        </div>
        <div class="docs">
         <span class="modifiers"></span>
         <div>
          <div class="originInfo"></div>
          <div class="memberDocumentation">
           <div class="documentableBrief doc">
            <p>Natural language processing.</p>
           </div>
           <div class="cover">
            <div class="doc">
             <p>Natural language processing.</p>
            </div>
            <h2 class="h200">Attributes</h2>
            <dl class="attributes attributes-small"></dl>
           </div>
          </div>
         </div>
        </div>
       </div>
       <div class="documentableElement">
        <div class="documentableElement-expander">
         <button class="icon-button ar show-content"></button>
         <div class="header monospace mono-medium">
          <div class="signature">
           <span class="modifiers"></span><span class="kind"><span t="k">package </span></span><a href="smile/plot.html" t="n" class="documentableName ">smile.plot</a>
          </div>
         </div>
        </div>
        <div class="docs">
         <span class="modifiers"></span>
         <div>
          <div class="originInfo"></div>
          <div class="memberDocumentation">
           <div class="documentableBrief doc">
            <p>Data visualization.</p>
           </div>
           <div class="cover">
            <div class="doc">
             <p>Data visualization.</p>
            </div>
            <h2 class="h200">Attributes</h2>
            <dl class="attributes attributes-small"></dl>
           </div>
          </div>
         </div>
        </div>
       </div>
       <div class="documentableElement">
        <div class="documentableElement-expander">
         <button class="icon-button ar show-content"></button>
         <div class="header monospace mono-medium">
          <div class="signature">
           <span class="modifiers"></span><span class="kind"><span t="k">package </span></span><a href="smile/regression.html" t="n" class="documentableName ">smile.regression</a>
          </div>
         </div>
        </div>
        <div class="docs">
         <span class="modifiers"></span>
         <div>
          <div class="originInfo"></div>
          <div class="memberDocumentation">
           <div class="documentableBrief doc">
            <p>Regression analysis. Regression analysis includes any techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables. Most commonly, regression analysis estimates the conditional expectation of the dependent variable given the independent variables. Therefore, the estimation target is a function of the independent variables called the regression function. Regression analysis is widely used for prediction and forecasting.</p>
           </div>
           <div class="cover">
            <div class="doc">
             <p>Regression analysis. Regression analysis includes any techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables. Most commonly, regression analysis estimates the conditional expectation of the dependent variable given the independent variables. Therefore, the estimation target is a function of the independent variables called the regression function. Regression analysis is widely used for prediction and forecasting.</p>
            </div>
            <h2 class="h200">Attributes</h2>
            <dl class="attributes attributes-small"></dl>
           </div>
          </div>
         </div>
        </div>
       </div>
       <div class="documentableElement">
        <div class="documentableElement-expander">
         <button class="icon-button ar show-content"></button>
         <div class="header monospace mono-medium">
          <div class="signature">
           <span class="modifiers"></span><span class="kind"><span t="k">package </span></span><a href="smile/sequence.html" t="n" class="documentableName ">smile.sequence</a>
          </div>
         </div>
        </div>
        <div class="docs">
         <span class="modifiers"></span>
         <div>
          <div class="originInfo"></div>
          <div class="memberDocumentation">
           <div class="documentableBrief doc">
            <p>Sequence labeling algorithms.</p>
           </div>
           <div class="cover">
            <div class="doc">
             <p>Sequence labeling algorithms.</p>
            </div>
            <h2 class="h200">Attributes</h2>
            <dl class="attributes attributes-small"></dl>
           </div>
          </div>
         </div>
        </div>
       </div>
       <div class="documentableElement">
        <div class="documentableElement-expander">
         <button class="icon-button ar show-content"></button>
         <div class="header monospace mono-medium">
          <div class="signature">
           <span class="modifiers"></span><span class="kind"><span t="k">package </span></span><a href="smile/util.html" t="n" class="documentableName ">smile.util</a>
          </div>
         </div>
        </div>
        <div class="docs">
         <span class="modifiers"></span>
         <div>
          <div class="originInfo"></div>
          <div class="memberDocumentation">
           <div class="documentableBrief doc">
            <p>Utility functions.</p>
           </div>
           <div class="cover">
            <div class="doc">
             <p>Utility functions.</p>
            </div>
            <h2 class="h200">Attributes</h2>
            <dl class="attributes attributes-small"></dl>
           </div>
          </div>
         </div>
        </div>
       </div>
       <div class="documentableElement">
        <div class="documentableElement-expander">
         <button class="icon-button ar show-content"></button>
         <div class="header monospace mono-medium">
          <div class="signature">
           <span class="modifiers"></span><span class="kind"><span t="k">package </span></span><a href="smile/validation.html" t="n" class="documentableName ">smile.validation</a>
          </div>
         </div>
        </div>
        <div class="docs">
         <span class="modifiers"></span>
         <div>
          <div class="originInfo"></div>
          <div class="memberDocumentation">
           <div class="documentableBrief doc">
            <p>Model validation.</p>
           </div>
           <div class="cover">
            <div class="doc">
             <p>Model validation.</p>
            </div>
            <h2 class="h200">Attributes</h2>
            <dl class="attributes attributes-small"></dl>
           </div>
          </div>
         </div>
        </div>
       </div>
       <div class="documentableElement">
        <div class="documentableElement-expander">
         <button class="icon-button ar show-content"></button>
         <div class="header monospace mono-medium">
          <div class="signature">
           <span class="modifiers"></span><span class="kind"><span t="k">package </span></span><a href="smile/wavelet.html" t="n" class="documentableName ">smile.wavelet</a>
          </div>
         </div>
        </div>
        <div class="docs">
         <span class="modifiers"></span>
         <div>
          <div class="originInfo"></div>
          <div class="memberDocumentation">
           <div class="documentableBrief doc">
            <p>A wavelet is a wave-like oscillation with an amplitude that starts out at zero, increases, and then decreases back to zero. Like the fast Fourier transform (FFT), the discrete wavelet transform (DWT) is a fast, linear operation that operates on a data vector whose length is an integer power of 2, transforming it into a numerically different vector of the same length. The wavelet transform is invertible and in fact orthogonal. Both FFT and DWT can be viewed as a rotation in function space.</p>
           </div>
           <div class="cover">
            <div class="doc">
             <p>A wavelet is a wave-like oscillation with an amplitude that starts out at zero, increases, and then decreases back to zero. Like the fast Fourier transform (FFT), the discrete wavelet transform (DWT) is a fast, linear operation that operates on a data vector whose length is an integer power of 2, transforming it into a numerically different vector of the same length. The wavelet transform is invertible and in fact orthogonal. Both FFT and DWT can be viewed as a rotation in function space.</p>
            </div>
            <h2 class="h200">Attributes</h2>
            <dl class="attributes attributes-small"></dl>
           </div>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
    </section>
    <section id="Type-members">
     <div data-togglable="Type members" class="tab expand">
      <div class="member-group-header">
       <h3 data-togglable="Type members" class="h400">Type members</h3>
      </div>
      <section id="Classlikes">
       <div class="documentableList expand">
        <div class="documentableList-expander">
         <button class="icon-button show-content expand"></button>
         <h4 class="groupHeader h200">Classlikes</h4>
        </div>
        <div class="documentableElement">
         <div class="documentableElement-expander">
          <button class="icon-button ar show-content"></button>
          <div class="header monospace mono-medium">
           <div class="signature">
            <span class="modifiers"></span><span class="kind"><span t="k">object </span></span><a href="smile/read$.html" t="n" class="documentableName ">read</a>
           </div>
          </div>
         </div>
         <div class="docs">
          <span class="modifiers"></span>
          <div>
           <div class="originInfo"></div>
           <div class="memberDocumentation">
            <div class="documentableBrief doc">
             <p>Data loading utilities.</p>
            </div>
            <div class="cover">
             <div class="doc">
              <p>Data loading utilities.</p>
             </div>
             <h2 class="h200">Attributes</h2>
             <dl class="attributes attributes-small">
              <dt class="body-small">
               Supertypes
              </dt>
              <dd class="body-medium">
               <div class="mono-small-block supertypes">
                <div>
                 class <span data-unresolved-link="" t="n">Object</span>
                </div>
                <div>
                 trait <span data-unresolved-link="" t="n">Matchable</span>
                </div>
                <div>
                 class <span data-unresolved-link="" t="n">Any</span>
                </div><span></span>
               </div>
              </dd>
              <dt class="body-small">
               Self type
              </dt>
              <dd class="body-medium">
               <div class="mono-small-block supertypes">
                <span></span>
                <div>
                 <a href="smile/read$.html" t="n">read</a>.<span t="k">type</span>
                </div>
               </div>
              </dd>
             </dl>
            </div>
           </div>
          </div>
         </div>
        </div>
        <div class="documentableElement">
         <div class="documentableElement-expander">
          <button class="icon-button ar show-content"></button>
          <div class="header monospace mono-medium">
           <div class="signature">
            <span class="modifiers"></span><span class="kind"><span t="k">object </span></span><a href="smile/write$.html" t="n" class="documentableName ">write</a>
           </div>
          </div>
         </div>
         <div class="docs">
          <span class="modifiers"></span>
          <div>
           <div class="originInfo"></div>
           <div class="memberDocumentation">
            <div class="documentableBrief doc">
             <p>Data saving utilities.</p>
            </div>
            <div class="cover">
             <div class="doc">
              <p>Data saving utilities.</p>
             </div>
             <h2 class="h200">Attributes</h2>
             <dl class="attributes attributes-small">
              <dt class="body-small">
               Supertypes
              </dt>
              <dd class="body-medium">
               <div class="mono-small-block supertypes">
                <div>
                 class <span data-unresolved-link="" t="n">Object</span>
                </div>
                <div>
                 trait <span data-unresolved-link="" t="n">Matchable</span>
                </div>
                <div>
                 class <span data-unresolved-link="" t="n">Any</span>
                </div><span></span>
               </div>
              </dd>
              <dt class="body-small">
               Self type
              </dt>
              <dd class="body-medium">
               <div class="mono-small-block supertypes">
                <span></span>
                <div>
                 <a href="smile/write$.html" t="n">write</a>.<span t="k">type</span>
                </div>
               </div>
              </dd>
             </dl>
            </div>
           </div>
          </div>
         </div>
        </div>
       </div>
      </section>
     </div>
    </section>
   </div>
  </div>
 </div>
</section></div><div id="toc" class="body-small"><div id="toc-container"><span class="toc-title h200">In this article</span><nav class="toc-nav"><ul class="toc-list"><li><a href="#members-list">Members list</a><ul><li><a href="#Packages">Packages</a></li><li><a href="#Type-members">Type members</a><ul><li><a href="#Classlikes">Classlikes</a></li></ul></li></ul></li></ul></nav></div></div></div><div id="footer" class="body-small mobile-footer"><div class="left-container">Generated with</div><div class="right-container"><div class="text">Copyright © 2010-2025 Haifeng Li. All rights reserved.
Use is subject to license terms.</div></div><div class="text-mobile">Copyright © 2010-2025 Haifeng Li. All rights reserved.
Use is subject to license terms.</div></div></div></div></body></html>